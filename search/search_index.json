{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HDXMS Datasets","text":"<p>Welcome to the HDXMS datasets repository. </p> <p>The <code>hdxms-datasets</code> package provides tools handling HDX-MS datasets.</p> <p>The package offers the following features:</p> <ul> <li>Defining datasets and their experimental metadata</li> <li>Verification of datasets and metadata</li> <li>Loading datasets from local or remote database</li> <li>Conversion of datasets from various formats (e.g., DynamX, HDExaminer) to a standardized format</li> <li>Propagation of standard deviations from replicates to fractional relative uptake values</li> </ul> <p>A database for open HDX datasets is set up at HDXMS DataBase</p>"},{"location":"#example-usage","title":"Example Usage","text":""},{"location":"#loading-datasets","title":"Loading datasets","text":"<pre><code>from hdxms_datasets import DataBase\n\ndb = DataBase('path/to/local_db')\ndataset = db.get_dataset('HDX_D9096080')\n\n# Protein identifier information\nprint(dataset.protein_identifiers.uniprot_entry_name)\n#&gt; 'SECB_ECOLI'\n\n# Access HDX states \nprint([state.name for state in dataset.states])\n#&gt; ['Tetramer', 'Dimer']\n\n# Get the sequence of the first state\nstate = dataset.states[0]\nprint(state.protein_state.sequence)\n#&gt; 'MSEQNNTEMTFQIQRIYT...'\n\n# Load peptides\npeptides = state.peptides[0]\n\n# Access peptide information\nprint(peptides.deuteration_type, peptides.pH, peptides.temperature)\n#&gt; DeuterationType.partially_deuterated 8.0 303.15\n\n# Load the peptide table as standardized narwhals DataFrame\ndf = peptides.load(\n    convert=True,  # convert column header names to open hdx stanard\n    aggregate=True, # aggregate centroids / uptake values across replicates\n)\n\nprint(df.columns)\n#&gt; ['start', 'end', 'sequence', 'state', 'exposure', 'centroid_mz', 'rt', 'rt_sd', 'uptake', ... \n</code></pre>"},{"location":"#define-and-process-datasets","title":"Define and process datasets","text":"<pre><code>from hdxms_datasets import ProteinState, Peptides, verify_sequence, merge_peptides, compute_uptake_metrics\n\n# Define the protein state\nprotein_state = ProteinState(\n    sequence=\"MSEQNNTEMTFQIQRIYTKDISFEAPNAPHVFQKDWQPEVKLDLDTASSQLADDVYEVVLRVTVTASLGEETAFLCEVQQGGIFSIAGIEGTQMAHCLGAYCPNILFPYARECITSMVSRGTFPQLNLAPVNFDALFMNYLQQQAGEGTEEHQDA\",\n    n_term=1,\n    c_term=155,\n    oligomeric_state=4,\n)\n\n# Define the partially deuterated peptides for the SecB state\npd_peptides = Peptides(\n    # path to the data file\n    data_file=data_dir / \"ecSecB_apo.csv\",\n    # specify the data format\n    data_format=PeptideFormat.DynamX_v3_state,\n    # specify the deuteration type (partially, fully or not deuterated)\n    deuteration_type=DeuterationType.partially_deuterated,\n    filters={\n        \"State\": \"SecB WT apo\",\n        # Optionally filter by exposure, leave out to include all exposures\n        \"Exposure\": [0.167, 0.5, 1.0, 10.0, 100.000008],\n    },\n    # pH read without corrections\n    pH=8.0,\n    # temperature of the exchange buffer\n    temperature=303.15,\n    # deuterium percentage of the exchange buffer\n    d_percentage=90.0,\n)\n\n# check for difference between the protein state sequence and the peptide sequences\nmismatches = verify_sequence(pd_peptides.load(), protein_state.sequence, n_term=protein_state.n_term)\nprint(mismatches)\n#&gt; [] # sequences match\n\n# Define the fully deuterated peptides for the SecB state\nfd_peptides = Peptides(\n    data_file=data_dir / \"ecSecB_apo.csv\",\n    data_format=PeptideFormat.DynamX_v3_state,\n    deuteration_type=DeuterationType.fully_deuterated,\n    filters={\n        \"State\": \"Full deuteration control\",\n        \"Exposure\": 0.167,\n    },\n)\n\n# merge both peptides together in a single dataframe\nmerged = merge_peptides([pd_peptides, fd_peptides])\nprint(merged.columns)\n#&gt; ['start', 'end', 'sequence', ... 'uptake', 'uptake_sd', 'fd_uptake', 'fd_uptake_sd']\n\n# compute uptake metrics for the merged peptides\n# this function computes uptake from centroid mass if not present\n# as well as fractional uptake\nprocessed = compute_uptake_metrics(merged)\nprint(processed.columns)\n#&gt; ['start', 'end', 'sequence', ... 'uptake', 'uptake_sd', 'fd_uptake', 'fd_uptake_sd', 'fractional_uptake', 'fractional_uptake_sd']\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install hdxms-datasets\n</code></pre>"},{"location":"fields/","title":"Fields","text":""},{"location":"fields/#start-int","title":"start (int)","text":"<p>residue number of the first amino acid in the peptide</p>"},{"location":"fields/#end-int","title":"end (int)","text":"<p>residue number of the last amino acid in the peptide</p>"},{"location":"fields/#sequence-str","title":"sequence (str)","text":"<p>fasta sequence of the peptide</p>"},{"location":"fields/#state-str","title":"state (str)","text":"<p>state label</p> <p>DynamX state/cluster name: State HDExaminer name: Protein State</p>"},{"location":"fields/#replicate-str","title":"replicate (str)","text":"<p>Label for the replicate DynamX cluster name: File HDExaminer name: Experiment</p>"},{"location":"fields/#exposure-float","title":"exposure (float)","text":"<p>Deuterium exposure time in seconds</p> <p>DynamX state/cluster name: Exposure HDExaminer name: Deut Time</p>"},{"location":"fields/#centroid_mass-str","title":"centroid_mass (str)","text":"<p>calculated mass of uncharged peptide derived from charge / centroid</p>"},{"location":"fields/#centroid_mass_sd-str","title":"centroid_mass_sd (str)","text":"<p>Standard deviation of the centroid mass value</p>"},{"location":"fields/#centroid_mz","title":"centroid_mz","text":"<p>HDExaminer name: Exp Cent DynamX name: ??</p>"},{"location":"fields/#centroid_mz_sd","title":"centroid_mz_sd","text":"<p>Standard deviation of the centroid m/z value</p>"},{"location":"fields/#rt","title":"rt","text":"<p>retention time  units unknown (minutes?)</p> <p>DynamX state/cluster name: RT HDExaminer name: Actual RT</p>"},{"location":"fields/#rt_sd-float","title":"rt_sd (float)","text":"<p>Standard deviation of the retention time value</p>"},{"location":"fields/#charge-int","title":"charge (int)","text":"<p>DynamX cluster name: z HDExaminer name: Charge</p>"},{"location":"fields/#intensity-float","title":"intensity (float)","text":"<p>HDExaminer name: Max Inty DynamX name?? is this max or mean intensity?</p>"},{"location":"fields/#optional-fields","title":"Optional fields:","text":"<p>These fields can be present in open-hdxms files, but can also be calculated from the other fields.</p>"},{"location":"fields/#max_uptake-int","title":"max_uptake (int)","text":"<p>Theoretical maximum deuterium uptake for the peptide. Typically equal to the number of amide hydrogens, thus number of non-proline residues minus one. </p>"},{"location":"fields/#uptake-float","title":"uptake (float)","text":"<p>Number of deuterium atoms incorporated into the peptide calculated from centroid mass, if available</p>"},{"location":"fields/#uptake_sd-float","title":"uptake_sd (float)","text":"<p>Standard deviation of the uptake value</p>"},{"location":"fields/#calculated-fields","title":"Calculated fields:","text":"<p>These fields are derived from other fields defined in the above sections.</p>"},{"location":"fields/#frac_fd_control-float","title":"frac_fd_control (float)","text":"<p>Fractional deuterium uptake with respect to fully deuterated control sample</p>"},{"location":"fields/#frac_fd_control_sd-float","title":"frac_fd_control_sd (float)","text":"<p>Standard deviation of the fractional deuterium uptake with respect to fully deuterated control sample</p>"},{"location":"fields/#frac_max_uptake-float","title":"frac_max_uptake (float)","text":"<p>Fractional deuterium uptake with respect to the maximum possible uptake for the peptide</p>"},{"location":"fields/#frac_max_uptake_sd-float","title":"frac_max_uptake_sd (float)","text":"<p>Standard deviation of the fractional deuterium uptake with respect to the maximum possible uptake for the peptide</p>"},{"location":"install/","title":"Installation","text":"<pre><code>$ pip install hdxms-datasets\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>convert</li> <li>database</li> <li>expr</li> <li>formats</li> <li>loader</li> <li>migration<ul> <li>v020</li> </ul> </li> <li>models</li> <li>plot</li> <li>process</li> <li>stable<ul> <li>v020<ul> <li>backend</li> <li>convert</li> <li>datasets</li> <li>datavault</li> <li>expr</li> <li>formats</li> <li>plot</li> <li>process</li> <li>reader</li> <li>utils</li> </ul> </li> </ul> </li> <li>utils</li> <li>verification</li> <li>view</li> <li>web<ul> <li>components</li> <li>models</li> <li>state</li> <li>upload_form</li> </ul> </li> </ul>"},{"location":"reference/convert/","title":"convert","text":""},{"location":"reference/convert/#convert.convert_rt","title":"<code>convert_rt(rt_str)</code>","text":"<p>Convert HDExaminer retention time string to float example: \"7.44-7.65\" -&gt; 7.545</p> <p>Lossy conversion</p> <p>This conversion loses information. The full range is not preserved. This was done such that retention time can be stored as float and thus be aggregated. Future versions may store the full range with additional <code>rt_min</code> and <code>rt_max</code> columns.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def convert_rt(rt_str: str) -&gt; float:\n    \"\"\"Convert HDExaminer retention time string to float\n    example: \"7.44-7.65\" -&gt; 7.545\n\n    !!! warning \"Lossy conversion\"\n        This conversion loses information. The full range is not preserved. This was done such that\n        retention time can be stored as float and thus be aggregated.\n        Future versions may store the full range with additional `rt_min` and `rt_max` columns.\n\n    \"\"\"\n    vmin, vmax = rt_str.split(\"-\")\n    mean = (float(vmin) + float(vmax)) / 2.0\n    return mean\n</code></pre>"},{"location":"reference/convert/#convert.from_dynamx_cluster","title":"<code>from_dynamx_cluster(dynamx_df)</code>","text":"<p>Convert a DynamX cluster DataFrame to OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_dynamx_cluster(dynamx_df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert a DynamX cluster DataFrame to OpenHDX format.\n    \"\"\"\n    column_mapping = {\n        \"State\": \"state\",\n        \"Exposure\": \"exposure\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Sequence\": \"sequence\",\n        \"File\": \"replicate\",\n        \"z\": \"charge\",\n        \"Center\": \"centroid_mz\",\n        \"Inten\": \"intensity\",\n        \"RT\": \"rt\",\n    }\n\n    column_order = list(column_mapping.values())\n    column_order.insert(column_order.index(\"charge\") + 1, \"centroid_mass\")\n\n    df = (\n        dynamx_df.rename(column_mapping)\n        .with_columns([centroid_mass, nw.col(\"exposure\") * 60.0])\n        .select(column_order)\n        .sort(by=[\"state\", \"exposure\", \"start\", \"end\", \"replicate\"])\n    )\n\n    return df\n</code></pre>"},{"location":"reference/convert/#convert.from_dynamx_state","title":"<code>from_dynamx_state(dynamx_df)</code>","text":"<p>Convert a DynamX state DataFrame to OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_dynamx_state(dynamx_df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert a DynamX state DataFrame to OpenHDX format.\n    \"\"\"\n    column_mapping = {\n        \"State\": \"state\",\n        \"Exposure\": \"exposure\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Sequence\": \"sequence\",\n        \"Uptake\": \"uptake\",\n        \"Uptake SD\": \"uptake_sd\",\n        \"Center\": \"centroid_mz\",\n        \"RT\": \"rt\",\n        \"RT SD\": \"rt_sd\",\n    }\n\n    column_order = list(column_mapping.values())\n\n    df = (\n        dynamx_df.rename(column_mapping)\n        .with_columns([nw.col(\"exposure\") * 60.0])\n        .select(column_order)\n        .sort(by=[\"state\", \"exposure\", \"start\", \"end\"])\n    )\n\n    return df\n</code></pre>"},{"location":"reference/convert/#convert.from_hdexaminer","title":"<code>from_hdexaminer(hd_examiner_df, extra_columns=None)</code>","text":"<p>Convert an HDExaminer DataFrame to OpenHDX format.</p> <p>Parameters:</p> Name Type Description Default <code>hd_examiner_df</code> <code>DataFrame</code> <p>DataFrame in HDExaminer format.</p> required <code>extra_columns</code> <code>list[str] | dict[str, str] | str | None</code> <p>Additional columns to include, either as a list/str of column name(s)            or a dictionary mapping original column names to new names.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame in OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_hdexaminer(\n    hd_examiner_df: nw.DataFrame,\n    extra_columns: list[str] | dict[str, str] | str | None = None,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert an HDExaminer DataFrame to OpenHDX format.\n\n    Args:\n        hd_examiner_df: DataFrame in HDExaminer format.\n        extra_columns: Additional columns to include, either as a list/str of column name(s)\n                       or a dictionary mapping original column names to new names.\n\n    Returns:\n        A DataFrame in OpenHDX format.\n\n    \"\"\"\n    from hdxms_datasets.loader import BACKEND\n\n    column_mapping = {\n        \"Protein State\": \"state\",\n        \"Deut Time\": \"exposure\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Sequence\": \"sequence\",\n        \"Experiment\": \"replicate\",\n        \"Charge\": \"charge\",\n        \"Exp Cent\": \"centroid_mz\",\n        \"Max Inty\": \"intensity\",\n    }\n\n    column_order = list(column_mapping.values())\n    column_order.insert(column_order.index(\"charge\") + 1, \"centroid_mass\")\n    column_order.append(\"rt\")\n\n    if isinstance(extra_columns, dict):\n        cols = extra_columns\n    elif isinstance(extra_columns, list):\n        cols = {col: col for col in extra_columns}\n    elif isinstance(extra_columns, str):\n        cols = {extra_columns: extra_columns}\n    elif extra_columns is None:\n        cols = {}\n    else:\n        raise ValueError(\n            \"additional_columns must be a list or dict, not {}\".format(type(extra_columns))\n        )\n\n    column_mapping.update(cols)\n    column_order.extend(cols.values())\n\n    rt_values = [convert_rt(rt_str) for rt_str in hd_examiner_df[\"Actual RT\"]]\n    rt_series = nw.new_series(values=rt_values, name=\"rt\", backend=BACKEND)\n\n    df = (\n        hd_examiner_df.rename(column_mapping)\n        .with_columns([centroid_mass, rt_series])\n        .select(column_order)\n        .sort(by=[\"state\", \"exposure\", \"start\", \"end\", \"replicate\"])\n    )\n\n    return cast_exposure(df)\n</code></pre>"},{"location":"reference/database/","title":"database","text":""},{"location":"reference/database/#database.DataBase","title":"<code>DataBase</code>","text":"Source code in <code>hdxms_datasets/database.py</code> <pre><code>class DataBase:\n    def __init__(self, database_dir: Path | str):\n        self.database_dir = Path(database_dir)\n        self.database_dir.mkdir(exist_ok=True, parents=True)\n\n    @property\n    def datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the cache dir\"\"\"\n        return [d.stem for d in self.database_dir.iterdir() if self.is_dataset(d)]\n\n    @staticmethod\n    def is_dataset(path: Path) -&gt; bool:\n        \"\"\"\n        Checks if the supplied path is a HDX-MS dataset.\n        \"\"\"\n\n        return (path / \"dataset.json\").exists()\n\n    def clear_cache(self) -&gt; None:\n        for dir in self.database_dir.iterdir():\n            shutil.rmtree(dir)\n\n    def load_dataset(self, dataset_id: str) -&gt; HDXDataSet:\n        dataset_root = self.database_dir / dataset_id\n        dataset = HDXDataSet.model_validate_json(\n            Path(dataset_root, \"dataset.json\").read_text(),\n            context={\"dataset_root\": dataset_root},\n        )\n        return dataset\n</code></pre>"},{"location":"reference/database/#database.DataBase.datasets","title":"<code>datasets: list[str]</code>  <code>property</code>","text":"<p>List of available datasets in the cache dir</p>"},{"location":"reference/database/#database.DataBase.is_dataset","title":"<code>is_dataset(path)</code>  <code>staticmethod</code>","text":"<p>Checks if the supplied path is a HDX-MS dataset.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>@staticmethod\ndef is_dataset(path: Path) -&gt; bool:\n    \"\"\"\n    Checks if the supplied path is a HDX-MS dataset.\n    \"\"\"\n\n    return (path / \"dataset.json\").exists()\n</code></pre>"},{"location":"reference/database/#database.RemoteDataBase","title":"<code>RemoteDataBase</code>","text":"<p>               Bases: <code>DataBase</code></p> <p>A database for HDX-MS datasets, with the ability to fetch datasets from a remote repository.</p> <p>Parameters:</p> Name Type Description Default <code>database_dir</code> <code>Path | str</code> <p>Directory to store downloaded datasets.</p> required <code>remote_url</code> <code>str</code> <p>URL of the remote repository (default: DATABASE_URL).</p> <code>DATABASE_URL</code> Source code in <code>hdxms_datasets/database.py</code> <pre><code>class RemoteDataBase(DataBase):\n    \"\"\"\n    A database for HDX-MS datasets, with the ability to fetch datasets from a remote repository.\n\n    Args:\n        database_dir: Directory to store downloaded datasets.\n        remote_url: URL of the remote repository (default: DATABASE_URL).\n    \"\"\"\n\n    def __init__(\n        self,\n        database_dir: Path | str,\n        remote_url: str = DATABASE_URL,\n    ):\n        super().__init__(database_dir)\n        self.remote_url = remote_url\n\n        index_url = urljoin(DATABASE_URL, CATALOG_FILE)\n        response = requests.get(index_url)\n\n        # TODO keep catalogs on a per-url basis in a singleton\n        if response.ok:\n            df = read_csv(response.content)\n            self.datasets_catalog = df\n        else:\n            raise HTTPError(\n                index_url,\n                response.status_code,\n                \"Error fetching dataset index\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n    @property\n    def remote_datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the remote repository\"\"\"\n        return self.datasets_catalog[\"id\"].to_list()\n\n    @property\n    def local_datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the local database directory\"\"\"\n        return self.datasets\n\n    def fetch_dataset(self, data_id: str) -&gt; tuple[bool, str]:\n        \"\"\"\n        Download a dataset from the online repository to `database_dir`\n\n        Args:\n            data_id: The ID of the dataset to download.\n\n        Returns:\n            A tuple (success: bool, message: str):\n            - success: True if the dataset was successfully downloaded, False otherwise.\n            - message: A message indicating the result of the download.\n        \"\"\"\n\n        if data_id not in self.remote_datasets:\n            return False, f\"Dataset ID {data_id!r} not found in remote database.\"\n\n        json_url = urljoin(DATABASE_URL, data_id + \"/dataset.json\")\n        response = requests.get(json_url)\n\n        # confirm if the json is according to spec\n        try:\n            dataset = HDXDataSet.model_validate_json(\n                response.content,\n            )\n        except Exception as e:\n            return False, f\"Error validating dataset JSON: {e}\"\n\n        # create a list of all Path objects in the dataset plus the dataset.json file\n        data_files = list(set(extract_values_by_types(dataset, Path))) + [Path(\"dataset.json\")]\n\n        # create the target directory to store the dataset\n        output_pth = self.database_dir / data_id\n        if output_pth.exists():\n            return False, \"Dataset already exists in the local database.\"\n        else:\n            output_pth.mkdir()\n\n        for data_file in data_files:\n            data_url = urljoin(DATABASE_URL, data_id + \"/\" + data_file.as_posix())\n\n            response = requests.get(data_url)\n            if response.ok:\n                # write the file to disk\n                fpath = output_pth / Path(data_file)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.write_bytes(response.content)\n            else:\n                shutil.rmtree(output_pth)  # clean up partial download\n                return False, f\"Failed to download {data_file}: {response.status_code}\"\n\n        return True, \"\"\n</code></pre>"},{"location":"reference/database/#database.RemoteDataBase.local_datasets","title":"<code>local_datasets: list[str]</code>  <code>property</code>","text":"<p>List of available datasets in the local database directory</p>"},{"location":"reference/database/#database.RemoteDataBase.remote_datasets","title":"<code>remote_datasets: list[str]</code>  <code>property</code>","text":"<p>List of available datasets in the remote repository</p>"},{"location":"reference/database/#database.RemoteDataBase.fetch_dataset","title":"<code>fetch_dataset(data_id)</code>","text":"<p>Download a dataset from the online repository to <code>database_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>The ID of the dataset to download.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A tuple (success: bool, message: str):</p> <code>str</code> <ul> <li>success: True if the dataset was successfully downloaded, False otherwise.</li> </ul> <code>tuple[bool, str]</code> <ul> <li>message: A message indicating the result of the download.</li> </ul> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def fetch_dataset(self, data_id: str) -&gt; tuple[bool, str]:\n    \"\"\"\n    Download a dataset from the online repository to `database_dir`\n\n    Args:\n        data_id: The ID of the dataset to download.\n\n    Returns:\n        A tuple (success: bool, message: str):\n        - success: True if the dataset was successfully downloaded, False otherwise.\n        - message: A message indicating the result of the download.\n    \"\"\"\n\n    if data_id not in self.remote_datasets:\n        return False, f\"Dataset ID {data_id!r} not found in remote database.\"\n\n    json_url = urljoin(DATABASE_URL, data_id + \"/dataset.json\")\n    response = requests.get(json_url)\n\n    # confirm if the json is according to spec\n    try:\n        dataset = HDXDataSet.model_validate_json(\n            response.content,\n        )\n    except Exception as e:\n        return False, f\"Error validating dataset JSON: {e}\"\n\n    # create a list of all Path objects in the dataset plus the dataset.json file\n    data_files = list(set(extract_values_by_types(dataset, Path))) + [Path(\"dataset.json\")]\n\n    # create the target directory to store the dataset\n    output_pth = self.database_dir / data_id\n    if output_pth.exists():\n        return False, \"Dataset already exists in the local database.\"\n    else:\n        output_pth.mkdir()\n\n    for data_file in data_files:\n        data_url = urljoin(DATABASE_URL, data_id + \"/\" + data_file.as_posix())\n\n        response = requests.get(data_url)\n        if response.ok:\n            # write the file to disk\n            fpath = output_pth / Path(data_file)\n            fpath.parent.mkdir(parents=True, exist_ok=True)\n            fpath.write_bytes(response.content)\n        else:\n            shutil.rmtree(output_pth)  # clean up partial download\n            return False, f\"Failed to download {data_file}: {response.status_code}\"\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/database/#database.export_dataset","title":"<code>export_dataset(dataset, tgt_dir)</code>","text":"<p>Store a dataset to a target directory. This will copy the data files to a 'data' subdirectory and write the dataset JSON.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def export_dataset(dataset: HDXDataSet, tgt_dir: Path) -&gt; None:\n    \"\"\"\n    Store a dataset to a target directory.\n    This will copy the data files to a 'data' subdirectory and write the dataset JSON.\n    \"\"\"\n\n    # copy the dataset to update the paths\n    ds_copy = dataset.model_copy(deep=True)\n\n    data_dir = tgt_dir / \"data\"\n    data_dir.mkdir(exist_ok=True, parents=True)\n\n    # copy the sequence file\n    shutil.copy(ds_copy.structure.data_file, data_dir / ds_copy.structure.data_file.name)\n    # the the path to the copied file relative path\n    ds_copy.structure.data_file = Path(\"data\") / ds_copy.structure.data_file.name\n\n    # repeat for the peptides\n    for state in ds_copy.states:\n        for peptides in state.peptides:\n            shutil.copy(peptides.data_file, data_dir / peptides.data_file.name)\n            # update the path to the copied file relative path\n            peptides.data_file = Path(\"data\") / peptides.data_file.name\n\n    # write the dataset to a JSON file\n    s = ds_copy.model_dump_json(indent=2, exclude_none=True)\n    Path(tgt_dir / \"dataset.json\").write_text(s)\n</code></pre>"},{"location":"reference/database/#database.find_file_hash_matches","title":"<code>find_file_hash_matches(dataset, database_dir)</code>","text":"<p>Check if a new dataset matches an existing dataset in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def find_file_hash_matches(dataset: HDXDataSet, database_dir: Path) -&gt; list[str]:\n    \"\"\"\n    Check if a new dataset matches an existing dataset in the database directory.\n    \"\"\"\n    try:\n        catalog = nw.read_csv(str(database_dir / \"datasets_catalog.csv\"), backend=BACKEND)\n    except FileNotFoundError:\n        return []\n\n    assert dataset.file_hash is not None, \"Dataset must have a file hash.\"\n    matching_datasets = catalog.filter(nw.col(\"file_hash\") == dataset.file_hash[:16])\n\n    return matching_datasets[\"id\"].to_list()\n</code></pre>"},{"location":"reference/database/#database.generate_datasets_catalog","title":"<code>generate_datasets_catalog(database_dir, save_csv=True)</code>","text":"<p>Generate an overview DataFrame of all datasets in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def generate_datasets_catalog(database_dir: Path, save_csv: bool = True) -&gt; nw.DataFrame:\n    \"\"\"\n    Generate an overview DataFrame of all datasets in the database directory.\n    \"\"\"\n    records = []\n    for ds_id in list_datasets(database_dir):\n        ds_path = database_dir / ds_id / \"dataset.json\"\n        if ds_path.exists():\n            dataset = HDXDataSet.model_validate_json(\n                ds_path.read_text(), context={\"dataset_root\": database_dir / ds_id}\n            )\n            records.append(\n                {\n                    \"id\": ds_id,\n                    \"description\": dataset.description,\n                    \"author\": dataset.metadata.authors[0].last_name,\n                    \"doi\": dataset.metadata.publication.doi\n                    if dataset.metadata.publication\n                    else None,\n                    \"created_date\": dataset.metadata.created_date,\n                    \"uniprot_accession_number\": dataset.protein_identifiers.uniprot_accession_number,\n                    \"file_hash\": dataset.file_hash,\n                }\n            )\n\n    df = nw.from_dict(records_to_dict(records), backend=BACKEND)\n    if save_csv:\n        df.write_csv(database_dir / \"datasets_catalog.csv\")\n\n    return df\n</code></pre>"},{"location":"reference/database/#database.list_datasets","title":"<code>list_datasets(database_dir)</code>","text":"<p>List all valid dataset IDs in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def list_datasets(database_dir: Path) -&gt; list[str]:\n    \"\"\"\n    List all valid dataset IDs in the database directory.\n    \"\"\"\n\n    return [p.stem for p in database_dir.iterdir() if valid_id(p.stem)]\n</code></pre>"},{"location":"reference/database/#database.load_dataset","title":"<code>load_dataset(pth)</code>","text":"<p>Load a dataset from a JSON file or directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def load_dataset(pth: Path) -&gt; HDXDataSet:\n    \"\"\"\n    Load a dataset from a JSON file or directory.\n    \"\"\"\n\n    if pth.is_file():\n        dataset_root = pth.parent\n        json_pth = pth\n    else:\n        dataset_root = pth\n        json_pth = dataset_root / \"dataset.json\"\n    dataset = HDXDataSet.model_validate_json(\n        json_pth.read_text(), context={\"dataset_root\": dataset_root}\n    )\n    return dataset\n</code></pre>"},{"location":"reference/database/#database.mint_new_dataset_id","title":"<code>mint_new_dataset_id(current_ids)</code>","text":"<p>Mint a new dataset ID that does not conflict with existing IDs in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def mint_new_dataset_id(current_ids: set[str]) -&gt; str:\n    \"\"\"\n    Mint a new dataset ID that does not conflict with existing IDs in the database directory.\n    \"\"\"\n    while True:\n        new_id = f\"HDX_{uuid.uuid4().hex[:8].upper()}\"\n        if new_id not in current_ids:\n            return new_id\n</code></pre>"},{"location":"reference/database/#database.submit_dataset","title":"<code>submit_dataset(dataset, database_dir, dataset_id=None, check_existing=True, verify=True)</code>","text":"<p>Submit a dataset to a local HDX-MS database.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>HDXDataSet</code> <p>The HDXDataSet to submit.</p> required <code>database_dir</code> <code>Path</code> <p>The directory where the dataset will be stored.</p> required <code>dataset_id</code> <code>str | None</code> <p>Optional ID for the dataset. If not provided, a new ID will be minted.</p> <code>None</code> <code>check_existing</code> <code>bool</code> <p>If True, checks if the dataset already exists in the database.</p> <code>True</code> <code>verify</code> <code>bool</code> <p>If True, verifies the dataset before submission.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>A tuple (success: bool, message: str):</p> <code>str</code> <ul> <li>success: True if the dataset was successfully submitted, False otherwise.</li> </ul> <code>tuple[bool, str]</code> <ul> <li>message: A message indicating the result of the submission.</li> </ul> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def submit_dataset(\n    dataset: HDXDataSet,\n    database_dir: Path,\n    dataset_id: str | None = None,\n    check_existing: bool = True,\n    verify: bool = True,\n) -&gt; tuple[bool, str]:\n    \"\"\"\n    Submit a dataset to a local HDX-MS database.\n\n    Args:\n        dataset: The HDXDataSet to submit.\n        database_dir: The directory where the dataset will be stored.\n        dataset_id: Optional ID for the dataset. If not provided, a new ID will be minted.\n        check_existing: If True, checks if the dataset already exists in the database.\n        verify: If True, verifies the dataset before submission.\n\n    Returns:\n        A tuple (success: bool, message: str):\n        - success: True if the dataset was successfully submitted, False otherwise.\n        - message: A message indicating the result of the submission.\n\n    \"\"\"\n\n    if verify:\n        verify_dataset(dataset)\n\n    if not database_dir.is_absolute():\n        raise ValueError(\"Database directory must be an absolute path.\")\n\n    # check if the uniprot ID is already there,\n    # although there could be multiple states with the same uniprot ID\n    # this is a quick check to avoid duplicates\n    if check_existing:\n        matches = find_file_hash_matches(dataset, database_dir)\n        if matches:\n            if len(matches) == 1:\n                msg = f\"Dataset matches an existing dataset in the database: {matches[0]}\"\n            else:\n                msg = f\"Dataset matches existing datasets in the database: {', '.join(matches)}\"\n            return False, msg\n\n    # mint a new ID if not provided\n    existing_ids = set(list_datasets(database_dir))\n    if dataset_id is None:\n        dataset_id = mint_new_dataset_id(existing_ids)\n    else:\n        if dataset_id in existing_ids:\n            return False, f\"Dataset ID {dataset_id} already exists in the database.\"\n\n    if not valid_id(dataset_id):\n        raise ValueError(\n            f\"Invalid dataset ID: {dataset_id}. \"\n            \"A valid ID starts with 'HDX_' followed by 8 uppercase alphanumeric characters.\"\n        )\n\n    # create the target directory\n    tgt_dir = database_dir / dataset_id\n    export_dataset(dataset, tgt_dir)\n\n    # update the catalogue\n    # TODO: update instead of regenerate\n    # TODO: lockfile? https://github.com/harlowja/fasteners\n    generate_datasets_catalog(database_dir, save_csv=True)\n\n    return True, dataset_id\n</code></pre>"},{"location":"reference/database/#database.valid_id","title":"<code>valid_id(dataset_id)</code>","text":"<p>Check if the dataset ID is valid. A valid ID starts with 'HDX_' followed by 8 uppercase alphanumeric characters.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def valid_id(dataset_id: str) -&gt; bool:\n    \"\"\"\n    Check if the dataset ID is valid.\n    A valid ID starts with 'HDX_' followed by 8 uppercase alphanumeric characters.\n    \"\"\"\n    return (\n        bool(dataset_id)\n        and dataset_id.startswith(\"HDX_\")\n        and len(dataset_id) == 12\n        and dataset_id[4:].isalnum()\n    )\n</code></pre>"},{"location":"reference/expr/","title":"expr","text":"<p>Narwhals expression used to compute common HDX-MS metrics.</p> <p>Exported metrics from this module are:</p> <ul> <li>centroid_mass</li> <li>max_uptake</li> <li>uptake</li> <li>uptake_sd</li> <li>fd_uptake</li> <li>fd_uptake_sd</li> <li>frac_fd_control</li> <li>frac_fd_control_sd</li> <li>frac_max_uptake</li> <li>frac_max_uptake_sd</li> </ul>"},{"location":"reference/formats/","title":"formats","text":""},{"location":"reference/formats/#formats.FormatSpec","title":"<code>FormatSpec</code>  <code>dataclass</code>","text":"<p>Specification for a data format</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the format.</p> required <code>required_columns</code> <code>list[str]</code> <p>List of columns required to identify this format.</p> required <code>filter_columns</code> <code>list[str]</code> <p>List of columns that can be used to filter data.</p> required <code>converter</code> <code>Callable[[DataFrame], DataFrame]</code> <p>Function to convert a DataFrame to OpenHDX format.</p> required <code>aggregated</code> <code>bool | Callable[[DataFrame], bool]</code> <p>Whether the format is aggregated, or a function to determine if a DataFrame is aggregated.</p> required Source code in <code>hdxms_datasets/formats.py</code> <pre><code>@dataclass(frozen=True)\nclass FormatSpec:\n    \"\"\"Specification for a data format\n\n    Args:\n        name: Name of the format.\n        required_columns: List of columns required to identify this format.\n        filter_columns: List of columns that can be used to filter data.\n        converter: Function to convert a DataFrame to OpenHDX format.\n        aggregated: Whether the format is aggregated, or a function to determine if a DataFrame is aggregated.\n\n    \"\"\"\n\n    name: str\n    required_columns: list[str]\n    filter_columns: list[str]\n    converter: Callable[[nw.DataFrame], nw.DataFrame]\n    aggregated: bool | Callable[[nw.DataFrame], bool]\n\n    def matches(self, df: nw.DataFrame) -&gt; bool:\n        \"\"\"Check if a DataFrame matches this format.\"\"\"\n        df_cols = set(df.columns)\n        required_cols = set(self.required_columns)\n        return required_cols.issubset(df_cols)\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"Convert DataFrame to OpenHDX format.\"\"\"\n        return self.converter(df)\n\n    def is_aggregated(self, df: nw.DataFrame | None = None) -&gt; bool:\n        \"\"\"Check if a DataFrame is aggregated.\"\"\"\n        if self.aggregated is True:\n            return True\n        if callable(self.aggregated):\n            if df is None:\n                raise ValueError(\"DataFrame must be provided to check aggregation\")\n            return self.aggregated(df)\n        return False\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.convert","title":"<code>convert(df)</code>","text":"<p>Convert DataFrame to OpenHDX format.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Convert DataFrame to OpenHDX format.\"\"\"\n    return self.converter(df)\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.is_aggregated","title":"<code>is_aggregated(df=None)</code>","text":"<p>Check if a DataFrame is aggregated.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>def is_aggregated(self, df: nw.DataFrame | None = None) -&gt; bool:\n    \"\"\"Check if a DataFrame is aggregated.\"\"\"\n    if self.aggregated is True:\n        return True\n    if callable(self.aggregated):\n        if df is None:\n            raise ValueError(\"DataFrame must be provided to check aggregation\")\n        return self.aggregated(df)\n    return False\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.matches","title":"<code>matches(df)</code>","text":"<p>Check if a DataFrame matches this format.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>def matches(self, df: nw.DataFrame) -&gt; bool:\n    \"\"\"Check if a DataFrame matches this format.\"\"\"\n    df_cols = set(df.columns)\n    required_cols = set(self.required_columns)\n    return required_cols.issubset(df_cols)\n</code></pre>"},{"location":"reference/formats/#formats.identify_format","title":"<code>identify_format(df)</code>","text":"<p>Identify format from DataFrame columns</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>def identify_format(df: nw.DataFrame) -&gt; Optional[FormatSpec]:\n    \"\"\"Identify format from DataFrame columns\"\"\"\n    for fmt in FORMATS:\n        if fmt.matches(df):\n            return fmt\n    return None\n</code></pre>"},{"location":"reference/loader/","title":"loader","text":""},{"location":"reference/loader/#loader.get_backend","title":"<code>get_backend()</code>","text":"<p>Returns the backend used for data handling.</p> Source code in <code>hdxms_datasets/loader.py</code> <pre><code>def get_backend():\n    \"\"\"\n    Returns the backend used for data handling.\n    \"\"\"\n    try:\n        import polars  # NOQA: F401 # type: ignore[import]\n\n        return \"polars\"\n    except ImportError:\n        pass\n\n    try:\n        import pandas  # NOQA: F401 # type: ignore[import]\n\n        return \"pandas\"\n    except ImportError:\n        pass\n\n    try:\n        import modin  # NOQA: F401 # type: ignore[import]\n\n        return \"modin\"\n    except ImportError:\n        pass\n\n    try:\n        import pyarrow  # NOQA: F401 # type: ignore[import]\n\n        return \"pyarrow\"\n    except ImportError:\n        pass\n\n    raise ImportError(\"No suitable backend found. Please install pandas, polars, pyarrow or modin.\")\n</code></pre>"},{"location":"reference/loader/#loader.load_peptides","title":"<code>load_peptides(peptides, base_dir=Path.cwd(), convert=True, aggregate=None, sort_rows=True, sort_columns=True, drop_null=True)</code>","text":"<p>Load peptides from the data file and return a Narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>Peptides</code> <p>Peptides object containing metadata and file path.</p> required <code>base_dir</code> <code>Path</code> <p>Base directory to resolve relative file paths. Defaults to the current working directory.</p> <code>cwd()</code> <code>convert</code> <code>bool</code> <p>Whether to convert the data to a standard format.</p> <code>True</code> <code>aggregate</code> <code>bool | None</code> <p>Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.</p> <code>None</code> <code>sort_rows</code> <code>bool</code> <p>Whether to sort the rows.</p> <code>True</code> <code>sort_columns</code> <code>bool</code> <p>Whether to sort the columns in a standard order.</p> <code>True</code> <code>drop_null</code> <code>bool</code> <p>Whether to drop columns that are entirely null.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Narwhals DataFrame containing the loaded peptide data.</p> Source code in <code>hdxms_datasets/loader.py</code> <pre><code>def load_peptides(\n    peptides: Peptides,\n    base_dir: Path = Path.cwd(),\n    convert: bool = True,\n    aggregate: bool | None = None,\n    sort_rows: bool = True,\n    sort_columns: bool = True,\n    drop_null: bool = True,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Load peptides from the data file and return a Narwhals DataFrame.\n\n    Args:\n        peptides: Peptides object containing metadata and file path.\n        base_dir: Base directory to resolve relative file paths. Defaults to the current working directory.\n        convert: Whether to convert the data to a standard format.\n        aggregate: Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.\n        sort_rows: Whether to sort the rows.\n        sort_columns: Whether to sort the columns in a standard order.\n        drop_null: Whether to drop columns that are entirely null.\n\n    Returns:\n        A Narwhals DataFrame containing the loaded peptide data.\n\n    \"\"\"\n\n    # Resolve the data file path\n    if peptides.data_file.is_absolute():\n        data_path = peptides.data_file\n    else:\n        data_path = base_dir / peptides.data_file\n\n    # Load the raw data\n    df = read_csv(data_path)\n\n    from hdxms_datasets import process\n\n    df = process.apply_filters(df, **peptides.filters)\n\n    format_spec = FORMAT_LUT.get(peptides.data_format)\n    assert format_spec is not None, f\"Unknown format: {peptides.data_format}\"\n\n    if callable(format_spec.aggregated):\n        is_aggregated = format_spec.aggregated(df)\n    else:\n        is_aggregated = format_spec.aggregated\n\n    # if aggregation is not specified, by default aggregate if the data is not already aggregated\n    if aggregate is None:\n        aggregate = not is_aggregated\n\n    if aggregate and is_aggregated:\n        warnings.warn(\"Data format is pre-aggregated. Aggregation will be skipped.\")\n        aggregate = False\n\n    if not convert and aggregate:\n        warnings.warn(\"Cannot aggregate data without conversion. Aggeregation will be skipped.\")\n        aggregate = False\n\n    if not convert and sort_rows:\n        warnings.warn(\"Cannot sort rows without conversion. Sorting will be skipped.\")\n        sort_rows = False\n\n    if not convert and sort_columns:\n        warnings.warn(\"Cannot sort columns without conversion. Sorting will be skipped.\")\n        sort_columns = False\n\n    if convert:\n        df = format_spec.convert(df)\n\n    if aggregate:\n        df = process.aggregate(df)\n\n    if drop_null:\n        df = process.drop_null_columns(df)\n\n    if sort_rows:\n        df = process.sort_rows(df)\n\n    if sort_columns:\n        df = process.sort_columns(df)\n\n    return df\n</code></pre>"},{"location":"reference/loader/#loader.read_csv","title":"<code>read_csv(source)</code>","text":"<p>Read a CSV file and return a Narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | IO | bytes</code> <p>Source object representing the CSV data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Narwhals DataFrame containing the CSV data.</p> Source code in <code>hdxms_datasets/loader.py</code> <pre><code>def read_csv(source: Path | str | IO | bytes) -&gt; nw.DataFrame:\n    \"\"\"\n    Read a CSV file and return a Narwhals DataFrame.\n\n    Args:\n        source: Source object representing the CSV data.\n\n    Returns:\n        A Narwhals DataFrame containing the CSV data.\n\n    \"\"\"\n    if isinstance(source, str):\n        return nw.read_csv(source, backend=BACKEND)\n    elif isinstance(source, Path):\n        return nw.read_csv(source.as_posix(), backend=BACKEND)\n    elif isinstance(source, bytes):\n        import polars as pl\n\n        return nw.from_native(pl.read_csv(source))\n    else:\n        try:\n            import polars as pl\n\n            return nw.from_native(pl.read_csv(source))\n        except ImportError:\n            pass\n        try:\n            import pandas as pd\n\n            return nw.from_native(pd.read_csv(source))  # type: ignore\n        except ImportError:\n            raise ValueError(\"No suitable backend found for reading file-like objects or bytes.\")\n</code></pre>"},{"location":"reference/models/","title":"models","text":""},{"location":"reference/models/#models.DataRepository","title":"<code>DataRepository</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about the data repository where the source data is published</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class DataRepository(BaseModel):\n    \"\"\"Information about the data repository where the source data is published\"\"\"\n\n    name: Annotated[str, Field(..., description=\"Repository name\")]  # ie Pride, Zenodo,\n    url: Annotated[Optional[HttpUrl], Field(None, description=\"Repository URL\")]\n    identifier: Annotated[Optional[str], Field(None, description=\"Repository entry identifier\")]\n    doi: Annotated[Optional[str], Field(None, description=\"Repository DOI\")]\n    description: Annotated[Optional[str], Field(None, description=\"Repository description\")]\n</code></pre>"},{"location":"reference/models/#models.DeuterationType","title":"<code>DeuterationType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Experimental Deuteration Type of the peptide</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class DeuterationType(str, Enum):\n    \"\"\"Experimental Deuteration Type of the peptide\"\"\"\n\n    partially_deuterated = \"partially_deuterated\"\n    fully_deuterated = \"fully_deuterated\"\n    non_deuterated = \"non_deuterated\"\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet","title":"<code>HDXDataSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>HDX-MS dataset containing multiple states</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class HDXDataSet(BaseModel):\n    \"\"\"HDX-MS dataset containing multiple states\"\"\"\n\n    # Basic information\n    description: Annotated[Optional[str], Field(None, description=\"Dataset description\")]\n\n    states: list[State] = Field(description=\"List of HDX states in the dataset\")\n    structure: Annotated[Structure, Field(description=\"Structural model file path\")]\n    protein_identifiers: Annotated[\n        ProteinIdentifiers, Field(description=\"Protein identifiers (UniProt, etc.)\")\n    ]\n    metadata: Annotated[DatasetMetadata, Field(description=\"Dataset metadata\")]\n    file_hash: Annotated[\n        Optional[str], Field(None, init=False, description=\"Hash of the files in the dataset\")\n    ]\n\n    @model_validator(mode=\"after\")\n    def compute_file_hash(self):\n        \"\"\"Compute a hash of the dataset based on its data files\"\"\"\n        if any(not p.exists() for p in self.data_files):\n            self.file_hash = None\n            return self\n\n        self.file_hash = self.hash_files()[:16]  # Shorten to 16 characters\n\n        return self\n\n    def hash_files(self) -&gt; str:\n        return hash_files(self.data_files)  # Ensure files are sorted and hashed consistently\n\n    def validate_file_integrity(self) -&gt; bool:\n        \"\"\"Match hash of files with the stored hash\"\"\"\n        if self.file_hash is None:\n            return False\n        current_hash = self.hash_files()\n        return current_hash.startswith(self.file_hash)\n\n    def get_state(self, state: str | int) -&gt; State:\n        \"\"\"Get a specific state by name or index\"\"\"\n        if isinstance(state, int):\n            return self.states[state]\n        elif isinstance(state, str):\n            for s in self.states:\n                if s.name == state:\n                    return s\n        raise ValueError(f\"State '{state}' not found in dataset.\")\n\n    @property\n    def data_files(self) -&gt; list[Path]:\n        \"\"\"List of all data files in the dataset\"\"\"\n        return sorted(set(extract_values_by_types(self, Path)))\n\n    @classmethod\n    def from_json(\n        cls,\n        json_str: str,\n        dataset_root: Optional[Path] = None,\n    ) -&gt; HDXDataSet:\n        \"\"\"Load dataset from JSON string\n\n        Args:\n            json_str: JSON string representing the dataset\n            dataset_root: Optional root directory to resolve relative paths\n\n        Returns:\n            HDXDataSet instance.\n\n        \"\"\"\n        if dataset_root is None:\n            context = {}\n        else:\n            context = {\"dataset_root\": dataset_root}\n        return cls.model_validate_json(json_str, context=context)\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.data_files","title":"<code>data_files: list[Path]</code>  <code>property</code>","text":"<p>List of all data files in the dataset</p>"},{"location":"reference/models/#models.HDXDataSet.compute_file_hash","title":"<code>compute_file_hash()</code>","text":"<p>Compute a hash of the dataset based on its data files</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef compute_file_hash(self):\n    \"\"\"Compute a hash of the dataset based on its data files\"\"\"\n    if any(not p.exists() for p in self.data_files):\n        self.file_hash = None\n        return self\n\n    self.file_hash = self.hash_files()[:16]  # Shorten to 16 characters\n\n    return self\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.from_json","title":"<code>from_json(json_str, dataset_root=None)</code>  <code>classmethod</code>","text":"<p>Load dataset from JSON string</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>JSON string representing the dataset</p> required <code>dataset_root</code> <code>Optional[Path]</code> <p>Optional root directory to resolve relative paths</p> <code>None</code> <p>Returns:</p> Type Description <code>HDXDataSet</code> <p>HDXDataSet instance.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    json_str: str,\n    dataset_root: Optional[Path] = None,\n) -&gt; HDXDataSet:\n    \"\"\"Load dataset from JSON string\n\n    Args:\n        json_str: JSON string representing the dataset\n        dataset_root: Optional root directory to resolve relative paths\n\n    Returns:\n        HDXDataSet instance.\n\n    \"\"\"\n    if dataset_root is None:\n        context = {}\n    else:\n        context = {\"dataset_root\": dataset_root}\n    return cls.model_validate_json(json_str, context=context)\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.get_state","title":"<code>get_state(state)</code>","text":"<p>Get a specific state by name or index</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def get_state(self, state: str | int) -&gt; State:\n    \"\"\"Get a specific state by name or index\"\"\"\n    if isinstance(state, int):\n        return self.states[state]\n    elif isinstance(state, str):\n        for s in self.states:\n            if s.name == state:\n                return s\n    raise ValueError(f\"State '{state}' not found in dataset.\")\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.validate_file_integrity","title":"<code>validate_file_integrity()</code>","text":"<p>Match hash of files with the stored hash</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def validate_file_integrity(self) -&gt; bool:\n    \"\"\"Match hash of files with the stored hash\"\"\"\n    if self.file_hash is None:\n        return False\n    current_hash = self.hash_files()\n    return current_hash.startswith(self.file_hash)\n</code></pre>"},{"location":"reference/models/#models.PeptideFormat","title":"<code>PeptideFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Format of the peptide data</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class PeptideFormat(str, Enum):\n    \"\"\"Format of the peptide data\"\"\"\n\n    DynamX_v3_state = \"DynamX_v3_state\"\n    DynamX_v3_cluster = \"DynamX_v3_cluster\"\n    DynamX_vx_state = \"DynamX_vx_state\"\n    HDExaminer_v3 = \"HDExaminer_v3\"\n    OpenHDX = \"OpenHDX\"\n\n    @classmethod\n    def identify(cls, df: nw.DataFrame) -&gt; PeptideFormat | None:\n        \"\"\"Identify format from DataFrame\"\"\"\n        from hdxms_datasets.formats import identify_format\n\n        fmt = identify_format(df)\n        if fmt:\n            return cls(fmt.name)\n        return None\n</code></pre>"},{"location":"reference/models/#models.PeptideFormat.identify","title":"<code>identify(df)</code>  <code>classmethod</code>","text":"<p>Identify format from DataFrame</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@classmethod\ndef identify(cls, df: nw.DataFrame) -&gt; PeptideFormat | None:\n    \"\"\"Identify format from DataFrame\"\"\"\n    from hdxms_datasets.formats import identify_format\n\n    fmt = identify_format(df)\n    if fmt:\n        return cls(fmt.name)\n    return None\n</code></pre>"},{"location":"reference/models/#models.Peptides","title":"<code>Peptides</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about HDX-MS peptides</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class Peptides(BaseModel):\n    \"\"\"Information about HDX-MS peptides\"\"\"\n\n    data_file: DataFilePath\n    data_format: Annotated[PeptideFormat, Field(description=\"Data format (e.g., OpenHDX)\")]\n    deuteration_type: Annotated[\n        DeuterationType, Field(description=\"Type of the peptide (e.g., fully_deuterated)\")\n    ]\n    entity_id: Annotated[\n        Optional[str],\n        Field(description=\"Entity identifier if multiple entities are present in the structure\"),\n    ] = None\n    chain: Annotated[Optional[list[str]], Field(description=\"Chain identifiers\")] = None\n    filters: Annotated[\n        dict[str, ValueType | list[ValueType]],\n        Field(default_factory=dict, description=\"Filters applied to the data\"),\n    ]\n    pH: Annotated[\n        Optional[float], Field(description=\"pH (read, uncorrected) of the experiment\")\n    ] = None\n    temperature: Annotated[Optional[float], Field(description=\"Temperature in Kelvin\")] = None\n    d_percentage: Annotated[Optional[float], Field(description=\"Deuteration percentage\")] = None\n    ionic_strength: Annotated[Optional[float], Field(description=\"Ionic strength in Molar\")] = None\n\n    def load(\n        self,\n        convert: bool = True,\n        aggregate: bool | None = None,\n        sort_rows: bool = True,\n        sort_columns: bool = True,\n        drop_null: bool = True,\n    ) -&gt; nw.DataFrame:\n        \"\"\"Load the peptides from the data file\n\n        Args:\n            convert: Whether to convert the data to a standard format.\n            aggregate: Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.\n            sort_rows: Whether to sort the rows.\n            sort_columns: Whether to sort the columns in a standard order.\n            drop_null: Whether to drop columns that are entirely null.\n\n        \"\"\"\n        if self.data_file.exists():\n            from hdxms_datasets.loader import load_peptides\n\n            return load_peptides(\n                self,\n                convert=convert,\n                aggregate=aggregate,\n                sort_rows=sort_rows,\n                sort_columns=sort_columns,\n                drop_null=drop_null,\n            )\n        else:\n            raise FileNotFoundError(f\"Data file {self.data_file} does not exist.\")\n</code></pre>"},{"location":"reference/models/#models.Peptides.load","title":"<code>load(convert=True, aggregate=None, sort_rows=True, sort_columns=True, drop_null=True)</code>","text":"<p>Load the peptides from the data file</p> <p>Parameters:</p> Name Type Description Default <code>convert</code> <code>bool</code> <p>Whether to convert the data to a standard format.</p> <code>True</code> <code>aggregate</code> <code>bool | None</code> <p>Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.</p> <code>None</code> <code>sort_rows</code> <code>bool</code> <p>Whether to sort the rows.</p> <code>True</code> <code>sort_columns</code> <code>bool</code> <p>Whether to sort the columns in a standard order.</p> <code>True</code> <code>drop_null</code> <code>bool</code> <p>Whether to drop columns that are entirely null.</p> <code>True</code> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def load(\n    self,\n    convert: bool = True,\n    aggregate: bool | None = None,\n    sort_rows: bool = True,\n    sort_columns: bool = True,\n    drop_null: bool = True,\n) -&gt; nw.DataFrame:\n    \"\"\"Load the peptides from the data file\n\n    Args:\n        convert: Whether to convert the data to a standard format.\n        aggregate: Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.\n        sort_rows: Whether to sort the rows.\n        sort_columns: Whether to sort the columns in a standard order.\n        drop_null: Whether to drop columns that are entirely null.\n\n    \"\"\"\n    if self.data_file.exists():\n        from hdxms_datasets.loader import load_peptides\n\n        return load_peptides(\n            self,\n            convert=convert,\n            aggregate=aggregate,\n            sort_rows=sort_rows,\n            sort_columns=sort_columns,\n            drop_null=drop_null,\n        )\n    else:\n        raise FileNotFoundError(f\"Data file {self.data_file} does not exist.\")\n</code></pre>"},{"location":"reference/models/#models.ProteinIdentifiers","title":"<code>ProteinIdentifiers</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>General protein information</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class ProteinIdentifiers(BaseModel):\n    \"\"\"General protein information\"\"\"\n\n    uniprot_accession_number: Annotated[Optional[str], Field(None, description=\"UniProt ID\")] = None\n    uniprot_entry_name: Annotated[Optional[str], Field(None, description=\"UniProt entry name\")] = (\n        None\n    )\n    protein_name: Annotated[Optional[str], Field(None, description=\"Recommended protein name\")] = (\n        None\n    )\n</code></pre>"},{"location":"reference/models/#models.ProteinState","title":"<code>ProteinState</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Protein information for a specific state</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class ProteinState(BaseModel):\n    \"\"\"Protein information for a specific state\"\"\"\n\n    sequence: Annotated[str | list[str], Field(description=\"Amino acid sequence\")]\n    n_term: Annotated[int, Field(description=\"N-terminal residue number\")]\n    c_term: Annotated[int, Field(description=\"C-terminal residue number\")]\n    mutations: Annotated[Optional[list[str]], Field(description=\"List of mutations\")] = None\n    oligomeric_state: Annotated[Optional[int], Field(description=\"Oligomeric state\")] = None\n    ligand: Annotated[Optional[str], Field(description=\"Bound ligand information\")] = None\n\n    @model_validator(mode=\"after\")\n    def check_sequence(self):\n        if len(self.sequence) != self.c_term - self.n_term + 1:\n            raise ValueError(\"Sequence length does not match N-term and C-term residue numbers.\")\n        return self\n</code></pre>"},{"location":"reference/models/#models.Publication","title":"<code>Publication</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Publication information</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class Publication(BaseModel):\n    \"\"\"Publication information\"\"\"\n\n    title: Optional[str] = None\n    authors: Optional[List[str]] = None\n    journal: Optional[str] = None\n    year: Optional[int] = None\n    doi: Optional[str] = None\n    pmid: Optional[str] = None\n    url: Optional[str] = None\n</code></pre>"},{"location":"reference/models/#models.State","title":"<code>State</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about HDX-MS state</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class State(BaseModel):\n    \"\"\"Information about HDX-MS state\"\"\"\n\n    name: Annotated[str, Field(description=\"State name\")]\n    peptides: list[Peptides] = Field(..., description=\"List of peptides in this state\")\n    description: Annotated[str, Field(description=\"State description\")] = \"\"  # TODO max length?\n    protein_state: ProteinState = Field(..., description=\"Protein information for this state\")\n</code></pre>"},{"location":"reference/models/#models.Structure","title":"<code>Structure</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structural model file information</p> <p>Residues or protein chains may have different numbering/labels depending on if they are the assigned labels by the author of the structure ('auth') or renumbered by the RCSB PDB.</p> <p>If your HDX data uses the author numbering/labels, set <code>auth_residue_numbers</code> and/or <code>auth_chain_labels</code> to True.</p> <p>You can also offset the residue numbering by setting <code>residue_offset</code>. For example, if your add an N-terminal his tag and renumber to start at 1 for the extended sequence.</p> <p>If you use both author numbers and offset, the offset is applied first and then translated to canonical residue numbers.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class Structure(BaseModel):\n    \"\"\"Structural model file information\n\n    Residues or protein chains may have different numbering/labels depending on if they are\n    the assigned labels by the author of the structure ('auth') or renumbered by the RCSB PDB.\n\n    If your HDX data uses the author numbering/labels, set `auth_residue_numbers` and/or\n    `auth_chain_labels` to True.\n\n    You can also offset the residue numbering by setting `residue_offset`. For example, if your add\n    an N-terminal his tag and renumber to start at 1 for the extended sequence.\n\n    If you use both author numbers and offset, the offset is applied first and then translated to\n    canonical residue numbers.\n\n    \"\"\"\n\n    data_file: DataFilePath\n    format: Annotated[str, Field(description=\"Format of the structure file (e.g., PDB, mmCIF)\")]\n    description: Annotated[Optional[str], Field(description=\"Description of the structure\")] = None\n\n    # source database identifiers\n    pdb_id: Annotated[Optional[str], Field(None, description=\"RCSB PDB ID\")] = None\n    alphafold_id: Annotated[Optional[str], Field(None, description=\"AlphaFold ID\")] = None\n\n    auth_residue_numbers: Annotated[\n        bool, Field(default=False, description=\"Use author residue numbers\")\n    ] = False\n    auth_chain_labels: Annotated[\n        bool, Field(default=False, description=\"Use author chain labels\")\n    ] = False\n\n    label_auth_mapping: Annotated[\n        Optional[dict[tuple[str, str], tuple[str, str]]],\n        Field(init=False, description=\"Maps author residue numbers to canonical residue numbers\"),\n    ] = None\n\n    def pdbemolstar_custom_data(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Returns a dictionary with custom data for PDBeMolstar visualization.\n        \"\"\"\n\n        if self.format in [\"bcif\"]:\n            binary = True\n        else:\n            binary = False\n\n        if self.data_file.is_file():\n            data = self.data_file.read_bytes()\n        else:\n            raise ValueError(f\"Path {self.data_file} is not a file.\")\n\n        return {\n            \"data\": data,\n            \"format\": self.format,\n            \"binary\": binary,\n        }\n\n    def get_auth_residue_mapping(self) -&gt; dict[tuple[str, str], tuple[str, str]]:\n        \"\"\"Create a mapping from author residue numbers to RCSB residue numbers.\"\"\"\n\n        if self.label_auth_mapping is not None:\n            return self.label_auth_mapping\n\n        else:\n            if self.format.lower() not in [\"cif\", \"mmcif\"]:\n                raise ValueError(\"Author residue number mapping is only supported for mmCIF files.\")\n\n            mapping = residue_number_mapping(self.data_file)\n            self.label_auth_mapping = mapping\n            return mapping\n\n    @property\n    def residue_name(self) -&gt; str:\n        \"\"\"\n        Returns the residue name based on whether auth residue numbers are used.\n        \"\"\"\n        return \"auth_residue_number\" if self.auth_residue_numbers else \"residue_number\"\n\n    @property\n    def chain_name(self) -&gt; str:\n        \"\"\"\n        Returns the chain name based on whether auth chain labels are used.\n\n        Note that 'struct_asym_id' used in PDBeMolstar is equivalent to\n        'label_asym_id' in mmCIF.\n\n        \"\"\"\n        return \"auth_asym_id\" if self.auth_chain_labels else \"struct_asym_id\"\n\n    def to_biopython(self) -&gt; BioStructure:\n        \"\"\"Load the structure using Biopython\"\"\"\n        try:\n            from Bio.PDB.PDBParser import PDBParser\n            from Bio.PDB.MMCIFParser import MMCIFParser\n        except ImportError:\n            raise ImportError(\"Biopython is required to load structures.\")\n\n        if self.format.lower() in [\"pdb\"]:\n            parser = PDBParser(QUIET=True)\n        elif self.format.lower() in [\"cif\", \"mmcif\"]:\n            parser = MMCIFParser(QUIET=True)\n        else:\n            raise ValueError(f\"Unsupported structure format: {self.format}\")\n\n        structure = parser.get_structure(self.pdb_id or \"structure\", self.data_file)\n        assert structure is not None\n\n        return structure\n</code></pre>"},{"location":"reference/models/#models.Structure.chain_name","title":"<code>chain_name: str</code>  <code>property</code>","text":"<p>Returns the chain name based on whether auth chain labels are used.</p> <p>Note that 'struct_asym_id' used in PDBeMolstar is equivalent to 'label_asym_id' in mmCIF.</p>"},{"location":"reference/models/#models.Structure.residue_name","title":"<code>residue_name: str</code>  <code>property</code>","text":"<p>Returns the residue name based on whether auth residue numbers are used.</p>"},{"location":"reference/models/#models.Structure.get_auth_residue_mapping","title":"<code>get_auth_residue_mapping()</code>","text":"<p>Create a mapping from author residue numbers to RCSB residue numbers.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def get_auth_residue_mapping(self) -&gt; dict[tuple[str, str], tuple[str, str]]:\n    \"\"\"Create a mapping from author residue numbers to RCSB residue numbers.\"\"\"\n\n    if self.label_auth_mapping is not None:\n        return self.label_auth_mapping\n\n    else:\n        if self.format.lower() not in [\"cif\", \"mmcif\"]:\n            raise ValueError(\"Author residue number mapping is only supported for mmCIF files.\")\n\n        mapping = residue_number_mapping(self.data_file)\n        self.label_auth_mapping = mapping\n        return mapping\n</code></pre>"},{"location":"reference/models/#models.Structure.pdbemolstar_custom_data","title":"<code>pdbemolstar_custom_data()</code>","text":"<p>Returns a dictionary with custom data for PDBeMolstar visualization.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def pdbemolstar_custom_data(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Returns a dictionary with custom data for PDBeMolstar visualization.\n    \"\"\"\n\n    if self.format in [\"bcif\"]:\n        binary = True\n    else:\n        binary = False\n\n    if self.data_file.is_file():\n        data = self.data_file.read_bytes()\n    else:\n        raise ValueError(f\"Path {self.data_file} is not a file.\")\n\n    return {\n        \"data\": data,\n        \"format\": self.format,\n        \"binary\": binary,\n    }\n</code></pre>"},{"location":"reference/models/#models.Structure.to_biopython","title":"<code>to_biopython()</code>","text":"<p>Load the structure using Biopython</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def to_biopython(self) -&gt; BioStructure:\n    \"\"\"Load the structure using Biopython\"\"\"\n    try:\n        from Bio.PDB.PDBParser import PDBParser\n        from Bio.PDB.MMCIFParser import MMCIFParser\n    except ImportError:\n        raise ImportError(\"Biopython is required to load structures.\")\n\n    if self.format.lower() in [\"pdb\"]:\n        parser = PDBParser(QUIET=True)\n    elif self.format.lower() in [\"cif\", \"mmcif\"]:\n        parser = MMCIFParser(QUIET=True)\n    else:\n        raise ValueError(f\"Unsupported structure format: {self.format}\")\n\n    structure = parser.get_structure(self.pdb_id or \"structure\", self.data_file)\n    assert structure is not None\n\n    return structure\n</code></pre>"},{"location":"reference/models/#models.extract_values_by_types","title":"<code>extract_values_by_types(obj, target_types)</code>","text":"<p>Recursively extract all values of specified type(s) from a nested structure. This function can handle Pydantic models, lists, tuples, sets, and dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Pydantic model instance or any nested structure</p> required <code>target_types</code> <code>Type | tuple[Type, ...]</code> <p>Single type or tuple of types to search for</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of all values matching any of the target types</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def extract_values_by_types(obj: Any, target_types: Type | tuple[Type, ...]) -&gt; list[Any]:\n    \"\"\"\n    Recursively extract all values of specified type(s) from a nested structure.\n    This function can handle Pydantic models, lists, tuples, sets, and dictionaries.\n\n    Args:\n        obj: Pydantic model instance or any nested structure\n        target_types: Single type or tuple of types to search for\n\n    Returns:\n        List of all values matching any of the target types\n    \"\"\"\n    values = []\n\n    # Normalize target_types to tuple\n    if not isinstance(target_types, tuple):\n        target_types = (target_types,)\n\n    # Check if current object is of any target type\n    if isinstance(obj, target_types):\n        values.append(obj)\n\n    elif isinstance(obj, BaseModel):\n        # Iterate through all field values in the Pydantic model\n        for field_name, field_value in obj.__dict__.items():\n            values.extend(extract_values_by_types(field_value, target_types))\n\n    elif isinstance(obj, (list, tuple, set)):\n        # Handle sequences\n        for item in obj:\n            values.extend(extract_values_by_types(item, target_types))\n\n    elif isinstance(obj, dict):\n        # Handle dictionaries (both keys and values)\n        for key, value in obj.items():\n            values.extend(extract_values_by_types(key, target_types))\n            values.extend(extract_values_by_types(value, target_types))\n\n    return values\n</code></pre>"},{"location":"reference/models/#models.hash_files","title":"<code>hash_files(data_files)</code>","text":"<p>Compute a hash of all data files in the dataset</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def hash_files(data_files: Iterable[Path]) -&gt; str:\n    \"\"\"Compute a hash of all data files in the dataset\"\"\"\n    hash_obj = hashlib.sha256()\n    files = sorted(data_files, key=lambda p: p.as_posix())  # Sort to ensure consistent order\n    for f in files:\n        if f.suffix in TEXT_FILE_FORMATS:\n            content = f.read_text(encoding=\"utf-8\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n            hash_obj.update(content.encode(\"utf-8\"))\n        else:\n            hash_obj.update(f.read_bytes())\n\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"reference/models/#models.residue_number_mapping","title":"<code>residue_number_mapping(cif_path, chain=True, residue=True)</code>","text":"<p>Create a mapping from author residue numbers to RCSB residue numbers from an mmCIF file.</p> <p>Parameters:</p> Name Type Description Default <code>cif_path</code> <code>Path</code> <p>Path to the mmCIF file.</p> required <code>chain</code> <p>Whether to include chain mapping.</p> <code>True</code> <code>residue</code> <p>Whether to include residue number mapping.</p> <code>True</code> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def residue_number_mapping(\n    cif_path: Path, chain=True, residue=True\n) -&gt; dict[tuple[str, str], tuple[str, str]]:\n    \"\"\"Create a mapping from author residue numbers to RCSB residue numbers from an mmCIF file.\n\n    Args:\n        cif_path: Path to the mmCIF file.\n        chain: Whether to include chain mapping.\n        residue: Whether to include residue number mapping.\n\n\n    \"\"\"\n    try:\n        from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n    except ImportError:\n        raise ImportError(\"Biopython is required for residue number mapping from mmCIF files.\")\n\n    mm = MMCIF2Dict(cif_path)\n\n    label_asym = mm[\"_atom_site.label_asym_id\"]\n    if chain:\n        auth_asym = mm.get(\"_atom_site.auth_asym_id\", label_asym)\n    else:\n        auth_asym = label_asym\n\n    label_seq = mm.get(\"_atom_site.label_seq_id\", [])\n    if residue:\n        auth_seq = mm.get(\"_atom_site.auth_seq_id\", label_seq)\n    else:\n        auth_seq = label_seq\n\n    # maps author chain/residue numbers to PDB chain/residue numbers\n    mapping = {\n        (a_asym, a_seq): (l_asym, l_seq)\n        for l_asym, l_seq, a_asym, a_seq in zip(label_asym, label_seq, auth_asym, auth_seq)\n        if l_asym != a_asym or l_seq != a_seq  # dont include identical mappings\n    }\n\n    return mapping\n</code></pre>"},{"location":"reference/models/#models.serialize_datafile_path","title":"<code>serialize_datafile_path(x, info)</code>","text":"<p>Pydantic serializer to convert paths to relative paths based on context</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def serialize_datafile_path(x: Path, info: ValidationInfo) -&gt; str:\n    \"\"\"Pydantic serializer to convert paths to relative paths based on context\"\"\"\n    context = info.context\n    if context and \"dataset_root\" in context:\n        relpath = x.relative_to(Path(context[\"dataset_root\"]))\n        return relpath.as_posix()\n    return x.as_posix()\n</code></pre>"},{"location":"reference/models/#models.validate_datafile_path","title":"<code>validate_datafile_path(x, info)</code>","text":"<p>Pydantic validator to resolve relative paths based on context</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def validate_datafile_path(x: Path, info: ValidationInfo):\n    \"\"\"Pydantic validator to resolve relative paths based on context\"\"\"\n    context = info.context\n    if context and \"dataset_root\" in context and not x.is_absolute():\n        root = Path(context[\"dataset_root\"])\n        x = root / x\n    return x\n</code></pre>"},{"location":"reference/plot/","title":"plot","text":""},{"location":"reference/plot/#plot.find_wrap","title":"<code>find_wrap(peptides, margin=4, step=5, wrap_limit=200)</code>","text":"<p>Find the minimum wrap value for a given list of intervals.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>Dataframe with columns 'start' and 'end' representing intervals.</p> required <code>margin</code> <code>int</code> <p>The margin applied to the wrap value. Defaults to 4.</p> <code>4</code> <code>step</code> <code>int</code> <p>The increment step for the wrap value. Defaults to 5.</p> <code>5</code> <code>wrap_limit</code> <code>int</code> <p>The maximum allowed wrap value. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <code>int</code> <p>The minimum wrap value that does not overlap with any intervals.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def find_wrap(\n    peptides: pl.DataFrame,\n    margin: int = 4,\n    step: int = 5,\n    wrap_limit: int = 200,\n) -&gt; int:\n    \"\"\"\n    Find the minimum wrap value for a given list of intervals.\n\n    Args:\n        peptides: Dataframe with columns 'start' and 'end' representing intervals.\n        margin: The margin applied to the wrap value. Defaults to 4.\n        step: The increment step for the wrap value. Defaults to 5.\n        wrap_limit: The maximum allowed wrap value. Defaults to 200.\n\n    Returns:\n        The minimum wrap value that does not overlap with any intervals.\n    \"\"\"\n    wrap = step\n\n    while True:\n        peptides_y = peptides.with_columns(\n            (pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"y\") % wrap)\n        )\n\n        no_overlaps = True\n        for name, df in peptides_y.group_by(\"y\", maintain_order=True):\n            overlaps = (np.array(df[\"end\"]) + 1 + margin)[:-1] &gt;= np.array(df[\"start\"])[1:]\n            if np.any(overlaps):\n                no_overlaps = False\n                break\n                # return wrap\n\n        wrap += step\n        if wrap &gt; wrap_limit:\n            return wrap_limit  # Return the maximum wrap limit if no valid wrap found\n        elif no_overlaps:\n            return wrap\n</code></pre>"},{"location":"reference/plot/#plot.peptide_rectangles","title":"<code>peptide_rectangles(peptides, wrap=None)</code>","text":"<p>Given a DataFrame with 'start' and 'end' columns, each describing a peptide range, this function computes the corresponding rectangle coordinates for visualization.</p> <p>Typicall used for Altair plotting. The rectangles will be stacked vertically based on the <code>wrap</code> parameter. Horizontally, each rectangle spans from <code>start - 0.5</code> to <code>end + 0.5</code>.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information with 'start' and 'end' columns.</p> required <code>wrap</code> <code>int | None</code> <p>The number of peptides to stack vertically before wrapping to the next row.   If <code>None</code>, the function will compute an optimal wrap value to avoid overlaps.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns 'x', 'x2', 'y', and 'y2' representing the rectangle coordinates.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def peptide_rectangles(peptides: pl.DataFrame, wrap: int | None = None) -&gt; pl.DataFrame:\n    \"\"\"\n    Given a DataFrame with 'start' and 'end' columns, each describing a peptide range,\n    this function computes the corresponding rectangle coordinates for visualization.\n\n    Typicall used for Altair plotting. The rectangles will be stacked vertically based on the `wrap` parameter.\n    Horizontally, each rectangle spans from `start - 0.5` to `end + 0.5`.\n\n    Args:\n        peptides: DataFrame containing peptide information with 'start' and 'end' columns.\n        wrap: The number of peptides to stack vertically before wrapping to the next row.\n              If `None`, the function will compute an optimal wrap value to avoid overlaps.\n\n    Returns:\n        A DataFrame with columns 'x', 'x2', 'y', and 'y2' representing the rectangle coordinates.\n\n    \"\"\"\n    wrap = find_wrap(peptides, step=1) if wrap is None else wrap\n    columns = [\n        (pl.col(\"start\") - 0.5).alias(\"x\"),\n        (pl.col(\"end\") + 0.5).alias(\"x2\"),\n        (wrap - (pl.col(\"idx\") % wrap)).alias(\"y\"),\n    ]\n\n    rectangles = (\n        peptides[\"start\", \"end\"]\n        .with_row_index(\"idx\")\n        .with_columns(columns)\n        .with_columns((pl.col(\"y\") - 1).alias(\"y2\"))\n    )\n\n    return rectangles\n</code></pre>"},{"location":"reference/plot/#plot.plot_peptides","title":"<code>plot_peptides(peptides, value='value', value_sd=None, colormap='viridis', domain=None, bad_color='#8c8c8c', N=256, label=None, width='container', height=350, wrap=None, fill_nan=True)</code>","text":"<p>Create an altair chart visualizing peptides as colored rectangles.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information with 'start', 'end', and <code>value</code> columns.</p> required <code>value</code> <code>str</code> <p>The column name in <code>peptides</code> to use for coloring the rectangles.</p> <code>'value'</code> <code>value_sd</code> <code>str | None</code> <p>Optional column name for standard deviation of <code>value</code>, used in tooltips.</p> <code>None</code> <code>colormap</code> <code>str | Colormap</code> <p>Colormap to use for coloring the rectangles. Can be a string or a Colormap object.</p> <code>'viridis'</code> <code>domain</code> <code>tuple[float | None, float | None] | None</code> <p>Tuple specifying the (min, max) values for the colormap. If <code>None</code>, uses min and max of <code>value</code>.</p> <code>None</code> <code>bad_color</code> <code>str</code> <p>Color to use for invalid or NaN values.</p> <code>'#8c8c8c'</code> <code>N</code> <code>int</code> <p>Number of discrete colors to generate from the colormap.</p> <code>256</code> <code>label</code> <code>str | None</code> <p>Label for the color legend. If <code>None</code>, uses a title-cased version of <code>value</code>.</p> <code>None</code> <code>width</code> <code>str | int</code> <p>Width of the chart. Can be an integer or 'container' for responsive width.</p> <code>'container'</code> <code>height</code> <code>str | int</code> <p>Height of the chart in pixels.</p> <code>350</code> <code>wrap</code> <code>int | None</code> <p>Number of peptides to stack vertically before wrapping to the next row. If <code>None</code>, computes an optimal wrap value.</p> <code>None</code> <code>fill_nan</code> <code>bool</code> <p>Whether to fill NaN values in <code>peptides</code> with None to avoid serialization issues.</p> <code>True</code> <p>Returns:</p> Type Description <code>Chart</code> <p>An Altair Chart object visualizing the peptides.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def plot_peptides(\n    peptides: pl.DataFrame,\n    value: str = \"value\",\n    value_sd: str | None = None,\n    colormap: str | Colormap = \"viridis\",\n    domain: tuple[float | None, float | None] | None = None,\n    bad_color: str = \"#8c8c8c\",\n    N: int = 256,\n    label: str | None = None,\n    width: str | int = \"container\",\n    height: str | int = 350,\n    wrap: int | None = None,\n    fill_nan: bool = True,\n) -&gt; alt.Chart:\n    \"\"\"\n    Create an altair chart visualizing peptides as colored rectangles.\n\n    Args:\n        peptides: DataFrame containing peptide information with 'start', 'end', and `value` columns.\n        value: The column name in `peptides` to use for coloring the rectangles.\n        value_sd: Optional column name for standard deviation of `value`, used in tooltips.\n        colormap: Colormap to use for coloring the rectangles. Can be a string or a Colormap object.\n        domain: Tuple specifying the (min, max) values for the colormap. If `None`, uses min and max of `value`.\n        bad_color: Color to use for invalid or NaN values.\n        N: Number of discrete colors to generate from the colormap.\n        label: Label for the color legend. If `None`, uses a title-cased version of `value`.\n        width: Width of the chart. Can be an integer or 'container' for responsive width.\n        height: Height of the chart in pixels.\n        wrap: Number of peptides to stack vertically before wrapping to the next row. If `None`, computes an optimal wrap value.\n        fill_nan: Whether to fill NaN values in `peptides` with None to avoid serialization issues.\n\n    Returns:\n        An Altair Chart object visualizing the peptides.\n\n    \"\"\"\n\n    if not unique_peptides(peptides):\n        raise ValueError(\"Peptides must be unique by 'start' and 'end' columns.\")\n\n    if fill_nan:\n        # nan values can cause problems in serialization\n        peptides = peptides.fill_nan(None)\n\n    value_sd = value_sd or f\"{value}_sd\"\n    colormap = Colormap(colormap) if isinstance(colormap, str) else colormap\n    if domain is None:\n        domain = (None, None)\n    vmin = domain[0] if domain[0] is not None else peptides[value].min()  # type: ignore\n    vmax = domain[1] if domain[1] is not None else peptides[value].max()  # type: ignore\n\n    scale = alt.Scale(domain=(vmin, vmax), range=colormap.to_altair(N=N))  # type: ignore\n    label = label or value.replace(\"_\", \" \").title()\n\n    if value_sd in peptides.columns:\n        tooltip_value = []\n        for v, v_sd in zip(peptides[value], peptides[value_sd]):\n            if v is not None and v_sd is not None:\n                tooltip_value.append(\n                    f\"{v:.2f} \\u00b1 {v_sd:.2f}\"  # type: ignore\n                )\n            else:\n                tooltip_value.append(\"NaN\")\n    else:\n        tooltip_value = [f\"{value:.2f}\" if value is not None else \"\" for value in peptides[value]]\n\n    rectangles = peptide_rectangles(peptides, wrap=wrap)\n    peptide_source = peptides.join(rectangles, on=[\"start\", \"end\"], how=\"left\").with_columns(\n        pl.col(value), pl.Series(tooltip_value).alias(\"tooltip_value\")\n    )\n\n    invalid = {\"color\": {\"value\": bad_color}}\n    peptide_chart = (\n        alt.Chart(peptide_source)\n        .mark_rect(\n            stroke=\"black\",\n        )\n        .encode(\n            x=alt.X(\"x:Q\", title=\"Residue Number\"),\n            y=alt.Y(\"y:Q\", title=\"\", axis=alt.Axis(ticks=False, domain=False, labels=False)),\n            x2=alt.X2(\"x2:Q\"),\n            y2=alt.Y2(\"y2:Q\"),\n            tooltip=[\n                alt.Tooltip(\"idx:Q\", title=\"Index\"),\n                alt.Tooltip(\"start:Q\", title=\"Start\"),\n                alt.Tooltip(\"end:Q\", title=\"End\"),\n                alt.Tooltip(\"sequence:N\", title=\"Sequence\"),\n                alt.Tooltip(\"tooltip_value:N\", title=label),\n            ],\n            color=alt.Color(f\"{value}:Q\", scale=scale, title=label),\n        )\n        .configure_scale(invalid=invalid)\n    )\n\n    return peptide_chart.properties(height=height, width=width)\n</code></pre>"},{"location":"reference/plot/#plot.unique_peptides","title":"<code>unique_peptides(df)</code>","text":"<p>Checks if all peptides in the DataFrame are unique. Needs to have columns 'start' and 'end' marking peptide intervals (inclusive).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing peptide information.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all peptides are unique, otherwise <code>False</code>.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def unique_peptides(df: pl.DataFrame) -&gt; bool:\n    \"\"\"\n    Checks if all peptides in the DataFrame are unique.\n    Needs to have columns 'start' and 'end' marking peptide intervals (inclusive).\n\n    Args:\n        df: DataFrame containing peptide information.\n\n    Returns:\n        `True` if all peptides are unique, otherwise `False`.\n\n    \"\"\"\n\n    return len(df) == len(df.unique(subset=[\"start\", \"end\"]))\n</code></pre>"},{"location":"reference/process/","title":"process","text":""},{"location":"reference/process/#process.aggregate","title":"<code>aggregate(df)</code>","text":"<p>Aggregate replicates by intensity-weighted average. Columns which are intensity-weighted averaged are: centroid_mz, centroid_mass, rt. All other columns are pass through if they are unique, otherwise set to <code>None</code>. Also adds n_replicates and n_cluster columns.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def aggregate(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Aggregate replicates by intensity-weighted average.\n    Columns which are intensity-weighted averaged are: centroid_mz, centroid_mass, rt.\n    All other columns are pass through if they are unique, otherwise set to `None`.\n    Also adds n_replicates and n_cluster columns.\n\n    \"\"\"\n    assert df[\"state\"].n_unique() == 1, (\n        \"DataFrame must be filtered to a single state before aggregation.\"\n    )\n\n    # columns which are intesity weighed averaged\n    intensity_wt_avg_columns = [\"centroid_mz\", \"centroid_mass\", \"rt\"]\n\n    output_columns = df.columns[:]\n\n    for col in intensity_wt_avg_columns:\n        col_idx = output_columns.index(col)\n        output_columns.insert(col_idx + 1, f\"{col}_sd\")\n    output_columns += [\"n_replicates\", \"n_cluster\"]\n\n    output = {k: [] for k in output_columns}\n    groups = df.group_by([\"start\", \"end\", \"exposure\"])\n    for (start, end, exposure), df_group in groups:\n        record = {}\n        record[\"start\"] = start\n        record[\"end\"] = end\n        record[\"exposure\"] = exposure\n        record[\"n_replicates\"] = df_group[\"replicate\"].n_unique()\n        record[\"n_cluster\"] = len(df_group)\n\n        # add intensity-weighted average columns\n        for col in intensity_wt_avg_columns:\n            val = ufloat_stats(df_group[col], df_group[\"intensity\"])\n            record[col] = val.nominal_value\n            record[f\"{col}_sd\"] = val.std_dev\n\n        # add other columns, taking the first value if unique, otherwise None\n        other_columns = set(df.columns) - record.keys()\n        for col in other_columns:\n            if df_group[col].n_unique() == 1:\n                record[col] = df_group[col][0]\n            else:\n                record[col] = None\n\n        # add record to output\n        assert output.keys() == record.keys()\n        for k in record:\n            output[k].append(record[k])\n\n    agg_df = nw.from_dict(output, backend=BACKEND)\n\n    return agg_df\n</code></pre>"},{"location":"reference/process/#process.aggregate_columns","title":"<code>aggregate_columns(df, columns, by=['start', 'end', 'exposure'])</code>","text":"<p>Aggregate the specified columns by intensity-weighted average. The dataframe must have a column named 'intensity' for weighting.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to aggregate.</p> required <code>columns</code> <code>list[str]</code> <p>List of columns to aggregate.</p> required <code>by</code> <code>list[str]</code> <p>List of columns to group by.</p> <code>['start', 'end', 'exposure']</code> Source code in <code>hdxms_datasets/process.py</code> <pre><code>@nw.narwhalify\ndef aggregate_columns(\n    df: nw.DataFrame, columns: list[str], by: list[str] = [\"start\", \"end\", \"exposure\"]\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Aggregate the specified columns by intensity-weighted average.\n    The dataframe must have a column named 'intensity' for weighting.\n\n    Args:\n        df: DataFrame to aggregate.\n        columns: List of columns to aggregate.\n        by: List of columns to group by.\n\n    \"\"\"\n    groups = df.group_by(by)\n    output = {k: [] for k in by}\n    for col in columns:\n        output[col] = []\n        output[f\"{col}_sd\"] = []\n\n    for (start, end, exposure), df_group in groups:\n        output[\"start\"].append(start)\n        output[\"end\"].append(end)\n        output[\"exposure\"].append(exposure)\n\n        for col in columns:\n            val = ufloat_stats(df_group[col], df_group[\"intensity\"])\n            output[col].append(val.nominal_value)\n            output[f\"{col}_sd\"].append(val.std_dev)\n\n    agg_df = nw.from_dict(output, backend=BACKEND)\n    return agg_df\n</code></pre>"},{"location":"reference/process/#process.apply_filters","title":"<code>apply_filters(df, **filters)</code>","text":"<p>Apply filters to the DataFrame based on the provided keyword arguments. Each keyword corresponds to a column name, and the value can be a single value or a list of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to filter.</p> required <code>**filters</code> <code>ValueType | list[ValueType]</code> <p>Column-value pairs to filter the DataFrame.</p> <code>{}</code> <p>Returns:     Filtered DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def apply_filters(df: nw.DataFrame, **filters: ValueType | list[ValueType]) -&gt; nw.DataFrame:\n    \"\"\"\n    Apply filters to the DataFrame based on the provided keyword arguments.\n    Each keyword corresponds to a column name, and the value can be a single value or a list of values.\n\n    Args:\n        df: The DataFrame to filter.\n        **filters: Column-value pairs to filter the DataFrame.\n    Returns:\n        Filtered DataFrame.\n    \"\"\"\n    exprs = []\n    for col, val in filters.items():\n        if isinstance(val, list):\n            expr = nw.col(col).is_in(val)\n        else:\n            expr = nw.col(col) == val\n        exprs.append(expr)\n    if not exprs:\n        return df\n    f_expr = reduce(and_, exprs)\n    return df.filter(f_expr)\n</code></pre>"},{"location":"reference/process/#process.compute_uptake_metrics","title":"<code>compute_uptake_metrics(df, exception='ignore')</code>","text":"<p>Tries to add columns to computed from other columns the DataFrame. Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to add columns to.</p> required <code>exception</code> <code>Literal['raise', 'warn', 'ignore']</code> <p>How to handle exceptions when adding columns. Options are 'raise', 'warn', 'ignore'.</p> <code>'ignore'</code> <p>Returns:     DataFrame with added columns.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def compute_uptake_metrics(\n    df: nw.DataFrame, exception: Literal[\"raise\", \"warn\", \"ignore\"] = \"ignore\"\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Tries to add columns to computed from other columns the DataFrame.\n    Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.\n\n    Args:\n        df: DataFrame to add columns to.\n        exception: How to handle exceptions when adding columns. Options are 'raise', 'warn', 'ignore'.\n    Returns:\n        DataFrame with added columns.\n\n    \"\"\"\n    all_columns = {\n        \"max_uptake\": hdx_expr.max_uptake,\n        \"uptake\": hdx_expr.uptake,\n        \"uptake_sd\": hdx_expr.uptake_sd,\n        \"fd_uptake\": hdx_expr.fd_uptake,\n        \"fd_uptake_sd\": hdx_expr.fd_uptake_sd,\n        \"frac_fd_control\": hdx_expr.frac_fd_control,\n        \"frac_fd_control_sd\": hdx_expr.frac_fd_control_sd,\n        \"frac_max_uptake\": hdx_expr.frac_max_uptake,\n        \"frac_max_uptake_sd\": hdx_expr.frac_max_uptake_sd,\n    }\n\n    for col, expr in all_columns.items():\n        if col not in df.columns:\n            try:\n                df = df.with_columns(expr)\n            except Exception as e:\n                if exception == \"raise\":\n                    raise e\n                elif exception == \"warn\":\n                    warnings.warn(f\"Failed to add column {col}: {e}\")\n                elif exception == \"ignore\":\n                    pass\n                else:\n                    raise ValueError(\"Invalid exception handling option\")\n\n    return df\n</code></pre>"},{"location":"reference/process/#process.drop_null_columns","title":"<code>drop_null_columns(df)</code>","text":"<p>Drop columns that are all null from the DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def drop_null_columns(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Drop columns that are all null from the DataFrame.\"\"\"\n    all_null_columns = [col for col in df.columns if df[col].is_null().all()]\n    return df.drop(all_null_columns)\n</code></pre>"},{"location":"reference/process/#process.dynamx_cluster_to_state","title":"<code>dynamx_cluster_to_state(cluster_data, nd_exposure=0.0)</code>","text":"<p>Convert dynamx cluster data to state data. Must contain only a single state.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_data</code> <code>DataFrame</code> <p>DataFrame containing dynamx cluster data.</p> required <code>nd_exposure</code> <code>float</code> <p>Exposure time for non-deuterated control.</p> <code>0.0</code> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def dynamx_cluster_to_state(cluster_data: nw.DataFrame, nd_exposure: float = 0.0) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert dynamx cluster data to state data.\n    Must contain only a single state.\n\n    Args:\n        cluster_data: DataFrame containing dynamx cluster data.\n        nd_exposure: Exposure time for non-deuterated control.\n\n    \"\"\"\n\n    assert len(cluster_data[\"state\"].unique()) == 1, \"Multiple states found in data\"\n\n    # determine undeuterated masses per peptide\n    nd_data = cluster_data.filter(nw.col(\"exposure\") == nd_exposure)\n    nd_peptides: list[tuple[int, int]] = sorted(\n        {(start, end) for start, end in zip(nd_data[\"start\"], nd_data[\"end\"])}\n    )\n\n    # create a dict of non-deuterated masses\n    peptides_nd_mass = {}\n    for p in nd_peptides:\n        start, end = p\n        df_nd_peptide = nd_data.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end))\n\n        masses = df_nd_peptide[\"z\"] * (df_nd_peptide[\"center\"] - PROTON_MASS)\n        nd_mass = ufloat_stats(masses, df_nd_peptide[\"inten\"])\n\n        peptides_nd_mass[p] = nd_mass\n\n    groups = cluster_data.group_by([\"start\", \"end\", \"exposure\"])\n    unique_columns = [\n        \"end\",\n        \"exposure\",\n        \"fragment\",\n        \"maxuptake\",\n        \"mhp\",\n        \"modification\",\n        \"protein\",\n        \"sequence\",\n        \"start\",\n        \"state\",\n        \"stop\",\n    ]\n\n    # Determine uptake and uptake_sd for each peptide/exposure by\n    # subtracting the non-deuterated mass from the observed mass\n    records = []\n    for (start, end, exposure), df_group in groups:\n        record = {col: df_group[col][0] for col in unique_columns}\n\n        rt = ufloat_stats(df_group[\"rt\"], df_group[\"inten\"])\n        record[\"rt\"] = rt.nominal_value\n        record[\"rt_sd\"] = rt.std_dev\n\n        # state data 'center' is mass as if |charge| would be 1\n        center = ufloat_stats(\n            df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS) + PROTON_MASS, df_group[\"inten\"]\n        )\n        record[\"center\"] = center.nominal_value\n        record[\"center_sd\"] = center.std_dev\n\n        masses = df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS)\n        exp_mass = ufloat_stats(masses, df_group[\"inten\"])\n\n        if (start, end) in peptides_nd_mass:\n            uptake = exp_mass - peptides_nd_mass[(start, end)]\n            record[\"uptake\"] = uptake.nominal_value\n            record[\"uptake_sd\"] = uptake.std_dev\n        else:\n            record[\"uptake\"] = None\n            record[\"uptake_sd\"] = None\n\n        records.append(record)\n\n    d = records_to_dict(records)\n    df = nw.from_dict(d, backend=BACKEND)\n\n    if set(df.columns) == set(STATE_DATA_COLUMN_ORDER):\n        df = df[STATE_DATA_COLUMN_ORDER]\n\n    return df\n</code></pre>"},{"location":"reference/process/#process.left_join","title":"<code>left_join(df_left, df_right, column, prefix, include_sd=True)</code>","text":"<p>Left join two DataFrames on start, end and the specified column.</p> <p>Parameters:</p> Name Type Description Default <code>df_left</code> <code>DataFrame</code> <p>Left DataFrame.</p> required <code>df_right</code> <code>DataFrame</code> <p>Right DataFrame.</p> required <code>column</code> <code>str</code> <p>Column name to join on in addition to start and end.</p> required <code>prefix</code> <code>str</code> <p>Prefix to add to the joined columns from the right DataFrame.</p> required <code>include_sd</code> <code>bool</code> <p>Whether to include the standard deviation column (column_sd) from the right DataFrame.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Merged DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def left_join(\n    df_left: nw.DataFrame, df_right: nw.DataFrame, column: str, prefix: str, include_sd: bool = True\n) -&gt; nw.DataFrame:\n    \"\"\"Left join two DataFrames on start, end and the specified column.\n\n    Args:\n        df_left: Left DataFrame.\n        df_right: Right DataFrame.\n        column: Column name to join on in addition to start and end.\n        prefix: Prefix to add to the joined columns from the right DataFrame.\n        include_sd: Whether to include the standard deviation column (column_sd) from the right DataFrame.\n\n    Returns:\n        Merged DataFrame.\n\n    \"\"\"\n    select = [nw.col(\"start\"), nw.col(\"end\")]\n    select.append(nw.col(column).alias(f\"{prefix}_{column}\"))\n    if include_sd:\n        select.append(nw.col(f\"{column}_sd\").alias(f\"{prefix}_{column}_sd\"))\n\n    merge = df_left.join(\n        df_right.select(select),\n        on=[\"start\", \"end\"],\n        how=\"left\",  # 'left' join ensures all rows from pd_peptides are kept\n    )\n\n    return merge\n</code></pre>"},{"location":"reference/process/#process.merge_peptide_tables","title":"<code>merge_peptide_tables(partially_deuterated, column=None, non_deuterated=None, fully_deuterated=None)</code>","text":"<p>Merges peptide tables from different deuteration types into a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>partially_deuterated</code> <code>DataFrame</code> <p>DataFrame containing partially deuterated peptides. Must be provided.</p> required <code>column</code> <code>Optional[str]</code> <p>Column name to join on. If None, 'centroid_mass' is used if present, otherwise 'uptake'.</p> <code>None</code> <code>non_deuterated</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame containing non-deuterated peptides.</p> <code>None</code> <code>fully_deuterated</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame containing fully deuterated peptides.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Merged DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def merge_peptide_tables(\n    partially_deuterated: nw.DataFrame,\n    column: Optional[str] = None,\n    non_deuterated: Optional[nw.DataFrame] = None,\n    fully_deuterated: Optional[nw.DataFrame] = None,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Merges peptide tables from different deuteration types into a single DataFrame.\n\n    Args:\n        partially_deuterated: DataFrame containing partially deuterated peptides. Must be provided.\n        column: Column name to join on. If None, 'centroid_mass' is used if present, otherwise 'uptake'.\n        non_deuterated: Optional DataFrame containing non-deuterated peptides.\n        fully_deuterated: Optional DataFrame containing fully deuterated peptides.\n\n    Returns:\n        Merged DataFrame.\n\n    \"\"\"\n    if column is not None:\n        join_column = column\n    elif \"centroid_mass\" in partially_deuterated.columns:\n        join_column = \"centroid_mass\"\n    elif \"uptake\" in partially_deuterated.columns:\n        join_column = \"uptake\"\n\n    output = partially_deuterated\n    if non_deuterated is not None:\n        # TODO move assert to `left_join` ?\n        assert peptides_are_unique(non_deuterated), \"Non-deuterated peptides must be unique.\"\n        output = left_join(output, non_deuterated, column=join_column, prefix=\"nd\")\n    if fully_deuterated is not None:\n        assert peptides_are_unique(fully_deuterated), \"Fully deuterated peptides must be unique.\"\n        output = left_join(output, fully_deuterated, column=join_column, prefix=\"fd\")\n    return output\n</code></pre>"},{"location":"reference/process/#process.merge_peptides","title":"<code>merge_peptides(peptides, base_dir=Path.cwd())</code>","text":"<p>Merge peptide tables from different deuteration types into a single DataFrame. This function is used to match control measurements to a set of partially deuterated peptides.</p> <p>Supports non-deuterated (nd) and fully deuterated peptides (fd) as controls. The column used in the merge is 'centroid_mass' if present, otherwise 'uptake'. Merged columns are prefixed with 'nd_' or 'fd_'.</p> When to use merge_peptide_tables vs left_join <ul> <li>Use <code>merge_peptide_tables</code> to merge already loaded peptide dataframes.</li> <li>Use <code>left_join</code> to merge peptide dataframes with other controls / data types.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>list[Peptides]</code> <p>List of Peptides objects to merge. Must contain one partially deuterated peptide.</p> required <code>base_dir</code> <code>Path</code> <p>Base directory to resolve relative paths in Peptides data_file.</p> <code>cwd()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Merged DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def merge_peptides(peptides: list[Peptides], base_dir: Path = Path.cwd()) -&gt; nw.DataFrame:\n    \"\"\"Merge peptide tables from different deuteration types into a single DataFrame.\n    This function is used to match control measurements to a set of partially deuterated peptides.\n\n    Supports non-deuterated (nd) and fully deuterated peptides (fd) as controls.\n    The column used in the merge is 'centroid_mass' if present, otherwise 'uptake'. Merged columns are prefixed\n    with 'nd_' or 'fd_'.\n\n    ??? tip \"When to use merge_peptide_tables vs left_join\"\n        - Use `merge_peptide_tables` to merge already loaded peptide dataframes.\n        - Use `left_join` to merge peptide dataframes with other controls / data types.\n\n    Args:\n        peptides: List of Peptides objects to merge. Must contain one partially deuterated peptide.\n        base_dir: Base directory to resolve relative paths in Peptides data_file.\n\n    Returns:\n        Merged DataFrame.\n\n    \"\"\"\n    peptide_types = {p.deuteration_type for p in peptides}\n    if not peptides:\n        raise ValueError(\"No peptides provided for merging.\")\n\n    if len(peptide_types) != len(peptides):\n        raise ValueError(\n            \"Multiple peptides of the same type found. Please ensure unique deuteration types.\"\n        )\n\n    if DeuterationType.partially_deuterated not in peptide_types:\n        raise ValueError(\"Partially deuterated peptide is required for uptake metrics calculation.\")\n\n    loaded_peptides = {\n        p.deuteration_type.value: load_peptides(p, base_dir=base_dir) for p in peptides\n    }\n\n    merged = merge_peptide_tables(**loaded_peptides, column=None)\n    return merged\n</code></pre>"},{"location":"reference/process/#process.sort_columns","title":"<code>sort_columns(df, columns=OPEN_HDX_COLUMNS)</code>","text":"<p>Sorts the DataFrame columns to match the specified order.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to sort.</p> required <code>columns</code> <code>list[str]</code> <p>List of columns in the desired order. Columns not in this list will be placed at the end.</p> <code>OPEN_HDX_COLUMNS</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns sorted.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def sort_columns(df: nw.DataFrame, columns: list[str] = OPEN_HDX_COLUMNS) -&gt; nw.DataFrame:\n    \"\"\"Sorts the DataFrame columns to match the specified order.\n\n    Args:\n        df: DataFrame to sort.\n        columns: List of columns in the desired order. Columns not in this list will be placed at the end.\n\n    Returns:\n        DataFrame with columns sorted.\n\n    \"\"\"\n    matching_columns = [col for col in columns if col in df.columns]\n    other_columns = [col for col in df.columns if col not in matching_columns]\n\n    assert set(df.columns) == set(matching_columns + other_columns)\n\n    return df[matching_columns + other_columns]\n</code></pre>"},{"location":"reference/process/#process.sort_rows","title":"<code>sort_rows(df)</code>","text":"<p>Sorts the DataFrame by state, exposure, start, end, file.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def sort_rows(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Sorts the DataFrame by state, exposure, start, end, file.\"\"\"\n    all_by = [\"state\", \"exposure\", \"start\", \"end\", \"replicate\"]\n    by = [col for col in all_by if col in df.columns]\n    return df.sort(by=by)\n</code></pre>"},{"location":"reference/process/#process.ufloat_stats","title":"<code>ufloat_stats(array, weights)</code>","text":"<p>Calculate the weighted mean and standard deviation.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def ufloat_stats(array, weights) -&gt; Variable:\n    \"\"\"Calculate the weighted mean and standard deviation.\"\"\"\n    weighted_stats = DescrStatsW(array, weights=weights, ddof=0)\n    return ufloat(weighted_stats.mean, weighted_stats.std)\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#utils.contiguous_peptides","title":"<code>contiguous_peptides(df)</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing contiguous regions.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef contiguous_peptides(df: IntoFrame) -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing contiguous regions.\n    \"\"\"\n    # cast to ensure df is a narwhals DataFrame\n    df = cast(nw.DataFrame, df).select([\"start\", \"end\"]).unique().sort(by=[\"start\", \"end\"])\n\n    regions = []\n    current_start, current_end = None, 0\n\n    for start_val, end_val in df.select([nw.col(\"start\"), nw.col(\"end\")]).iter_rows(named=False):\n        if current_start is None:\n            # Initialize the first region\n            current_start, current_end = start_val, end_val\n        elif start_val &lt;= current_end + 1:  # Check for contiguity\n            # Extend the current region\n            current_end = max(current_end, end_val)\n        else:\n            # Save the previous region and start a new one\n            regions.append((current_start, current_end))\n            current_start, current_end = start_val, end_val\n\n    # Don't forget to add the last region\n    if current_start is not None:\n        regions.append((current_start, current_end))\n\n    return regions\n</code></pre>"},{"location":"reference/utils/#utils.diff_sequence","title":"<code>diff_sequence(a, b)</code>","text":"<p>Compute the similarity ratio between two sequences.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def diff_sequence(a: str, b: str) -&gt; float:\n    \"\"\"\n    Compute the similarity ratio between two sequences.\n    \"\"\"\n    return difflib.SequenceMatcher(None, a, b).ratio()\n</code></pre>"},{"location":"reference/utils/#utils.get_peptides_by_type","title":"<code>get_peptides_by_type(peptides, deuteration_type)</code>","text":"<p>Get peptides of a specific deuteration type.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def get_peptides_by_type(\n    peptides: list[Peptides], deuteration_type: DeuterationType\n) -&gt; Optional[Peptides]:\n    \"\"\"Get peptides of a specific deuteration type.\"\"\"\n    matching_peptides = [p for p in peptides if p.deuteration_type == deuteration_type]\n    if not matching_peptides:\n        return None\n    if len(matching_peptides) &gt; 1:\n        return None\n    return matching_peptides[0]\n</code></pre>"},{"location":"reference/utils/#utils.non_overlapping_peptides","title":"<code>non_overlapping_peptides(df)</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing non-overlapping peptides.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef non_overlapping_peptides(\n    df: IntoFrame,\n) -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing non-overlapping peptides.\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([\"start\", \"end\"]).unique().sort(by=[\"start\", \"end\"])\n\n    regions = df.rows()\n    out = [regions[0]]\n    for start_val, end_val in regions[1:]:\n        if start_val &gt; out[-1][1]:\n            out.append((start_val, end_val))\n        else:\n            continue\n\n    return out\n</code></pre>"},{"location":"reference/utils/#utils.peptide_redundancy","title":"<code>peptide_redundancy(df)</code>","text":"<p>Compute the redundancy of peptides in a DataFrame based on their start and end positions. Redundancy is defined as the number of peptides overlapping at each position.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrame</code> <p>DataFrame containing peptide information with 'start' and 'end' columns.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A tuple containing:</p> <code>ndarray</code> <ul> <li>r_number: An array of positions from the minimum start to the maximum end.</li> </ul> <code>tuple[ndarray, ndarray]</code> <ul> <li>redundancy: An array of redundancy counts for each position in r_number.</li> </ul> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef peptide_redundancy(df: IntoFrame) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the redundancy of peptides in a DataFrame based on their start and end positions.\n    Redundancy is defined as the number of peptides overlapping at each position.\n\n    Args:\n        df: DataFrame containing peptide information with 'start' and 'end' columns.\n\n    Returns:\n        A tuple containing:\n        - r_number: An array of positions from the minimum start to the maximum end.\n        - redundancy: An array of redundancy counts for each position in r_number.\n\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([\"start\", \"end\"]).unique().sort(by=[\"start\", \"end\"])\n    vmin, vmax = df[\"start\"][0], df[\"end\"][-1]\n\n    r_number = np.arange(vmin, vmax + 1, dtype=int)\n    redundancy = np.zeros_like(r_number, dtype=int)\n    for s, e in df.rows():\n        i0, i1 = np.searchsorted(r_number, (s, e))\n        redundancy[i0:i1] += 1\n\n    return r_number, redundancy\n</code></pre>"},{"location":"reference/utils/#utils.peptides_are_unique","title":"<code>peptides_are_unique(peptides_df)</code>","text":"<p>Check if the peptides in the dataframe are unique.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def peptides_are_unique(peptides_df: nw.DataFrame) -&gt; bool:\n    \"\"\"Check if the peptides in the dataframe are unique.\"\"\"\n    unique_peptides = peptides_df.select([\"start\", \"end\"]).unique()\n    return len(unique_peptides) == len(peptides_df)\n</code></pre>"},{"location":"reference/utils/#utils.reconstruct_sequence","title":"<code>reconstruct_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Reconstruct the sequence form a dataframe of peptides with sequence information. The sequence is reconstructed by replacing the known sequence with the peptide sequences at the specified start and end positions.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>Starting sequence. Can be a string 'X' as placeholder.</p> required <code>n_term</code> <code>int</code> <p>The residue number of the N-terminal residue. This is typically 1, can be negative in case of purification tags.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>The reconstructed sequence.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef reconstruct_sequence(\n    peptides: nw.DataFrame,\n    known_sequence: str,\n    n_term: int = 1,\n) -&gt; str:\n    \"\"\"\n    Reconstruct the sequence form a dataframe of peptides with sequence information.\n    The sequence is reconstructed by replacing the known sequence with the peptide\n    sequences at the specified start and end positions.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: Starting sequence. Can be a string 'X' as placeholder.\n        n_term: The residue number of the N-terminal residue. This is typically 1, can be\n            negative in case of purification tags.\n\n    Returns:\n        The reconstructed sequence.\n    \"\"\"\n\n    reconstructed = list(known_sequence)\n    for start_, end_, sequence_ in peptides.select([\"start\", \"end\", \"sequence\"]).iter_rows():  # type: ignore\n        start_idx = start_ - n_term\n        assert end_ - start_ + 1 == len(sequence_), (\n            f\"Length mismatch at {start_}:{end_} with sequence {sequence_}\"\n        )\n\n        for i, aa in enumerate(sequence_, start=start_idx):\n            reconstructed[i] = aa\n\n    return \"\".join(reconstructed)\n</code></pre>"},{"location":"reference/utils/#utils.records_to_dict","title":"<code>records_to_dict(records)</code>","text":"<p>Convert a list of records to a dictionary.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def records_to_dict(records: list[dict[str, Any]]) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert a list of records to a dictionary.\n    \"\"\"\n    output = defaultdict(list)\n    for record in records:\n        for key, value in record.items():\n            output[key].append(value)\n\n    return dict(output)\n</code></pre>"},{"location":"reference/utils/#utils.verify_sequence","title":"<code>verify_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Verify the sequence of peptides against the given sequence.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>IntoFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>The original sequence to check against.</p> required <code>n_term</code> <code>int</code> <p>The number of N-terminal residues to consider.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[tuple[int, str, str]]</code> <p>A tuple containing the fixed sequence and a list of mismatches.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef verify_sequence(\n    peptides: IntoFrame,\n    known_sequence: str,\n    n_term: int = 1,\n) -&gt; list[tuple[int, str, str]]:\n    \"\"\"\n    Verify the sequence of peptides against the given sequence.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: The original sequence to check against.\n        n_term: The number of N-terminal residues to consider.\n\n    Returns:\n        A tuple containing the fixed sequence and a list of mismatches.\n    \"\"\"\n\n    reconstructed_sequence = reconstruct_sequence(peptides, known_sequence, n_term)\n\n    mismatches = []\n    for r_number, (expected, found) in enumerate(\n        zip(known_sequence, reconstructed_sequence), start=n_term\n    ):\n        if expected != found:\n            mismatches.append((r_number, expected, found))\n\n    return mismatches\n</code></pre>"},{"location":"reference/verification/","title":"verification","text":""},{"location":"reference/verification/#verification.build_structure_peptides_comparison","title":"<code>build_structure_peptides_comparison(structure, peptides)</code>","text":"<p>Compares residue numbering and identity between a structure and peptides.</p> <p>dictionary with:</p> Type Description <code>DataFrame</code> <ul> <li>total_residues: Total number of residues in the peptides (considering chains)</li> </ul> <code>DataFrame</code> <ul> <li>matched_residues: Number of residues that are matched to the structure (by chain and resi)</li> </ul> <code>DataFrame</code> <ul> <li>identical_residues: Number of matched residues that have the correct amino acid identity</li> </ul> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def build_structure_peptides_comparison(\n    structure: Structure,\n    peptides: Peptides,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Compares residue numbering and identity between a structure and peptides.\n\n    Returns:  dictionary with:\n        - total_residues: Total number of residues in the peptides (considering chains)\n        - matched_residues: Number of residues that are matched to the structure (by chain and resi)\n        - identical_residues: Number of matched residues that have the correct amino acid identity\n\n    \"\"\"\n    structure_df = residue_df_from_structure(structure)\n    residue_df = residue_df_from_peptides(peptides)\n\n    chains = (\n        peptides.chain if peptides.chain is not None else structure_df[\"chain\"].unique().to_list()\n    )\n\n    # supplement the residue_df with all chains\n    # multie-chain peptides are expected to correspond to homomultimers\n    import polars as pl\n\n    residue_df_chain = pl.concat(\n        [residue_df.with_columns(pl.lit(ch).alias(\"chain\")) for ch in chains]\n    )\n\n    merged = residue_df_chain.join(\n        structure_df,\n        on=[\"chain\", \"resi\"],\n        how=\"left\",\n    )\n\n    return merged\n</code></pre>"},{"location":"reference/verification/#verification.compare_structure_peptides","title":"<code>compare_structure_peptides(structure, peptides, returns='dict')</code>","text":"<pre><code>compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"dict\"] = \"dict\",\n) -&gt; CompareSummary\n</code></pre><pre><code>compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"df\"],\n) -&gt; pl.DataFrame\n</code></pre><pre><code>compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"both\"],\n) -&gt; tuple[CompareSummary, pl.DataFrame]\n</code></pre> <p>Compares structure and peptide data.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"dict\", \"df\", \"both\"] = \"dict\",\n) -&gt; pl.DataFrame | CompareSummary | tuple[CompareSummary, pl.DataFrame]:\n    \"\"\"Compares structure and peptide data.\"\"\"\n\n    df = build_structure_peptides_comparison(structure, peptides)\n\n    if returns == \"dict\":\n        return summarize_compare_table(df)\n    elif returns == \"df\":\n        return df\n    elif returns == \"both\":\n        return summarize_compare_table(df), df\n    else:\n        raise ValueError(f\"Invalid returns value: {returns!r}\")\n</code></pre>"},{"location":"reference/verification/#verification.datafiles_exist","title":"<code>datafiles_exist(dataset)</code>","text":"<p>Check if the data files for all peptides and structures in the dataset exist.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def datafiles_exist(dataset: HDXDataSet) -&gt; bool:\n    \"\"\"\n    Check if the data files for all peptides and structures in the dataset exist.\n    \"\"\"\n    for state in dataset.states:\n        for peptides in state.peptides:\n            if not peptides.data_file.exists():\n                return False\n    if not dataset.structure.data_file.exists():\n        return False\n    return True\n</code></pre>"},{"location":"reference/verification/#verification.residue_df_from_peptides","title":"<code>residue_df_from_peptides(peptides)</code>","text":"<p>Create a dataframe from the peptides with resi, resn_TLA</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def residue_df_from_peptides(peptides: Peptides) -&gt; pl.DataFrame:\n    \"\"\"Create a dataframe from the peptides with resi, resn_TLA\"\"\"\n    from Bio.Data import IUPACData\n    import polars as pl\n\n    one_to_three = IUPACData.protein_letters_1to3\n\n    peptide_df = peptides.load().to_native()\n\n    start, end = peptide_df[\"start\"].min(), peptide_df[\"end\"].max()\n    residues = range(start, end + 1)\n    known_sequence = \"X\" * len(residues)\n    sequence = reconstruct_sequence(peptide_df, known_sequence, n_term=start)\n\n    residue_df = (\n        pl.DataFrame({\"resi\": residues, \"resn\": list(sequence)})\n        .filter(pl.col(\"resn\") != \"X\")\n        .with_columns(\n            [\n                pl.col(\"resi\").cast(str),\n                pl.col(\"resn\").replace_strict(one_to_three).str.to_uppercase().alias(\"resn_TLA\"),\n            ]\n        )\n    )\n\n    return residue_df\n</code></pre>"},{"location":"reference/verification/#verification.residue_df_from_structure","title":"<code>residue_df_from_structure(structure)</code>","text":"<p>Create a dataframe from the structure with chain, resi, resn_TLA</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def residue_df_from_structure(structure: Structure) -&gt; pl.DataFrame:\n    \"\"\"Create a dataframe from the structure with chain, resi, resn_TLA\"\"\"\n    if structure.format not in [\"cif\", \"mmcif\"]:\n        raise ValueError(f\"Unsupported structure format: {structure.format}\")\n\n    # create the reference structure amino acid dataframe\n    from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n    from Bio.Data import IUPACData\n    import polars as pl\n\n    mm = MMCIF2Dict(structure.data_file)\n\n    one_to_three = IUPACData.protein_letters_1to3\n    AA_codes = [a.upper() for a in list(one_to_three.values())]\n\n    # create a dataframe with chain, resi, resn_TLA from structure\n    chain_name = \"label_asym_id\" if not structure.auth_chain_labels else \"auth_asym_id\"\n    resn_name = \"label_seq_id\" if not structure.auth_residue_numbers else \"auth_seq_id\"\n\n    structure_df = (\n        pl.DataFrame(\n            {\n                \"chain\": mm[\"_atom_site.\" + chain_name],\n                \"resi\": mm[\"_atom_site.\" + resn_name],\n                \"resn_TLA\": [s.upper() for s in mm[\"_atom_site.label_comp_id\"]],\n            }\n        )\n        .unique()\n        .filter(pl.col(\"resn_TLA\").is_in(AA_codes))\n    )\n\n    return structure_df\n</code></pre>"},{"location":"reference/verification/#verification.summarize_compare_table","title":"<code>summarize_compare_table(df)</code>","text":"<p>Derive the metrics from the merged table.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def summarize_compare_table(df: pl.DataFrame) -&gt; CompareSummary:\n    \"\"\"Derive the metrics from the merged table.\"\"\"\n    # rows with a structure match (adjust column names to your schema)\n    import polars as pl\n\n    matched = df.drop_nulls(subset=[\"resn_TLA_right\"])\n    identical = matched.filter(pl.col(\"resn_TLA\") == pl.col(\"resn_TLA_right\"))\n\n    return {\n        \"total_residues\": df.height,\n        \"matched_residues\": matched.height,\n        \"identical_residues\": identical.height,\n    }\n</code></pre>"},{"location":"reference/verification/#verification.verify_dataset","title":"<code>verify_dataset(dataset)</code>","text":"<p>Verify the integrity of the dataset by checking sequences and data files.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def verify_dataset(dataset: HDXDataSet):\n    \"\"\"Verify the integrity of the dataset by checking sequences and data files.\"\"\"\n    verify_peptides(dataset)\n    if not datafiles_exist(dataset):\n        raise ValueError(\"Missing datafiles\")\n\n    if dataset.file_hash is None:\n        raise ValueError(\"Dataset file hash is not set\")\n</code></pre>"},{"location":"reference/verification/#verification.verify_peptides","title":"<code>verify_peptides(dataset)</code>","text":"<p>Verify that all peptide sequences match the protein sequence in the dataset states.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def verify_peptides(dataset: HDXDataSet):\n    \"\"\"Verify that all peptide sequences match the protein sequence in the dataset states.\"\"\"\n    for state in dataset.states:\n        sequence = state.protein_state.sequence\n        for i, peptides in enumerate(state.peptides):\n            peptide_table = peptides.load()\n            try:\n                mismatches = verify_sequence(\n                    peptide_table, sequence, n_term=state.protein_state.n_term\n                )\n            except IndexError as e:\n                raise IndexError(f\"State: {state.name}, Peptides[{i}] has an index error: {e}\")\n            if mismatches:\n                raise ValueError(\n                    f\"State: {state.name}, Peptides[{i}] does not match protein sequence, mismatches: {mismatches}\"\n                )\n</code></pre>"},{"location":"reference/view/","title":"view","text":""},{"location":"reference/view/#view.StructureView","title":"<code>StructureView</code>","text":"Source code in <code>hdxms_datasets/view.py</code> <pre><code>class StructureView:\n    def __init__(self, structure: Structure, hide_water=True, **kwargs: ValueType):\n        \"\"\"\n        Initialize the PDBeMolstar visualization namespace.\n\n        Args:\n            structure: The structure to visualize.\n            **kwargs: Additional keyword arguments for customization.\n        \"\"\"\n        self.structure = structure\n\n        from ipymolstar import PDBeMolstar\n\n        self.view = PDBeMolstar(\n            custom_data=self.structure.pdbemolstar_custom_data(),\n            hide_water=hide_water,\n            **kwargs,\n        )\n\n    @staticmethod\n    def resolve_peptides(peptides: Peptides | nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Loads peptides as a DataFrame or returns the DataFrame.\n        \"\"\"\n        if isinstance(peptides, Peptides):\n            df = peptides.load()\n        else:\n            df = peptides\n\n        return df\n\n    @staticmethod\n    def resolve_chain(peptides: Peptides | nw.DataFrame, chain: list[str] | None) -&gt; list[str]:\n        \"\"\"\n        Resolves the chain information from a Peptides object or a DataFrame.\n        \"\"\"\n        if isinstance(chain, list):\n            return chain\n        elif isinstance(peptides, Peptides):\n            return peptides.chain if peptides.chain else []\n        else:\n            return []\n\n    def show(self) -&gt; StructureView:\n        return self.view\n\n    def color_peptide(\n        self,\n        start: int,\n        end: int,\n        chain: list[str] | None = None,\n        color: str = \"red\",\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; StructureView:\n        c_dict = {\n            \"start_\" + self.structure.residue_name: int(start),\n            \"end_\" + self.structure.residue_name: int(end),\n            \"color\": color,\n        }\n\n        data = self._augment_chain([c_dict], chain or [])\n\n        color_data = {\n            \"data\": data,\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = None\n\n        return self\n\n    def peptide_coverage(\n        self,\n        peptides: Peptides | nw.DataFrame,\n        color: str = \"darkgreen\",\n        chain: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; StructureView:\n        \"\"\"\n        Plots peptide coverage on the structure.\n\n        Args:\n            peptides: Peptides object or DataFrame containing peptide data.\n            color: Color for the covered regions.\n            chain: List of chains to apply the coloring to.\n            non_selected_color: Color for non-covered regions.\n        \"\"\"\n        df = self.resolve_peptides(peptides)\n        chain = self.resolve_chain(peptides, chain)\n        intervals = contiguous_peptides(df)\n\n        data = []\n        for start, end in intervals:\n            elem = {\n                f\"start_{self.structure.residue_name}\": int(start),\n                f\"end_{self.structure.residue_name}\": int(end),\n                \"color\": color,\n            }\n            data.append(elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data, chain),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = None\n        return self\n\n    def non_overlapping_peptides(\n        self,\n        peptides: Peptides | nw.DataFrame,\n        colors: list[str] | None = None,\n        chain: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; StructureView:\n        \"\"\"Selects a set of non-overlapping peptides to display on the structure. Starts with the first\n        peptide and successively adds peptides that do not overlap with already selected peptides.\n\n        Args:\n            peptides: Peptides object or DataFrame containing peptide data.\n            colors: List of colors to cycle through for different peptides.\n            chain: List of chains to apply the coloring to.\n            non_selected_color: Color for non-covered regions.\n\n        Returns:\n            StructureView: The updated StructureView object.\n\n        \"\"\"\n        df = self.resolve_peptides(peptides)\n        chain = self.resolve_chain(peptides, chain)\n\n        intervals = non_overlapping_peptides(df)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n        )\n\n        cdata = []\n        tdata = []\n        for (start, end), color in zip(intervals, itertools.cycle(colors)):\n            cdata.append(\n                {\n                    f\"start_{self.structure.residue_name}\": int(start),\n                    f\"end_{self.structure.residue_name}\": int(end),\n                    \"color\": color,\n                }\n            )\n            df_f = df.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end)).to_native()\n            sequence = df_f[\"sequence\"].unique().first()\n            tdata.append(\n                {\n                    f\"start_{self.structure.residue_name}\": int(start),\n                    f\"end_{self.structure.residue_name}\": int(end),\n                    \"tooltip\": f\"Peptide: {sequence}\",\n                }\n            )\n\n        color_data = {\n            \"data\": self._augment_chain(cdata, chain),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = {\"data\": self._augment_chain(tdata, chain)}\n        return self\n\n    def peptide_redundancy(\n        self,\n        peptides: Peptides | nw.DataFrame,\n        chain: list[str] | None = None,\n        colors: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; StructureView:\n        \"\"\"Colors residues by peptide redundancy.\n\n        Args:\n            peptides: Peptides object or DataFrame containing peptide data.\n            chain: List of chains to apply the coloring to.\n            colors: List of colors to use for different redundancy levels.\n            non_selected_color: Color for non-covered regions.\n\n        \"\"\"\n        df = self.resolve_peptides(peptides)\n        chain = self.resolve_chain(peptides, chain)\n\n        r_number, redundancy = peptide_redundancy(df)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n        )\n        color_lut = {i + 1: colors[i] for i in range(len(colors))}\n\n        data = []\n        tooltips = []\n        for rn, rv in zip(r_number, redundancy.clip(0, len(colors) - 1)):\n            tooltips.append(\n                {\n                    f\"{self.structure.residue_name}\": int(rn),\n                    \"tooltip\": f\"Redundancy: {rv} peptides\",\n                }\n            )\n\n            if rv == 0:\n                continue\n            color_elem = {\n                f\"{self.structure.residue_name}\": int(rn),\n                \"color\": color_lut[rv],\n            }\n            data.append(color_elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data, chain),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = {\"data\": self._augment_chain(tooltips, chain)}\n        return self\n\n    def _augment_chain(\n        self, data: list[dict[str, ValueType]], chains: Sequence[str]\n    ) -&gt; list[dict[str, ValueType]]:\n        \"\"\"Augment a list of data with chain information\"\"\"\n        if chains:\n            aug_data = []\n            for elem, chain in itertools.product(data, chains):\n                aug_data.append(elem | {self.structure.chain_name: chain})\n        else:\n            aug_data = data\n\n        return aug_data\n\n    def _repr_mimebundle_(self, include=None, exclude=None):\n        return self.show()._repr_mimebundle_(include=include, exclude=exclude)\n</code></pre>"},{"location":"reference/view/#view.StructureView.__init__","title":"<code>__init__(structure, hide_water=True, **kwargs)</code>","text":"<p>Initialize the PDBeMolstar visualization namespace.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Structure</code> <p>The structure to visualize.</p> required <code>**kwargs</code> <code>ValueType</code> <p>Additional keyword arguments for customization.</p> <code>{}</code> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def __init__(self, structure: Structure, hide_water=True, **kwargs: ValueType):\n    \"\"\"\n    Initialize the PDBeMolstar visualization namespace.\n\n    Args:\n        structure: The structure to visualize.\n        **kwargs: Additional keyword arguments for customization.\n    \"\"\"\n    self.structure = structure\n\n    from ipymolstar import PDBeMolstar\n\n    self.view = PDBeMolstar(\n        custom_data=self.structure.pdbemolstar_custom_data(),\n        hide_water=hide_water,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/view/#view.StructureView.non_overlapping_peptides","title":"<code>non_overlapping_peptides(peptides, colors=None, chain=None, non_selected_color='lightgray')</code>","text":"<p>Selects a set of non-overlapping peptides to display on the structure. Starts with the first peptide and successively adds peptides that do not overlap with already selected peptides.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>Peptides | DataFrame</code> <p>Peptides object or DataFrame containing peptide data.</p> required <code>colors</code> <code>list[str] | None</code> <p>List of colors to cycle through for different peptides.</p> <code>None</code> <code>chain</code> <code>list[str] | None</code> <p>List of chains to apply the coloring to.</p> <code>None</code> <code>non_selected_color</code> <code>str</code> <p>Color for non-covered regions.</p> <code>'lightgray'</code> <p>Returns:</p> Name Type Description <code>StructureView</code> <code>StructureView</code> <p>The updated StructureView object.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def non_overlapping_peptides(\n    self,\n    peptides: Peptides | nw.DataFrame,\n    colors: list[str] | None = None,\n    chain: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n) -&gt; StructureView:\n    \"\"\"Selects a set of non-overlapping peptides to display on the structure. Starts with the first\n    peptide and successively adds peptides that do not overlap with already selected peptides.\n\n    Args:\n        peptides: Peptides object or DataFrame containing peptide data.\n        colors: List of colors to cycle through for different peptides.\n        chain: List of chains to apply the coloring to.\n        non_selected_color: Color for non-covered regions.\n\n    Returns:\n        StructureView: The updated StructureView object.\n\n    \"\"\"\n    df = self.resolve_peptides(peptides)\n    chain = self.resolve_chain(peptides, chain)\n\n    intervals = non_overlapping_peptides(df)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n    )\n\n    cdata = []\n    tdata = []\n    for (start, end), color in zip(intervals, itertools.cycle(colors)):\n        cdata.append(\n            {\n                f\"start_{self.structure.residue_name}\": int(start),\n                f\"end_{self.structure.residue_name}\": int(end),\n                \"color\": color,\n            }\n        )\n        df_f = df.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end)).to_native()\n        sequence = df_f[\"sequence\"].unique().first()\n        tdata.append(\n            {\n                f\"start_{self.structure.residue_name}\": int(start),\n                f\"end_{self.structure.residue_name}\": int(end),\n                \"tooltip\": f\"Peptide: {sequence}\",\n            }\n        )\n\n    color_data = {\n        \"data\": self._augment_chain(cdata, chain),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self.view.color_data = color_data\n    self.view.tooltips = {\"data\": self._augment_chain(tdata, chain)}\n    return self\n</code></pre>"},{"location":"reference/view/#view.StructureView.peptide_coverage","title":"<code>peptide_coverage(peptides, color='darkgreen', chain=None, non_selected_color='lightgray')</code>","text":"<p>Plots peptide coverage on the structure.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>Peptides | DataFrame</code> <p>Peptides object or DataFrame containing peptide data.</p> required <code>color</code> <code>str</code> <p>Color for the covered regions.</p> <code>'darkgreen'</code> <code>chain</code> <code>list[str] | None</code> <p>List of chains to apply the coloring to.</p> <code>None</code> <code>non_selected_color</code> <code>str</code> <p>Color for non-covered regions.</p> <code>'lightgray'</code> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def peptide_coverage(\n    self,\n    peptides: Peptides | nw.DataFrame,\n    color: str = \"darkgreen\",\n    chain: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n) -&gt; StructureView:\n    \"\"\"\n    Plots peptide coverage on the structure.\n\n    Args:\n        peptides: Peptides object or DataFrame containing peptide data.\n        color: Color for the covered regions.\n        chain: List of chains to apply the coloring to.\n        non_selected_color: Color for non-covered regions.\n    \"\"\"\n    df = self.resolve_peptides(peptides)\n    chain = self.resolve_chain(peptides, chain)\n    intervals = contiguous_peptides(df)\n\n    data = []\n    for start, end in intervals:\n        elem = {\n            f\"start_{self.structure.residue_name}\": int(start),\n            f\"end_{self.structure.residue_name}\": int(end),\n            \"color\": color,\n        }\n        data.append(elem)\n\n    color_data = {\n        \"data\": self._augment_chain(data, chain),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self.view.color_data = color_data\n    self.view.tooltips = None\n    return self\n</code></pre>"},{"location":"reference/view/#view.StructureView.peptide_redundancy","title":"<code>peptide_redundancy(peptides, chain=None, colors=None, non_selected_color='lightgray')</code>","text":"<p>Colors residues by peptide redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>Peptides | DataFrame</code> <p>Peptides object or DataFrame containing peptide data.</p> required <code>chain</code> <code>list[str] | None</code> <p>List of chains to apply the coloring to.</p> <code>None</code> <code>colors</code> <code>list[str] | None</code> <p>List of colors to use for different redundancy levels.</p> <code>None</code> <code>non_selected_color</code> <code>str</code> <p>Color for non-covered regions.</p> <code>'lightgray'</code> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def peptide_redundancy(\n    self,\n    peptides: Peptides | nw.DataFrame,\n    chain: list[str] | None = None,\n    colors: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n) -&gt; StructureView:\n    \"\"\"Colors residues by peptide redundancy.\n\n    Args:\n        peptides: Peptides object or DataFrame containing peptide data.\n        chain: List of chains to apply the coloring to.\n        colors: List of colors to use for different redundancy levels.\n        non_selected_color: Color for non-covered regions.\n\n    \"\"\"\n    df = self.resolve_peptides(peptides)\n    chain = self.resolve_chain(peptides, chain)\n\n    r_number, redundancy = peptide_redundancy(df)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n    )\n    color_lut = {i + 1: colors[i] for i in range(len(colors))}\n\n    data = []\n    tooltips = []\n    for rn, rv in zip(r_number, redundancy.clip(0, len(colors) - 1)):\n        tooltips.append(\n            {\n                f\"{self.structure.residue_name}\": int(rn),\n                \"tooltip\": f\"Redundancy: {rv} peptides\",\n            }\n        )\n\n        if rv == 0:\n            continue\n        color_elem = {\n            f\"{self.structure.residue_name}\": int(rn),\n            \"color\": color_lut[rv],\n        }\n        data.append(color_elem)\n\n    color_data = {\n        \"data\": self._augment_chain(data, chain),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self.view.color_data = color_data\n    self.view.tooltips = {\"data\": self._augment_chain(tooltips, chain)}\n    return self\n</code></pre>"},{"location":"reference/view/#view.StructureView.resolve_chain","title":"<code>resolve_chain(peptides, chain)</code>  <code>staticmethod</code>","text":"<p>Resolves the chain information from a Peptides object or a DataFrame.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>@staticmethod\ndef resolve_chain(peptides: Peptides | nw.DataFrame, chain: list[str] | None) -&gt; list[str]:\n    \"\"\"\n    Resolves the chain information from a Peptides object or a DataFrame.\n    \"\"\"\n    if isinstance(chain, list):\n        return chain\n    elif isinstance(peptides, Peptides):\n        return peptides.chain if peptides.chain else []\n    else:\n        return []\n</code></pre>"},{"location":"reference/view/#view.StructureView.resolve_peptides","title":"<code>resolve_peptides(peptides)</code>  <code>staticmethod</code>","text":"<p>Loads peptides as a DataFrame or returns the DataFrame.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>@staticmethod\ndef resolve_peptides(peptides: Peptides | nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Loads peptides as a DataFrame or returns the DataFrame.\n    \"\"\"\n    if isinstance(peptides, Peptides):\n        df = peptides.load()\n    else:\n        df = peptides\n\n    return df\n</code></pre>"},{"location":"reference/migration/v020/","title":"v020","text":"<p>Functions to help with migration from v020 datasets.</p>"},{"location":"reference/migration/v020/#migration.v020.get_peptides","title":"<code>get_peptides(spec, data_files, root_dir=Path.cwd(), **kwargs)</code>","text":"<p>Get peptides from the spec</p> Source code in <code>hdxms_datasets/migration/v020.py</code> <pre><code>def get_peptides(\n    spec: dict, data_files: dict, root_dir: Path = Path.cwd(), **kwargs\n) -&gt; list[Peptides]:\n    \"\"\"Get peptides from the spec\"\"\"\n\n    peptides = []\n    for deut_type, p_spec in spec.items():\n        data_file = data_files[p_spec[\"data_file\"]]\n        p = Peptides(\n            data_file=root_dir / data_file[\"filename\"],\n            data_format=data_file[\"format\"],\n            deuteration_type=deut_type,\n            filters=p_spec.get(\"filters\", {}),\n            **kwargs,\n            **get_metadata(p_spec),\n        )\n\n        peptides.append(p)\n\n    return peptides\n</code></pre>"},{"location":"reference/stable/v020/backend/","title":"backend","text":""},{"location":"reference/stable/v020/backend/#stable.v020.backend.get_backend","title":"<code>get_backend()</code>","text":"<p>Returns the backend used for data handling.</p> Source code in <code>hdxms_datasets/stable/v020/backend.py</code> <pre><code>def get_backend():\n    \"\"\"\n    Returns the backend used for data handling.\n    \"\"\"\n    try:\n        import polars  # NOQA: F401 # type: ignore[import]\n\n        return \"polars\"\n    except ImportError:\n        pass\n\n    try:\n        import pandas  # NOQA: F401 # type: ignore[import]\n\n        return \"pandas\"\n    except ImportError:\n        pass\n\n    try:\n        import modin  # NOQA: F401 # type: ignore[import]\n\n        return \"modin\"\n    except ImportError:\n        pass\n\n    try:\n        import pyarrow  # NOQA: F401 # type: ignore[import]\n\n        return \"pyarrow\"\n    except ImportError:\n        pass\n\n    raise ImportError(\"No suitable backend found. Please install pandas, polars, pyarrow or modin.\")\n</code></pre>"},{"location":"reference/stable/v020/convert/","title":"convert","text":""},{"location":"reference/stable/v020/convert/#stable.v020.convert.convert_rt","title":"<code>convert_rt(rt_str)</code>","text":"<p>convert hd examiner rt string to float example: \"7.44-7.65\" -&gt; 7.545</p> Source code in <code>hdxms_datasets/stable/v020/convert.py</code> <pre><code>def convert_rt(rt_str: str) -&gt; float:\n    \"\"\"convert hd examiner rt string to float\n    example: \"7.44-7.65\" -&gt; 7.545\n    \"\"\"\n    lower, upper = rt_str.split(\"-\")\n    mean = (float(lower) + float(upper)) / 2.0\n    return mean\n</code></pre>"},{"location":"reference/stable/v020/datasets/","title":"datasets","text":""},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet","title":"<code>DataSet</code>  <code>dataclass</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass DataSet:\n    data_id: str\n    \"\"\"Unique identifier for the dataset\"\"\"\n\n    states: dict[str, DataState]\n\n    metadata: dict = field(default_factory=dict)  # author, publication, etc\n\n    def get_state(self, state: str | int) -&gt; DataState:\n        \"\"\"\n        Get a specific state by name or index\n        \"\"\"\n        if isinstance(state, int):\n            return self.states[list(self.states.keys())[state]]\n        elif isinstance(state, str):\n            return self.states[state]\n        else:\n            raise TypeError(f\"Invalid type {type(state)} for state {state!r}\")\n\n    @classmethod\n    def from_spec(\n        cls,\n        hdx_spec: dict,\n        data_dir: Path,\n        data_id: Optional[str] = None,\n        metadata: Optional[dict] = None,\n    ):\n        data_id = data_id or uuid.uuid4().hex\n        data_files = parse_data_files(hdx_spec[\"data_files\"], data_dir)\n\n        peptide_table_files = {\n            k: f for k, f in data_files.items() if isinstance(f, PeptideTableFile)\n        }\n        peptides = parse_peptides(hdx_spec[\"peptides\"], peptide_table_files)\n\n        structure_file = next(\n            (f for f in data_files.values() if isinstance(f, StructureFile)), None\n        )\n\n        protein_spec = hdx_spec[\"protein\"]\n\n        for state_name, state_peptide_dict in peptides.items():\n            # Get protein information for this state\n            try:\n                protein_info = protein_spec[state_name]\n            except KeyError:\n                if ALLOW_MISSING_FIELDS:\n                    # Generate minimal protein info from peptides\n                    # take partially deuterated peptides as default,\n                    # if not available, take the first one\n                    peptide_df = state_peptide_dict.get(\n                        \"partially_deuterated\", next(iter(state_peptide_dict.values()))\n                    ).load()\n                    protein_info = default_protein_info(peptide_df)\n                    warnings.warn(\n                        f\"Generated minimal protein info for state '{state_name}'. \"\n                        f\"This is not recommended for production use.\"\n                    )\n                else:\n                    raise KeyError(\n                        f\"No protein information found for state '{state_name}'. \"\n                        f\"Use 'allow_missing_fields()' context manager to generate minimal info.\"\n                    )\n\n            structure_spec = hdx_spec.get(\"structures\", {}).get(\n                protein_info.get(\"structure\", \"\"), None\n            )\n            if structure_spec is None or structure_file is None:\n                if ALLOW_MISSING_FIELDS:\n                    # If no structure is specified, use a null structure\n                    structure = Structure.null_structure()\n                else:\n                    raise ValueError(\n                        f\"No structure information found for state '{state_name}'. \"\n                        f\"Use 'allow_missing_fields()' context manager to generate a null structure.\"\n                    )\n            else:\n                structure = Structure(\n                    data_file=structure_file,\n                    chain=structure_spec.get(\"chain\", []),  # empty list for all chains\n                    auth_residue_numbers=structure_spec.get(\"auth_residue_numbers\", False),\n                )\n\n            # Create and store the DataState object\n            self.states[state_name] = DataState(\n                name=state_name,\n                peptides=state_peptide_dict,\n                protein=protein_info,\n                structure=structure,\n            )\n\n        return cls(\n            data_id=data_id,\n            data_files=data_files,\n            hdx_specification=hdx_spec,\n            metadata=metadata or {},\n        )\n\n    @property\n    def protein_spec(self) -&gt; dict[str, ProteinInfo]:\n        \"\"\"Access the protein section of the specification\"\"\"\n        return self.hdx_specification.get(\"protein\", {})\n\n    @property\n    def peptide_spec(self) -&gt; dict:\n        return self.hdx_specification[\"peptides\"]\n\n    @property\n    def peptides_per_state(self) -&gt; dict[str, list[str]]:\n        \"\"\"Dictionary of state names and list of peptide sets for each state\"\"\"\n        return {state: list(spec) for state, spec in self.peptide_spec.items()}\n\n    def describe(\n        self,\n        peptide_template: Optional[str] = \"Total peptides: $num_peptides, timepoints: $timepoints\",\n        return_type: Union[Type[str], type[dict]] = str,\n    ) -&gt; Union[dict, str]:\n        def fmt_t(val: str | float | int) -&gt; str:\n            if isinstance(val, str):\n                return val\n            elif isinstance(val, (int, float)):\n                return f\"{val:.1f}\"\n            else:\n                raise TypeError(f\"Invalid type {type(val)} for value {val!r}\")\n\n        output_dict = {}\n        for state in self.states.values():\n            state_desc = {}\n            if peptide_template:\n                for peptides_types, peptides in state.peptides.items():\n                    peptide_df = peptides.load()\n                    timepoints = peptide_df[\"exposure\"].unique()\n                    mapping = {\n                        \"num_peptides\": len(peptide_df),\n                        \"num_timepoints\": len(timepoints),\n                        \"timepoints\": \", \".join([fmt_t(t) for t in timepoints]),\n                    }\n                    mapping[\"timepoints\"]\n                    state_desc[peptides_types] = Template(peptide_template).substitute(**mapping)\n\n            output_dict[state.name] = state_desc\n\n        if return_type is str:\n            return yaml.dump(output_dict, sort_keys=False)\n        elif return_type is dict:\n            return output_dict\n        else:\n            raise TypeError(f\"Invalid return type {return_type!r}\")\n\n    def cite(self) -&gt; str:\n        \"\"\"\n        Returns citation information\n        \"\"\"\n\n        raise NotImplementedError(\"Citation information is not yet implemented\")\n        try:\n            return self.metadata[\"publications\"]\n        except KeyError:\n            return \"No publication information available\"\n\n    def __len__(self) -&gt; int:\n        return len(self.states)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.data_id","title":"<code>data_id: str</code>  <code>instance-attribute</code>","text":"<p>Unique identifier for the dataset</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.peptides_per_state","title":"<code>peptides_per_state: dict[str, list[str]]</code>  <code>property</code>","text":"<p>Dictionary of state names and list of peptide sets for each state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.protein_spec","title":"<code>protein_spec: dict[str, ProteinInfo]</code>  <code>property</code>","text":"<p>Access the protein section of the specification</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.cite","title":"<code>cite()</code>","text":"<p>Returns citation information</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def cite(self) -&gt; str:\n    \"\"\"\n    Returns citation information\n    \"\"\"\n\n    raise NotImplementedError(\"Citation information is not yet implemented\")\n    try:\n        return self.metadata[\"publications\"]\n    except KeyError:\n        return \"No publication information available\"\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.get_state","title":"<code>get_state(state)</code>","text":"<p>Get a specific state by name or index</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_state(self, state: str | int) -&gt; DataState:\n    \"\"\"\n    Get a specific state by name or index\n    \"\"\"\n    if isinstance(state, int):\n        return self.states[list(self.states.keys())[state]]\n    elif isinstance(state, str):\n        return self.states[state]\n    else:\n        raise TypeError(f\"Invalid type {type(state)} for state {state!r}\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState","title":"<code>DataState</code>  <code>dataclass</code>","text":"<p>Encapsulates all data for a specific protein state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass DataState:\n    \"\"\"Encapsulates all data for a specific protein state\"\"\"\n\n    name: str\n    \"\"\"Name of the state\"\"\"\n\n    peptides: dict[str, Peptides]\n    \"\"\"Dictionary of peptide sets for this state\"\"\"\n\n    protein: ProteinInfo\n    \"\"\"Protein information for this state\"\"\"\n\n    structure: Structure\n    \"\"\"Optional structure file information for this state\"\"\"\n\n    def get_peptides(self, peptide_set: str) -&gt; Peptides:\n        \"\"\"Get a specific peptide set\"\"\"\n        try:\n            return self.peptides[peptide_set]\n        except KeyError:\n            raise KeyError(f\"Peptide set '{peptide_set}' not found in state '{self.name}'\")\n\n    def get_sequence(self) -&gt; str:\n        \"\"\"Get the protein sequence for this state\"\"\"\n        return self.protein[\"sequence\"]\n\n    def get_protein_property(self, property_name: str) -&gt; Any:\n        \"\"\"Get a specific protein property\"\"\"\n        try:\n            return self.protein[property_name]\n        except KeyError:\n            raise KeyError(f\"Property '{property_name}' not found in state '{self.name}'\")\n\n    def compute_uptake_metrics(self) -&gt; nw.DataFrame:\n        \"\"\"Compute uptake metrics for this state\"\"\"\n        peptide_types = list(self.peptides.keys())\n\n        if \"fully_deuterated\" in peptide_types:\n            fd = self.peptides[\"fully_deuterated\"].load()\n        else:\n            fd = None\n\n        if \"non_deuterated\" in peptide_types:\n            nd = self.peptides[\"non_deuterated\"].load()\n        else:\n            nd = None\n\n        pd = self.peptides[\"partially_deuterated\"].load()\n\n        merged = process.merge_peptides(\n            partially_deuterated=pd, fully_deuterated=fd, non_deuterated=nd\n        )\n        return process.compute_uptake_metrics(merged)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Name of the state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.peptides","title":"<code>peptides: dict[str, Peptides]</code>  <code>instance-attribute</code>","text":"<p>Dictionary of peptide sets for this state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.protein","title":"<code>protein: ProteinInfo</code>  <code>instance-attribute</code>","text":"<p>Protein information for this state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.structure","title":"<code>structure: Structure</code>  <code>instance-attribute</code>","text":"<p>Optional structure file information for this state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.compute_uptake_metrics","title":"<code>compute_uptake_metrics()</code>","text":"<p>Compute uptake metrics for this state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def compute_uptake_metrics(self) -&gt; nw.DataFrame:\n    \"\"\"Compute uptake metrics for this state\"\"\"\n    peptide_types = list(self.peptides.keys())\n\n    if \"fully_deuterated\" in peptide_types:\n        fd = self.peptides[\"fully_deuterated\"].load()\n    else:\n        fd = None\n\n    if \"non_deuterated\" in peptide_types:\n        nd = self.peptides[\"non_deuterated\"].load()\n    else:\n        nd = None\n\n    pd = self.peptides[\"partially_deuterated\"].load()\n\n    merged = process.merge_peptides(\n        partially_deuterated=pd, fully_deuterated=fd, non_deuterated=nd\n    )\n    return process.compute_uptake_metrics(merged)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.get_peptides","title":"<code>get_peptides(peptide_set)</code>","text":"<p>Get a specific peptide set</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_peptides(self, peptide_set: str) -&gt; Peptides:\n    \"\"\"Get a specific peptide set\"\"\"\n    try:\n        return self.peptides[peptide_set]\n    except KeyError:\n        raise KeyError(f\"Peptide set '{peptide_set}' not found in state '{self.name}'\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.get_protein_property","title":"<code>get_protein_property(property_name)</code>","text":"<p>Get a specific protein property</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_protein_property(self, property_name: str) -&gt; Any:\n    \"\"\"Get a specific protein property\"\"\"\n    try:\n        return self.protein[property_name]\n    except KeyError:\n        raise KeyError(f\"Property '{property_name}' not found in state '{self.name}'\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.get_sequence","title":"<code>get_sequence()</code>","text":"<p>Get the protein sequence for this state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_sequence(self) -&gt; str:\n    \"\"\"Get the protein sequence for this state\"\"\"\n    return self.protein[\"sequence\"]\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar","title":"<code>PDBeMolstar</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>class PDBeMolstar:\n    def __init__(self, structure: Structure, **kwargs: Any):\n        \"\"\"\n        Initialize the PDBeMolstar visualization namespace.\n\n        Args:\n            structure: The structure to visualize.\n            **kwargs: Additional keyword arguments for customization.\n        \"\"\"\n        self.structure = structure\n        self._molstar_kwargs = kwargs\n\n    @property\n    def residue_name(self) -&gt; str:\n        \"\"\"\n        Returns the residue name based on whether auth residue numbers are used.\n        \"\"\"\n        return \"auth_residue_number\" if self.structure.auth_residue_numbers else \"residue_number\"\n\n    @property\n    def chain_name(self) -&gt; str:\n        \"\"\"\n        Returns the chain name based on whether auth chain labels are used.\n        \"\"\"\n        return \"auth_asym_id\" if self.structure.auth_chain_labels else \"struct_asym_id\"\n\n    def show(self, hide_water=True, **kwargs) -&gt; PDBeMolstar:\n        from ipymolstar import PDBeMolstar\n\n        molstar_kwargs = {**self._molstar_kwargs, **kwargs}\n        return PDBeMolstar(\n            custom_data=self.structure.data_file.pdbemolstar_custom_data(),\n            hide_water=hide_water,\n            **molstar_kwargs,\n        )\n\n    def color_peptide(\n        self, start: int, end: int, color: str = \"red\", non_selected_color: str = \"lightgray\"\n    ) -&gt; PDBeMolstar:\n        c_dict = {\n            \"start_\" + self.residue_name: start,\n            \"end_\" + self.residue_name: end,\n            \"color\": color,\n        }\n\n        data = self._augment_chain([c_dict])\n\n        color_data = {\n            \"data\": data,\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = None\n        return self\n\n    def peptide_coverage(\n        self, df, start=\"start\", end=\"end\", color=\"red\", non_selected_color: str = \"lightgray\"\n    ) -&gt; PDBeMolstar:\n        intervals = contiguous_peptides(df, start=start, end=end)\n\n        data = []\n        for start, end in intervals:\n            elem = {\n                f\"start_{self.residue_name}\": start,\n                f\"end_{self.residue_name}\": end,\n                \"color\": color,\n            }\n            data.append(elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = None\n        return self\n\n    def non_overlapping_peptides(\n        self,\n        df,\n        start=\"start\",\n        end=\"end\",\n        colors: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; PDBeMolstar:\n        \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n        intervals = non_overlapping_peptides(df, start=start, end=end)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n        )\n\n        data = []\n        for (start, end), color in zip(intervals, itertools.cycle(colors)):\n            elem = {\n                f\"start_{self.residue_name}\": start,\n                f\"end_{self.residue_name}\": end,\n                \"color\": color,\n            }\n            data.append(elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = None\n        return self\n\n    def peptide_redundancy(\n        self,\n        df,\n        start=\"start\",\n        end=\"end\",\n        colors: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; PDBeMolstar:\n        \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n        r_number, redundancy = peptide_redundancy(df, start=start, end=end)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n        )\n        color_lut = {i + 1: colors[i] for i in range(len(colors))}\n\n        data = []\n        tooltips = []\n        for rn, rv in zip(r_number, redundancy.clip(0, len(colors) - 1)):\n            tooltips.append(\n                {\n                    f\"{self.residue_name}\": int(rn),\n                    \"tooltip\": f\"Redundancy: {rv} peptides\",\n                }\n            )\n\n            if rv == 0:\n                continue\n            color_elem = {\n                f\"{self.residue_name}\": int(rn),\n                \"color\": color_lut[rv],\n            }\n            data.append(color_elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = {\"data\": self._augment_chain(tooltips)}\n        return self\n\n    def _augment_chain(self, data: list[dict[str, ValueType]]) -&gt; list[dict[str, ValueType]]:\n        \"\"\"augment a list of data with chain information\"\"\"\n        if self.structure.chain:\n            aug_data = []\n            for elem, chain in itertools.product(data, self.structure.chain):\n                aug_data.append(elem | {self.chain_name: chain})\n        else:\n            aug_data = data\n\n        return aug_data\n\n    def _repr_mimebundle_(self, include=None, exclude=None):\n        return self.show()._repr_mimebundle_(\n            include=include, exclude=exclude\n        )  # or however ipymolstar handles display\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.chain_name","title":"<code>chain_name: str</code>  <code>property</code>","text":"<p>Returns the chain name based on whether auth chain labels are used.</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.residue_name","title":"<code>residue_name: str</code>  <code>property</code>","text":"<p>Returns the residue name based on whether auth residue numbers are used.</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.__init__","title":"<code>__init__(structure, **kwargs)</code>","text":"<p>Initialize the PDBeMolstar visualization namespace.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Structure</code> <p>The structure to visualize.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for customization.</p> <code>{}</code> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def __init__(self, structure: Structure, **kwargs: Any):\n    \"\"\"\n    Initialize the PDBeMolstar visualization namespace.\n\n    Args:\n        structure: The structure to visualize.\n        **kwargs: Additional keyword arguments for customization.\n    \"\"\"\n    self.structure = structure\n    self._molstar_kwargs = kwargs\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.non_overlapping_peptides","title":"<code>non_overlapping_peptides(df, start='start', end='end', colors=None, non_selected_color='lightgray')</code>","text":"<p>selects a set of non-overlapping peptides to display on the structure</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def non_overlapping_peptides(\n    self,\n    df,\n    start=\"start\",\n    end=\"end\",\n    colors: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n) -&gt; PDBeMolstar:\n    \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n    intervals = non_overlapping_peptides(df, start=start, end=end)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n    )\n\n    data = []\n    for (start, end), color in zip(intervals, itertools.cycle(colors)):\n        elem = {\n            f\"start_{self.residue_name}\": start,\n            f\"end_{self.residue_name}\": end,\n            \"color\": color,\n        }\n        data.append(elem)\n\n    color_data = {\n        \"data\": self._augment_chain(data),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self._molstar_kwargs[\"color_data\"] = color_data\n    self._molstar_kwargs[\"tooltips\"] = None\n    return self\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.peptide_redundancy","title":"<code>peptide_redundancy(df, start='start', end='end', colors=None, non_selected_color='lightgray')</code>","text":"<p>selects a set of non-overlapping peptides to display on the structure</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def peptide_redundancy(\n    self,\n    df,\n    start=\"start\",\n    end=\"end\",\n    colors: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n) -&gt; PDBeMolstar:\n    \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n    r_number, redundancy = peptide_redundancy(df, start=start, end=end)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n    )\n    color_lut = {i + 1: colors[i] for i in range(len(colors))}\n\n    data = []\n    tooltips = []\n    for rn, rv in zip(r_number, redundancy.clip(0, len(colors) - 1)):\n        tooltips.append(\n            {\n                f\"{self.residue_name}\": int(rn),\n                \"tooltip\": f\"Redundancy: {rv} peptides\",\n            }\n        )\n\n        if rv == 0:\n            continue\n        color_elem = {\n            f\"{self.residue_name}\": int(rn),\n            \"color\": color_lut[rv],\n        }\n        data.append(color_elem)\n\n    color_data = {\n        \"data\": self._augment_chain(data),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self._molstar_kwargs[\"color_data\"] = color_data\n    self._molstar_kwargs[\"tooltips\"] = {\"data\": self._augment_chain(tooltips)}\n    return self\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PeptideMetadata","title":"<code>PeptideMetadata</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for peptide metadata</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>class PeptideMetadata(TypedDict):\n    \"\"\"TypedDict for peptide metadata\"\"\"\n\n    pH: float  # pH of the experiment (pH read, uncorrected)\n    temperature: float  # Temperature of the experiment (K)\n    d_percentage: float  # Deuteration percentage\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PeptideTableFile","title":"<code>PeptideTableFile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataFile</code></p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass(frozen=True)\nclass PeptideTableFile(DataFile):\n    name: str\n\n    filepath_or_buffer: Union[Path, StringIO, BytesIO]\n\n    format: Optional[HDXFormat] = None\n\n    extension: Optional[str] = None\n    \"\"\"File extension, e.g. .csv, in case of a file-like object\"\"\"\n\n    def __post_init__(self):\n        if self.format is None:\n            df = self.read()\n            fmt = identify_format(df, exact=False)\n            self.__dict__[\"format\"] = fmt\n\n    def read(self) -&gt; nw.DataFrame:\n        if isinstance(self.filepath_or_buffer, (StringIO, BytesIO)):\n            extension = self.extension\n            assert isinstance(extension, str), \"File-like object must have an extension\"\n        else:\n            extension = self.filepath_or_buffer.suffix[1:]\n\n        if extension == \"csv\":\n            return read_csv(self.filepath_or_buffer)\n        else:\n            raise ValueError(f\"Invalid file extension {self.extension!r}\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PeptideTableFile.extension","title":"<code>extension: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>File extension, e.g. .csv, in case of a file-like object</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Peptides","title":"<code>Peptides</code>  <code>dataclass</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass Peptides:\n    data_file: PeptideTableFile\n    filters: dict[str, ValueType | list[ValueType]] = field(default_factory=dict)\n    metadata: PeptideMetadata | None = None\n\n    def load(\n        self,\n        convert: bool = True,\n        aggregate: bool | None = None,\n        sort_rows: bool = True,\n        sort_columns: bool = True,\n        drop_null: bool = True,\n    ) -&gt; nw.DataFrame:\n        df = process.apply_filters(self.data_file.read(), **self.filters)\n\n        is_aggregated = getattr(self.data_file.format, \"aggregated\", False)\n        if aggregate is None:\n            aggregate = not is_aggregated\n\n        if aggregate and is_aggregated:\n            warnings.warn(\"Data format is pre-aggregated. Aggregation will be skipped.\")\n            aggregate = False\n\n        if not convert and aggregate:\n            warnings.warn(\"Cannot aggregate data without conversion. Aggeregation will be skipped.\")\n            aggregate = False\n\n        if not convert and sort_rows:\n            warnings.warn(\"Cannot sort rows without conversion. Sorting will be skipped.\")\n            sort_rows = False\n\n        if not convert and sort_columns:\n            warnings.warn(\"Cannot sort columns without conversion. Sorting will be skipped.\")\n            sort_columns = False\n\n        if convert:\n            assert self.data_file.format is not None, (\n                \"Data file format must be set before conversion\"\n            )\n            df = self.data_file.format.convert(df)\n\n        if aggregate:\n            df = process.aggregate(df)\n\n        if drop_null:\n            df = process.drop_null_columns(df)\n\n        if sort_rows:\n            df = process.sort_rows(df)\n\n        if sort_columns:\n            df = process.sort_columns(df)\n\n        return df\n\n    def get_temperature(self) -&gt; Optional[float]:\n        \"\"\"Get the temperature of the experiment\"\"\"\n\n        if self.metadata is None:\n            return None\n        elif \"temperature\" not in self.metadata:\n            return None\n\n        temperature = self.metadata[\"temperature\"]\n        return temperature\n\n    def get_pH(self) -&gt; Optional[float]:\n        \"\"\"Get the pH of the experiment\"\"\"\n        if self.metadata is None:\n            return None\n        elif \"pH\" not in self.metadata:\n            return None\n        return self.metadata[\"pH\"]\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Peptides.get_pH","title":"<code>get_pH()</code>","text":"<p>Get the pH of the experiment</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_pH(self) -&gt; Optional[float]:\n    \"\"\"Get the pH of the experiment\"\"\"\n    if self.metadata is None:\n        return None\n    elif \"pH\" not in self.metadata:\n        return None\n    return self.metadata[\"pH\"]\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Peptides.get_temperature","title":"<code>get_temperature()</code>","text":"<p>Get the temperature of the experiment</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_temperature(self) -&gt; Optional[float]:\n    \"\"\"Get the temperature of the experiment\"\"\"\n\n    if self.metadata is None:\n        return None\n    elif \"temperature\" not in self.metadata:\n        return None\n\n    temperature = self.metadata[\"temperature\"]\n    return temperature\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.ProteinInfo","title":"<code>ProteinInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for protein information in a state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>class ProteinInfo(TypedDict):\n    \"\"\"TypedDict for protein information in a state\"\"\"\n\n    sequence: str  # Amino acid sequence\n    n_term: int  # N-terminal residue number\n    c_term: int  # C-terminal residue number\n    mutations: NotRequired[list[str]]  # Optional list of mutations\n    oligomeric_state: NotRequired[int]  # Optional oligomeric state\n    ligand: NotRequired[str]  # Optional bound ligand information\n    uniprot_id: NotRequired[str]  # Optional UniProt ID\n    molecular_weight: NotRequired[float]  # Optional molecular weight in Da\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Structure","title":"<code>Structure</code>  <code>dataclass</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass Structure:\n    data_file: StructureFile\n    chain: list[str] = field(default_factory=list)  # empty list for all chains\n    auth_residue_numbers: bool = field(default=False)\n    auth_chain_labels: bool = field(default=False)\n\n    _molstar_kwargs: dict[str, Any] = field(default_factory=dict, init=False)\n\n    def pdbemolstar(self, **kwargs):\n        \"\"\"Return a PDBeMolstar visualization namespace.\"\"\"\n        return PDBeMolstar(self, **kwargs)\n\n    @classmethod\n    def null_structure(cls) -&gt; Structure:\n        \"\"\"\n        Returns a null structure with no data.\n        This is useful for cases where no structure is available.\n        \"\"\"\n        return cls(\n            data_file=StructureFile(\n                name=\"null\",\n                filepath_or_buffer=BytesIO(),\n                format=\"null\",\n                extension=\".null\",\n            ),\n            auth_residue_numbers=False,\n        )\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Structure.null_structure","title":"<code>null_structure()</code>  <code>classmethod</code>","text":"<p>Returns a null structure with no data. This is useful for cases where no structure is available.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@classmethod\ndef null_structure(cls) -&gt; Structure:\n    \"\"\"\n    Returns a null structure with no data.\n    This is useful for cases where no structure is available.\n    \"\"\"\n    return cls(\n        data_file=StructureFile(\n            name=\"null\",\n            filepath_or_buffer=BytesIO(),\n            format=\"null\",\n            extension=\".null\",\n        ),\n        auth_residue_numbers=False,\n    )\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Structure.pdbemolstar","title":"<code>pdbemolstar(**kwargs)</code>","text":"<p>Return a PDBeMolstar visualization namespace.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def pdbemolstar(self, **kwargs):\n    \"\"\"Return a PDBeMolstar visualization namespace.\"\"\"\n    return PDBeMolstar(self, **kwargs)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.StructureFile","title":"<code>StructureFile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataFile</code></p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass(frozen=True)\nclass StructureFile(DataFile):\n    name: str\n\n    filepath_or_buffer: Union[Path, BytesIO]\n\n    format: str\n\n    extension: Optional[str] = None\n    \"\"\"File extension, e.g. .pdf, in case of a file-like object\"\"\"\n\n    def pdbemolstar_custom_data(self):\n        \"\"\"\n        Returns a dictionary with custom data for PDBeMolstar visualization.\n        \"\"\"\n\n        if self.format in [\"bcif\"]:\n            binary = True\n        else:\n            binary = False\n\n        if isinstance(self.filepath_or_buffer, BytesIO):\n            data = self.filepath_or_buffer.getvalue()\n        elif isinstance(self.filepath_or_buffer, Path):\n            if self.filepath_or_buffer.is_file():\n                data = self.filepath_or_buffer.read_bytes()\n            else:\n                raise ValueError(f\"Path {self.filepath_or_buffer} is not a file.\")\n\n        return {\n            \"data\": data,\n            \"format\": self.format,\n            \"binary\": binary,\n        }\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.StructureFile.extension","title":"<code>extension: Optional[str] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>File extension, e.g. .pdf, in case of a file-like object</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.StructureFile.pdbemolstar_custom_data","title":"<code>pdbemolstar_custom_data()</code>","text":"<p>Returns a dictionary with custom data for PDBeMolstar visualization.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def pdbemolstar_custom_data(self):\n    \"\"\"\n    Returns a dictionary with custom data for PDBeMolstar visualization.\n    \"\"\"\n\n    if self.format in [\"bcif\"]:\n        binary = True\n    else:\n        binary = False\n\n    if isinstance(self.filepath_or_buffer, BytesIO):\n        data = self.filepath_or_buffer.getvalue()\n    elif isinstance(self.filepath_or_buffer, Path):\n        if self.filepath_or_buffer.is_file():\n            data = self.filepath_or_buffer.read_bytes()\n        else:\n            raise ValueError(f\"Path {self.filepath_or_buffer} is not a file.\")\n\n    return {\n        \"data\": data,\n        \"format\": self.format,\n        \"binary\": binary,\n    }\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.allow_missing_fields","title":"<code>allow_missing_fields(allow=True)</code>","text":"<p>Context manager to temporarily allow missing protein information</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@contextmanager\ndef allow_missing_fields(allow=True):\n    \"\"\"Context manager to temporarily allow missing protein information\"\"\"\n    global ALLOW_MISSING_FIELDS\n    old_value = ALLOW_MISSING_FIELDS\n    ALLOW_MISSING_FIELDS = allow\n    try:\n        yield\n    finally:\n        ALLOW_MISSING_FIELDS = old_value\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.create_dataset","title":"<code>create_dataset(target_dir, author_name, tag=None, template_dir=TEMPLATE_DIR)</code>","text":"<p>Create a dataset in the specified target directory.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>Path</code> <p>The directory where the dataset will be created.</p> required <code>author_name</code> <code>str</code> <p>The name of the author of the dataset.</p> required <code>tag</code> <code>Optional[str]</code> <p>An optional tag to append to the directory name. Defaults to None.</p> <code>None</code> <code>template_dir</code> <code>Path</code> <p>The directory containing the template files for the dataset. Defaults to TEMPLATE_DIR.</p> <code>TEMPLATE_DIR</code> <p>Returns:</p> Type Description <code>str</code> <p>The id of the created dataset.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def create_dataset(\n    target_dir: Path,\n    author_name: str,\n    tag: Optional[str] = None,\n    template_dir: Path = TEMPLATE_DIR,\n) -&gt; str:\n    \"\"\"\n    Create a dataset in the specified target directory.\n\n    Args:\n        target_dir: The directory where the dataset will be created.\n        author_name: The name of the author of the dataset.\n        tag: An optional tag to append to the directory name. Defaults to None.\n        template_dir: The directory containing the template files for the dataset. Defaults to TEMPLATE_DIR.\n\n    Returns:\n        The id of the created dataset.\n\n    \"\"\"\n    dirname = str(int(time.time()))\n\n    if tag:\n        dirname += f\"_{tag}\"\n\n    dirname += f\"_{author_name}\"\n\n    target_dir.mkdir(parents=True, exist_ok=True)\n    target_dir = target_dir / dirname\n\n    shutil.copytree(template_dir, target_dir)\n\n    (target_dir / \"readme.md\").write_text(f\"# {dirname}\")\n\n    return dirname\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.parse_data_files","title":"<code>parse_data_files(data_file_spec, data_dir)</code>","text":"<p>Parse data file specifications from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_spec</code> <code>dict</code> <p>Dictionary with data file specifications.</p> required <code>data_dir</code> <code>Path</code> <p>Path to data directory.</p> required <p>Returns:</p> Type Description <code>dict[str, DataFile]</code> <p>Dictionary with parsed data file specifications.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def parse_data_files(data_file_spec: dict, data_dir: Path) -&gt; dict[str, DataFile]:\n    \"\"\"\n    Parse data file specifications from a YAML file.\n\n    Args:\n        data_file_spec: Dictionary with data file specifications.\n        data_dir: Path to data directory.\n\n    Returns:\n        Dictionary with parsed data file specifications.\n    \"\"\"\n\n    data_files = {}\n    for name, spec in data_file_spec.items():\n        fpath = Path(data_dir / spec[\"filename\"])\n\n        if spec[\"type\"] == \"structure\":\n            format = spec[\"format\"]\n            data_file = StructureFile(\n                name=name,\n                filepath_or_buffer=fpath,\n                format=format,\n                extension=fpath.suffix,\n            )\n        elif spec[\"type\"] == \"peptide_table\":\n            format = FMT_LUT.get(spec[\"format\"], None)\n            data_file = PeptideTableFile(\n                name=name,\n                filepath_or_buffer=fpath,\n                format=format,\n                extension=fpath.suffix,\n            )\n        else:\n            raise ValueError(f\"Unknown data file type {spec['type']} for {name}.\")\n\n        data_files[name] = data_file\n\n    return data_files\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.parse_peptides","title":"<code>parse_peptides(peptides_spec, data_files)</code>","text":"<p>Parse the peptides specification and return a dictionary of PeptideTableFile objects.</p> <p>Parameters:</p> Name Type Description Default <code>peptides_spec</code> <code>dict[str, Any]</code> <p>Dictionary containing peptide specifications.</p> required <code>data_files</code> <code>dict[str, PeptideTableFile]</code> <p>Dictionary of available data files.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Peptides]]</code> <p>Dictionary of Peptides dictionaries.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def parse_peptides(\n    peptides_spec: dict[str, Any], data_files: dict[str, PeptideTableFile]\n) -&gt; dict[str, dict[str, Peptides]]:\n    \"\"\"\n    Parse the peptides specification and return a dictionary of PeptideTableFile objects.\n\n    Args:\n        peptides_spec: Dictionary containing peptide specifications.\n        data_files: Dictionary of available data files.\n\n    Returns:\n        Dictionary of Peptides dictionaries.\n    \"\"\"\n    peptides = {}\n    for state_name, state_peptides in peptides_spec.items():\n        # Build peptide dictionary for this state\n        state_peptide_dict = {}\n\n        # Process each peptide set for this state\n        for peptide_type, peptide_spec in state_peptides.items():\n            peptide_obj = Peptides(\n                data_file=data_files[peptide_spec[\"data_file\"]],\n                filters=peptide_spec[\"filters\"],\n                metadata=peptide_spec.get(\"metadata\", None),\n            )\n\n            # Add to state-specific dictionary\n            state_peptide_dict[peptide_type] = peptide_obj\n\n        peptides[state_name] = state_peptide_dict\n\n    return peptides\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.parse_structures","title":"<code>parse_structures(structures_spec, data_file)</code>","text":"<p>Parse the structures specification and return a dictionary of Structure objects.</p> <p>Parameters:</p> Name Type Description Default <code>structures_spec</code> <code>dict[str, Any]</code> <p>Dictionary containing structure specifications.</p> required <code>data_file</code> <code>StructureFile</code> <p>StructureFile object for the structure data.</p> required <p>Returns:</p> Type Description <code>dict[str, Structure]</code> <p>Dictionary of Structure objects keyed by structure name.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def parse_structures(\n    structures_spec: dict[str, Any], data_file: StructureFile\n) -&gt; dict[str, Structure]:\n    \"\"\"\n    Parse the structures specification and return a dictionary of Structure objects.\n\n    Args:\n        structures_spec: Dictionary containing structure specifications.\n        data_file: StructureFile object for the structure data.\n\n    Returns:\n        Dictionary of Structure objects keyed by structure name.\n    \"\"\"\n    structures = {}\n    for name, spec in structures_spec.items():\n        structure = Structure(\n            data_file=data_file,\n            chain=spec.get(\"chain\", []),  # empty list for all chains\n            auth_residue_numbers=spec.get(\"auth_residue_numbers\", False),\n        )\n        structures[name] = structure\n    return structures\n</code></pre>"},{"location":"reference/stable/v020/datavault/","title":"datavault","text":""},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.DataVault","title":"<code>DataVault</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>class DataVault:\n    def __init__(self, cache_dir: Union[Path, str]):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\n\n    @property\n    def datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the cache dir\"\"\"\n        return [d.stem for d in self.cache_dir.iterdir() if self.is_dataset(d)]\n\n    @staticmethod\n    def is_dataset(path: Path) -&gt; bool:\n        \"\"\"\n        Checks if the supplied path is a HDX-MS dataset.\n        \"\"\"\n\n        return (path / \"hdx_spec.yaml\").exists()\n\n    def clear_cache(self) -&gt; None:\n        for dir in self.cache_dir.iterdir():\n            shutil.rmtree(dir)\n\n    def get_metadata(self, data_id: str) -&gt; dict:\n        return yaml.safe_load((self.cache_dir / data_id / \"metadata.yaml\").read_text())\n\n    def load_dataset(self, data_id: str) -&gt; DataSet:\n        hdx_spec = yaml.safe_load((self.cache_dir / data_id / \"hdx_spec.yaml\").read_text())\n        dataset_metadata = self.get_metadata(data_id)\n\n        return DataSet.from_spec(\n            hdx_spec=hdx_spec,\n            data_dir=self.cache_dir / data_id,\n            data_id=data_id,\n            metadata=dataset_metadata,\n        )\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.DataVault.datasets","title":"<code>datasets: list[str]</code>  <code>property</code>","text":"<p>List of available datasets in the cache dir</p>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.DataVault.is_dataset","title":"<code>is_dataset(path)</code>  <code>staticmethod</code>","text":"<p>Checks if the supplied path is a HDX-MS dataset.</p> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>@staticmethod\ndef is_dataset(path: Path) -&gt; bool:\n    \"\"\"\n    Checks if the supplied path is a HDX-MS dataset.\n    \"\"\"\n\n    return (path / \"hdx_spec.yaml\").exists()\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.RemoteDataVault","title":"<code>RemoteDataVault</code>","text":"<p>               Bases: <code>DataVault</code></p> <p>A vault for HDX-MS datasets, with the ability to fetch datasets from a remote repository.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Union[Path, str]</code> <p>Directory to store downloaded datasets.</p> required <code>remote_url</code> <code>str</code> <p>URL of the remote repository (default: DATABASE_URL).</p> <code>DATABASE_URL</code> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>class RemoteDataVault(DataVault):\n    \"\"\"\n    A vault for HDX-MS datasets, with the ability to fetch datasets from a remote repository.\n\n    Args:\n        cache_dir: Directory to store downloaded datasets.\n        remote_url: URL of the remote repository (default: DATABASE_URL).\n    \"\"\"\n\n    def __init__(self, cache_dir: Union[Path, str], remote_url: str = DATABASE_URL):\n        super().__init__(cache_dir)\n        self.remote_url = remote_url\n\n    def get_index(self) -&gt; nw.DataFrame:\n        \"\"\"Retrieves the index of available datasets\n\n        on success, returns the index dataframe and\n        stores as `remote_index` attribute.\n\n        \"\"\"\n\n        url = urllib.parse.urljoin(self.remote_url, \"index.csv\")\n        response = requests.get(url)\n\n        if response.ok:\n            (self.cache_dir / \"index.csv\").write_bytes(response.content)\n            return nw.read_csv(str(self.cache_dir / \"index.csv\"), backend=BACKEND)\n        else:\n            raise urllib.error.HTTPError(\n                url,\n                response.status_code,\n                \"Error downloading database index\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n    def fetch_dataset(self, data_id: str) -&gt; bool:\n        \"\"\"\n        Download a dataset from the online repository to the cache dir\n\n        Args:\n            data_id: The ID of the dataset to download.\n\n        Returns:\n            `True` if the dataset was downloaded successfully, `False`  otherwise.\n        \"\"\"\n\n        output_pth = self.cache_dir / data_id\n        if output_pth.exists():\n            return False\n        else:\n            output_pth.mkdir()\n\n        dataset_url = urllib.parse.urljoin(self.remote_url, data_id + \"/\")\n\n        files = [\"hdx_spec.yaml\", \"metadata.yaml\"]\n        optional_files = [\"CITATION.cff\"]\n        hdx_spec = None\n        for f in files + optional_files:\n            url = urllib.parse.urljoin(dataset_url, f)\n            response = requests.get(url)\n\n            if response.ok:\n                (output_pth / f).write_bytes(response.content)\n\n            elif f in files:\n                raise urllib.error.HTTPError(\n                    url,\n                    response.status_code,\n                    f\"Error for file {f!r}\",\n                    response.headers,  # type: ignore\n                    None,\n                )\n\n            if f == \"hdx_spec.yaml\":\n                hdx_spec = yaml.safe_load(response.text)\n\n        if hdx_spec is None:\n            raise ValueError(f\"Could not find HDX spec for data_id {data_id!r}\")\n\n        data_pth = output_pth / \"data\"\n        data_pth.mkdir()\n\n        for file_spec in hdx_spec[\"data_files\"].values():\n            filename = file_spec[\"filename\"]\n            f_url = urllib.parse.urljoin(dataset_url, filename)\n            response = requests.get(f_url)\n\n            if response.ok:\n                (output_pth / filename).write_bytes(response.content)\n            else:\n                raise urllib.error.HTTPError(\n                    f_url,\n                    response.status_code,\n                    f\"Error for data file {filename!r}\",\n                    response.headers,  # type: ignore\n                    None,\n                )\n\n        return True\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.RemoteDataVault.fetch_dataset","title":"<code>fetch_dataset(data_id)</code>","text":"<p>Download a dataset from the online repository to the cache dir</p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>The ID of the dataset to download.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the dataset was downloaded successfully, <code>False</code>  otherwise.</p> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>def fetch_dataset(self, data_id: str) -&gt; bool:\n    \"\"\"\n    Download a dataset from the online repository to the cache dir\n\n    Args:\n        data_id: The ID of the dataset to download.\n\n    Returns:\n        `True` if the dataset was downloaded successfully, `False`  otherwise.\n    \"\"\"\n\n    output_pth = self.cache_dir / data_id\n    if output_pth.exists():\n        return False\n    else:\n        output_pth.mkdir()\n\n    dataset_url = urllib.parse.urljoin(self.remote_url, data_id + \"/\")\n\n    files = [\"hdx_spec.yaml\", \"metadata.yaml\"]\n    optional_files = [\"CITATION.cff\"]\n    hdx_spec = None\n    for f in files + optional_files:\n        url = urllib.parse.urljoin(dataset_url, f)\n        response = requests.get(url)\n\n        if response.ok:\n            (output_pth / f).write_bytes(response.content)\n\n        elif f in files:\n            raise urllib.error.HTTPError(\n                url,\n                response.status_code,\n                f\"Error for file {f!r}\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n        if f == \"hdx_spec.yaml\":\n            hdx_spec = yaml.safe_load(response.text)\n\n    if hdx_spec is None:\n        raise ValueError(f\"Could not find HDX spec for data_id {data_id!r}\")\n\n    data_pth = output_pth / \"data\"\n    data_pth.mkdir()\n\n    for file_spec in hdx_spec[\"data_files\"].values():\n        filename = file_spec[\"filename\"]\n        f_url = urllib.parse.urljoin(dataset_url, filename)\n        response = requests.get(f_url)\n\n        if response.ok:\n            (output_pth / filename).write_bytes(response.content)\n        else:\n            raise urllib.error.HTTPError(\n                f_url,\n                response.status_code,\n                f\"Error for data file {filename!r}\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n    return True\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.RemoteDataVault.get_index","title":"<code>get_index()</code>","text":"<p>Retrieves the index of available datasets</p> <p>on success, returns the index dataframe and stores as <code>remote_index</code> attribute.</p> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>def get_index(self) -&gt; nw.DataFrame:\n    \"\"\"Retrieves the index of available datasets\n\n    on success, returns the index dataframe and\n    stores as `remote_index` attribute.\n\n    \"\"\"\n\n    url = urllib.parse.urljoin(self.remote_url, \"index.csv\")\n    response = requests.get(url)\n\n    if response.ok:\n        (self.cache_dir / \"index.csv\").write_bytes(response.content)\n        return nw.read_csv(str(self.cache_dir / \"index.csv\"), backend=BACKEND)\n    else:\n        raise urllib.error.HTTPError(\n            url,\n            response.status_code,\n            \"Error downloading database index\",\n            response.headers,  # type: ignore\n            None,\n        )\n</code></pre>"},{"location":"reference/stable/v020/expr/","title":"expr","text":""},{"location":"reference/stable/v020/formats/","title":"formats","text":""},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_v3_cluster","title":"<code>DynamX_v3_cluster</code>","text":"Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class DynamX_v3_cluster:\n    columns = [\n        \"Protein\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"Modification\",\n        \"Fragment\",\n        \"MaxUptake\",\n        \"MHP\",\n        \"State\",\n        \"Exposure\",\n        \"File\",\n        \"z\",\n        \"RT\",\n        \"Inten\",\n        \"Center\",\n    ]\n    state_name = \"State\"\n    exposure_name = \"Exposure\"\n    aggregated = False\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        return from_dynamx_cluster(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_v3_cluster.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    return from_dynamx_cluster(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_vx_state","title":"<code>DynamX_vx_state</code>","text":"<p>There are also DynamX state data files which do not have 'Modification' and 'Fragment' columns. not sure which version this is.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class DynamX_vx_state:\n    \"\"\"There are also DynamX state data files which do not have 'Modification' and 'Fragment' columns.\n    not sure which version this is.\n    \"\"\"\n\n    columns = [\n        \"Protein\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"MaxUptake\",\n        \"MHP\",\n        \"State\",\n        \"Exposure\",\n        \"Center\",\n        \"Center SD\",\n        \"Uptake\",\n        \"Uptake SD\",\n        \"RT\",\n        \"RT SD\",\n    ]\n\n    state_name = \"State\"\n    exposure_name = \"Exposure\"\n    aggregated = True\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        return from_dynamx_state(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_vx_state.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    return from_dynamx_state(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDExaminer_v3","title":"<code>HDExaminer_v3</code>","text":"Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class HDExaminer_v3:\n    columns = [\n        \"Protein State\",\n        \"Deut Time\",\n        \"Experiment\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"Charge\",\n        \"Search RT\",\n        \"Actual RT\",\n        \"# Spectra\",\n        \"Peak Width Da\",\n        \"m/z Shift Da\",\n        \"Max Inty\",\n        \"Exp Cent\",\n        \"Theor Cent\",\n        \"Score\",\n        \"Cent Diff\",\n        \"# Deut\",\n        \"Deut %\",\n        \"Confidence\",\n    ]\n    state_name = \"Protein State\"\n    exposure_name = \"Deut Time\"\n    aggregated = False\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        return from_hdexaminer(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDExaminer_v3.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    return from_hdexaminer(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDXFormat","title":"<code>HDXFormat</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class HDXFormat(Protocol):\n    columns: list[str]\n    state_name: str\n    exposure_name: str\n    # aggregated: bool = False  # whether the data is aggregated or expanded as multiple replicates\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        ...\n\n    @property\n    def aggregated(self) -&gt; bool: ...\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDXFormat.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.OpenHDXFormat","title":"<code>OpenHDXFormat</code>  <code>dataclass</code>","text":"<p>A format where columns names are standardized to a common set.</p> <p>Hence OpenHDXFormat.convert() is a no-op.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>@dataclass\nclass OpenHDXFormat:\n    \"\"\"A format where columns names are standardized to a common set.\n\n    Hence OpenHDXFormat.convert() is a no-op.\n\n    \"\"\"\n\n    columns = STANDARD_COLUMNS + OPTIONAL_COLUMNS\n    state_name = \"state\"\n    exposure_name = \"exposure\"\n    aggregated: bool  #  = True  # whether the data is aggregated or expanded as multiple replicates\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n\n        return df\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.OpenHDXFormat.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.identify_format","title":"<code>identify_format(df, *, exact=True)</code>","text":"<p>Identify which HDXFormat subclass the given column list matches. If there is no match, return an OpenHDXFormat instance with aggregated set to True if 'replicate' is in the columns.</p> <p>Parameters:</p> Name Type Description Default <code>exact</code> <code>bool</code> <p>If True, order must match; otherwise, uses set equality.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[HDXFormat]</code> <p>The matching HDXFormat subclass, or None if no match.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def identify_format(df: nw.DataFrame, *, exact: bool = True) -&gt; Optional[HDXFormat]:\n    \"\"\"\n    Identify which HDXFormat subclass the given column list matches. If there is no match,\n    return an OpenHDXFormat instance with aggregated set to True if 'replicate' is in the columns.\n\n    Args:\n        exact: If True, order must match; otherwise, uses set equality.\n\n    Returns:\n        The matching HDXFormat subclass, or None if no match.\n    \"\"\"\n    cols = df.columns\n    for fmt in HDX_FORMATS:\n        template = fmt.columns\n        if exact and cols == template:\n            return fmt\n        elif not exact and set(cols) == set(template):\n            return fmt\n\n    # it there is no match, we try to return the OpenHDXFormat\n    aggregated = \"replicate\" not in cols\n    fmt = OpenHDXFormat(aggregated=aggregated)\n\n    return fmt\n</code></pre>"},{"location":"reference/stable/v020/plot/","title":"plot","text":""},{"location":"reference/stable/v020/plot/#stable.v020.plot.find_wrap","title":"<code>find_wrap(peptides, margin=4, step=5, wrap_limit=200)</code>","text":"<p>Find the minimum wrap value for a given list of intervals.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>Dataframe with columns 'start' and 'end' representing intervals.</p> required <code>margin</code> <code>int</code> <p>The margin applied to the wrap value. Defaults to 4.</p> <code>4</code> <code>step</code> <code>int</code> <p>The increment step for the wrap value. Defaults to 5.</p> <code>5</code> <code>wrap_limit</code> <code>int</code> <p>The maximum allowed wrap value. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The minimum wrap value that does not overlap with any intervals.</p> Source code in <code>hdxms_datasets/stable/v020/plot.py</code> <pre><code>def find_wrap(\n    peptides: pl.DataFrame,\n    margin: int = 4,\n    step: int = 5,\n    wrap_limit: int = 200,\n) -&gt; int:\n    \"\"\"\n    Find the minimum wrap value for a given list of intervals.\n\n    Args:\n        peptides: Dataframe with columns 'start' and 'end' representing intervals.\n        margin: The margin applied to the wrap value. Defaults to 4.\n        step: The increment step for the wrap value. Defaults to 5.\n        wrap_limit: The maximum allowed wrap value. Defaults to 200.\n\n    Returns:\n        int: The minimum wrap value that does not overlap with any intervals.\n    \"\"\"\n    wrap = step\n\n    while True:\n        peptides_y = peptides.with_columns(\n            (pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"y\") % wrap)\n        )\n\n        no_overlaps = True\n        for name, df in peptides_y.group_by(\"y\", maintain_order=True):\n            overlaps = (np.array(df[\"end\"]) + 1 + margin)[:-1] &gt;= np.array(df[\"start\"])[1:]\n            if np.any(overlaps):\n                no_overlaps = False\n                break\n                # return wrap\n\n        wrap += step\n        if wrap &gt; wrap_limit:\n            return wrap_limit  # Return the maximum wrap limit if no valid wrap found\n        elif no_overlaps:\n            return wrap\n</code></pre>"},{"location":"reference/stable/v020/plot/#stable.v020.plot.unique_peptides","title":"<code>unique_peptides(df)</code>","text":"<p>Checks if all peptides in the DataFrame are unique.</p> Source code in <code>hdxms_datasets/stable/v020/plot.py</code> <pre><code>def unique_peptides(df: pl.DataFrame) -&gt; bool:\n    \"\"\"\n    Checks if all peptides in the DataFrame are unique.\n    \"\"\"\n\n    return len(df) == len(df.unique(subset=[\"start\", \"end\"]))\n</code></pre>"},{"location":"reference/stable/v020/process/","title":"process","text":""},{"location":"reference/stable/v020/process/#stable.v020.process.TemperatureDict","title":"<code>TemperatureDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for temperature dictionary.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>class TemperatureDict(TypedDict):\n    \"\"\"TypedDict for temperature dictionary.\"\"\"\n\n    value: float\n    unit: Literal[\"C\", \"K\"]\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.aggregate_columns","title":"<code>aggregate_columns(df, columns, by=['start', 'end', 'exposure'])</code>","text":"<p>Aggregate the DataFrame the specified columns by intensity-weighted average.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def aggregate_columns(\n    df: nw.DataFrame, columns: list[str], by: list[str] = [\"start\", \"end\", \"exposure\"]\n):\n    \"\"\"\n    Aggregate the DataFrame the specified columns by intensity-weighted average.\n    \"\"\"\n    groups = df.group_by(by)\n    output = {k: [] for k in by}\n    for col in columns:\n        output[col] = []\n        output[f\"{col}_sd\"] = []\n\n    for (start, end, exposure), df_group in groups:\n        output[\"start\"].append(start)\n        output[\"end\"].append(end)\n        output[\"exposure\"].append(exposure)\n\n        for col in columns:\n            val = ufloat_stats(df_group[col], df_group[\"intensity\"])\n            output[col].append(val.nominal_value)\n            output[f\"{col}_sd\"].append(val.std_dev)\n\n    agg_df = nw.from_dict(output, backend=BACKEND)\n    return agg_df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.compute_uptake_metrics","title":"<code>compute_uptake_metrics(df, exception='raise')</code>","text":"<p>Tries to add derived columns to the DataFrame. Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def compute_uptake_metrics(df: nw.DataFrame, exception=\"raise\") -&gt; nw.DataFrame:\n    \"\"\"\n    Tries to add derived columns to the DataFrame.\n    Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.\n    \"\"\"\n    all_columns = {\n        \"max_uptake\": hdx_expr.max_uptake,\n        \"uptake\": hdx_expr.uptake,\n        \"uptake_sd\": hdx_expr.uptake_sd,\n        \"fd_uptake\": hdx_expr.fd_uptake,\n        \"fd_uptake_sd\": hdx_expr.fd_uptake_sd,\n        \"frac_fd_control\": hdx_expr.frac_fd_control,\n        \"frac_fd_control_sd\": hdx_expr.frac_fd_control_sd,\n        \"frac_max_uptake\": hdx_expr.frac_max_uptake,\n        \"frac_max_uptake_sd\": hdx_expr.frac_max_uptake_sd,\n    }\n\n    for col, expr in all_columns.items():\n        if col not in df.columns:\n            try:\n                df = df.with_columns(expr)\n            except Exception as e:\n                if exception == \"raise\":\n                    raise e\n                elif exception == \"warn\":\n                    warnings.warn(f\"Failed to add column {col}: {e}\")\n                elif exception == \"ignore\":\n                    pass\n                else:\n                    raise ValueError(\"Invalid exception handling option\")\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.convert_temperature","title":"<code>convert_temperature(temperature_dict, target_unit='C')</code>","text":"<p>Convenience function to convert temperature values.</p> <p>Parameters:</p> Name Type Description Default <code>temperature_dict</code> <code>TemperatureDict</code> <p>Dictionary with temperature value(s) and unit.</p> required <code>target_unit</code> <code>str</code> <p>Target unit for temperature. Must be \"C, or \"K\"</p> <code>'C'</code> <p>Returns:</p> Type Description <code>float</code> <p>Converted temperature value(s).</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def convert_temperature(temperature_dict: TemperatureDict, target_unit: str = \"C\") -&gt; float:\n    \"\"\"\n    Convenience function to convert temperature values.\n\n    Args:\n        temperature_dict: Dictionary with temperature value(s) and unit.\n        target_unit: Target unit for temperature. Must be \"C, or \"K\"\n\n    Returns:\n        Converted temperature value(s).\n    \"\"\"\n\n    src_unit = temperature_dict[\"unit\"]\n    temp_offset = TEMPERATURE_OFFSETS[src_unit] - TEMPERATURE_OFFSETS[target_unit]\n    return temperature_dict[\"value\"] + temp_offset\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.convert_time","title":"<code>convert_time(time_dict, target_unit='s')</code>","text":"<p>Convenience function to convert time values.</p> <p>Parameters:</p> Name Type Description Default <code>time_dict</code> <code>dict</code> <p>Dictionary with time value(s) and unit.</p> required <code>target_unit</code> <code>Literal['s', 'min', 'h']</code> <p>Target unit for time.</p> <code>'s'</code> <p>Returns:</p> Type Description <code>Union[float, list[float]]</code> <p>Converted time value(s).</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def convert_time(\n    time_dict: dict, target_unit: Literal[\"s\", \"min\", \"h\"] = \"s\"\n) -&gt; Union[float, list[float]]:\n    \"\"\"\n    Convenience function to convert time values.\n\n    Args:\n        time_dict: Dictionary with time value(s) and unit.\n        target_unit: Target unit for time.\n\n    Returns:\n        Converted time value(s).\n    \"\"\"\n    raise DeprecationWarning()\n    src_unit = time_dict[\"unit\"]\n\n    time_factor = TIME_FACTORS[src_unit] / TIME_FACTORS[target_unit]\n    if values := time_dict.get(\"values\"):\n        return [v * time_factor for v in values]\n    elif value := time_dict.get(\"value\"):\n        return value * time_factor\n    else:\n        raise ValueError(\"Invalid time dictionary\")\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.drop_null_columns","title":"<code>drop_null_columns(df)</code>","text":"<p>Drop columns that are all null from the DataFrame.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def drop_null_columns(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Drop columns that are all null from the DataFrame.\"\"\"\n    all_null_columns = [col for col in df.columns if df[col].is_null().all()]\n    return df.drop(all_null_columns)\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.dynamx_cluster_to_state","title":"<code>dynamx_cluster_to_state(cluster_data, nd_exposure=0.0)</code>","text":"<p>convert dynamx cluster data to state data must contain only a single state</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def dynamx_cluster_to_state(cluster_data: nw.DataFrame, nd_exposure: float = 0.0) -&gt; nw.DataFrame:\n    \"\"\"\n    convert dynamx cluster data to state data\n    must contain only a single state\n    \"\"\"\n\n    assert len(cluster_data[\"state\"].unique()) == 1, \"Multiple states found in data\"\n\n    # determine undeuterated masses per peptide\n    nd_data = cluster_data.filter(nw.col(\"exposure\") == nd_exposure)\n    nd_peptides: list[tuple[int, int]] = sorted(\n        {(start, end) for start, end in zip(nd_data[\"start\"], nd_data[\"end\"])}\n    )\n\n    peptides_nd_mass = {}\n    for p in nd_peptides:\n        start, end = p\n        df_nd_peptide = nd_data.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end))\n\n        masses = df_nd_peptide[\"z\"] * (df_nd_peptide[\"center\"] - PROTON_MASS)\n        nd_mass = ufloat_stats(masses, df_nd_peptide[\"inten\"])\n\n        peptides_nd_mass[p] = nd_mass\n\n    groups = cluster_data.group_by([\"start\", \"end\", \"exposure\"])\n    unique_columns = [\n        \"end\",\n        \"exposure\",\n        \"fragment\",\n        \"maxuptake\",\n        \"mhp\",\n        \"modification\",\n        \"protein\",\n        \"sequence\",\n        \"start\",\n        \"state\",\n        \"stop\",\n    ]\n    records = []\n    for (start, end, exposure), df_group in groups:\n        record = {col: df_group[col][0] for col in unique_columns}\n\n        rt = ufloat_stats(df_group[\"rt\"], df_group[\"inten\"])\n        record[\"rt\"] = rt.nominal_value\n        record[\"rt_sd\"] = rt.std_dev\n\n        # state data 'center' is mass as if |charge| would be 1\n        center = ufloat_stats(\n            df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS) + PROTON_MASS, df_group[\"inten\"]\n        )\n        record[\"center\"] = center.nominal_value\n        record[\"center_sd\"] = center.std_dev\n\n        masses = df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS)\n        exp_mass = ufloat_stats(masses, df_group[\"inten\"])\n\n        if (start, end) in peptides_nd_mass:\n            uptake = exp_mass - peptides_nd_mass[(start, end)]\n            record[\"uptake\"] = uptake.nominal_value\n            record[\"uptake_sd\"] = uptake.std_dev\n        else:\n            record[\"uptake\"] = None\n            record[\"uptake_sd\"] = None\n\n        records.append(record)\n\n    d = records_to_dict(records)\n    df = nw.from_dict(d, backend=BACKEND)\n\n    if set(df.columns) == set(STATE_DATA_COLUMN_ORDER):\n        df = df[STATE_DATA_COLUMN_ORDER]\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.filter_peptides","title":"<code>filter_peptides(df, state=None, exposure=None)</code>","text":"<p>Convenience function to filter a peptides DataFrame. .</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe.</p> required <code>state</code> <code>Optional[str]</code> <p>Name of protein state to select.</p> <code>None</code> <code>exposure</code> <code>Optional[dict]</code> <p>Exposure value(s) to select. Exposure is given as a :obj:<code>dict</code>, with keys \"value\" or \"values\" for exposure value, and \"unit\" for the time unit.</p> <code>None</code> <p>Examples:</p> <p>Filter peptides for a specific protein state and exposure time:</p> <pre><code>&gt;&gt;&gt; d = {\"state\", \"SecB WT apo\", \"exposure\": {\"value\": 0.167, \"unit\": \"min\"}\n&gt;&gt;&gt; filtered_df = filter_peptides(df, **d)\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered dataframe.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def filter_peptides(\n    df: nw.DataFrame,\n    state: Optional[str] = None,\n    exposure: Optional[dict] = None,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Convenience function to filter a peptides DataFrame. .\n\n    Args:\n        df: Input dataframe.\n        state: Name of protein state to select.\n        exposure: Exposure value(s) to select. Exposure is given as a :obj:`dict`, with keys \"value\" or \"values\" for\n            exposure value, and \"unit\" for the time unit.\n\n    Examples:\n        Filter peptides for a specific protein state and exposure time:\n\n        &gt;&gt;&gt; d = {\"state\", \"SecB WT apo\", \"exposure\": {\"value\": 0.167, \"unit\": \"min\"}\n        &gt;&gt;&gt; filtered_df = filter_peptides(df, **d)\n\n    Returns:\n        Filtered dataframe.\n    \"\"\"\n    raise DeprecationWarning()\n    if state is not None:\n        df = df.filter(nw.col(\"state\") == state)\n\n    if exposure is not None:\n        # NA unit is used when exposure is given as string, in case of HD examiner this can be 'FD'\n        if exposure[\"unit\"] == \"NA\":\n            t_val = exposure[\"value\"]\n        else:\n            t_val = convert_time(exposure, \"s\")\n        if isinstance(t_val, list):\n            if all(isinstance(v, float) for v in t_val):\n                col = nw.col(\"exposure\")\n            elif all(isinstance(v, str) for v in t_val):\n                col = nw.col(\"exposure\").cast(nw.Float64)\n            else:\n                raise ValueError(\"Invalid exposure values\")\n            df = df.filter(col.is_in(t_val))\n        else:\n            df = df.filter(nw.col(\"exposure\") == t_val)\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.records_to_dict","title":"<code>records_to_dict(records)</code>","text":"<p>Convert a list of records to a dictionary of lists.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list[dict]</code> <p>List of dictionaries.</p> required <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>Dictionary with keys as column names and values as lists.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def records_to_dict(records: list[dict]) -&gt; dict[str, list]:\n    \"\"\"\n    Convert a list of records to a dictionary of lists.\n\n    Args:\n        records: List of dictionaries.\n\n    Returns:\n        Dictionary with keys as column names and values as lists.\n    \"\"\"\n\n    df_dict = defaultdict(list)\n    for record in records:\n        for key, value in record.items():\n            df_dict[key].append(value)\n\n    return dict(df_dict)\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.sort_rows","title":"<code>sort_rows(df)</code>","text":"<p>Sorts the DataFrame by state, exposure, start, end, file.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def sort_rows(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Sorts the DataFrame by state, exposure, start, end, file.\"\"\"\n    all_by = [\"state\", \"exposure\", \"start\", \"end\", \"replicate\"]\n    by = [col for col in all_by if col in df.columns]\n    return df.sort(by=by)\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.ufloat_stats","title":"<code>ufloat_stats(array, weights)</code>","text":"<p>Calculate the weighted mean and standard deviation.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def ufloat_stats(array, weights) -&gt; Variable:\n    \"\"\"Calculate the weighted mean and standard deviation.\"\"\"\n    weighted_stats = DescrStatsW(array, weights=weights, ddof=0)\n    return ufloat(weighted_stats.mean, weighted_stats.std)\n</code></pre>"},{"location":"reference/stable/v020/reader/","title":"reader","text":"<p>Reader functions for various file formats</p>"},{"location":"reference/stable/v020/reader/#stable.v020.reader.read_dynamx","title":"<code>read_dynamx(filepath_or_buffer, time_conversion=('min', 's'))</code>","text":"<p>Reads DynamX .csv files and returns the resulting peptide table as a narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>filepath_or_buffer</code> <code>Path | str | IO | bytes</code> <p>File path of the .csv file or :class:<code>~io.StringIO</code> object.</p> required <code>time_conversion</code> <code>Optional[tuple[Literal['h', 'min', 's'], Literal['h', 'min', 's']]]</code> <p>How to convert the time unit of the field 'exposure'. Format is ('', &lt;'to'&gt;). Unit options are 'h', 'min' or 's'. <code>('min', 's')</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Peptide table as a narwhals DataFrame.</p> Source code in <code>hdxms_datasets/stable/v020/reader.py</code> <pre><code>def read_dynamx(\n    filepath_or_buffer: Path | str | IO | bytes,\n    time_conversion: Optional[tuple[Literal[\"h\", \"min\", \"s\"], Literal[\"h\", \"min\", \"s\"]]] = (\n        \"min\",\n        \"s\",\n    ),\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Reads DynamX .csv files and returns the resulting peptide table as a narwhals DataFrame.\n\n    Args:\n        filepath_or_buffer: File path of the .csv file or :class:`~io.StringIO` object.\n        time_conversion: How to convert the time unit of the field 'exposure'. Format is ('&lt;from&gt;', &lt;'to'&gt;).\n            Unit options are 'h', 'min' or 's'.\n\n    Returns:\n        Peptide table as a narwhals DataFrame.\n    \"\"\"\n\n    df = read_csv(filepath_or_buffer)\n    df = df.rename({col: col.replace(\" \", \"_\").lower() for col in df.columns})\n\n    # insert 'stop' column (which is end + 1)\n    columns = df.columns\n    columns.insert(columns.index(\"end\") + 1, \"stop\")\n    df = df.with_columns((nw.col(\"end\") + 1).alias(\"stop\")).select(columns)\n\n    if time_conversion is not None:\n        time_lut = {\"h\": 3600, \"min\": 60, \"s\": 1}\n        time_factor = time_lut[time_conversion[0]] / time_lut[time_conversion[1]]\n        df = df.with_columns((nw.col(\"exposure\") * time_factor))\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/utils/","title":"utils","text":""},{"location":"reference/stable/v020/utils/#stable.v020.utils.contiguous_peptides","title":"<code>contiguous_peptides(df, start='start', end='end')</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing contiguous regions.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef contiguous_peptides(df: IntoFrame, start=\"start\", end=\"end\") -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing contiguous regions.\n    \"\"\"\n    # cast to ensure df is a narwhals DataFrame\n    df = cast(nw.DataFrame, df).select([start, end]).unique().sort(by=[start, end])\n\n    regions = []\n    current_start, current_end = None, 0\n\n    for start_val, end_val in df.select([nw.col(start), nw.col(end)]).iter_rows(named=False):\n        if current_start is None:\n            # Initialize the first region\n            current_start, current_end = start_val, end_val\n        elif start_val &lt;= current_end + 1:  # Check for contiguity\n            # Extend the current region\n            current_end = max(current_end, end_val)\n        else:\n            # Save the previous region and start a new one\n            regions.append((current_start, current_end))\n            current_start, current_end = start_val, end_val\n\n    # Don't forget to add the last region\n    if current_start is not None:\n        regions.append((current_start, current_end))\n\n    return regions\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.default_protein_info","title":"<code>default_protein_info(peptides)</code>","text":"<p>Generate minimal protein info from a set of peptides</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef default_protein_info(peptides: IntoFrame) -&gt; ProteinInfo:\n    \"\"\"Generate minimal protein info from a set of peptides\"\"\"\n\n    # Find minimum start and maximum end positions\n    min_start = peptides[\"start\"].min()  # type: ignore\n    max_end = peptides[\"end\"].max()  # type: ignore\n\n    # Estimate sequence length\n    sequence_length = max_end - min_start + 1\n\n    placeholder_sequence = \"X\" * sequence_length\n    sequence = reconstruct_sequence(peptides, placeholder_sequence, n_term=min_start)\n\n    # Create a minimal ProteinInfo\n    return {\n        \"sequence\": sequence,  # sequence with \"X\" gaps\n        \"n_term\": int(min_start),\n        \"c_term\": int(max_end),\n    }\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.non_overlapping_peptides","title":"<code>non_overlapping_peptides(df, start='start', end='end')</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing non-overlapping peptides.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef non_overlapping_peptides(\n    df: IntoFrame,\n    start: str = \"start\",\n    end: str = \"end\",\n) -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing non-overlapping peptides.\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([start, end]).unique().sort(by=[start, end])\n\n    regions = df.rows()\n    out = [regions[0]]\n    for start_val, end_val in regions[1:]:\n        if start_val &gt; out[-1][1]:\n            out.append((start_val, end_val))\n        else:\n            continue\n\n    return out\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.peptide_redundancy","title":"<code>peptide_redundancy(df, start='start', end='end')</code>","text":"<p>Compute the redundancy of peptides in a DataFrame based on their start and end positions. Redundancy is defined as the number of peptides overlapping at each position.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrame</code> <p>DataFrame containing peptide information with 'start' and 'end' columns.</p> required <code>start</code> <code>str</code> <p>Column name for the start position.</p> <code>'start'</code> <code>end</code> <code>str</code> <p>Column name for the end position.</p> <code>'end'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tuple containing:</p> <code>ndarray</code> <ul> <li>r_number: An array of positions from the minimum start to the maximum end.</li> </ul> <code>tuple[ndarray, ndarray]</code> <ul> <li>redundancy: An array of redundancy counts for each position in r_number.</li> </ul> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef peptide_redundancy(\n    df: IntoFrame, start: str = \"start\", end: str = \"end\"\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the redundancy of peptides in a DataFrame based on their start and end positions.\n    Redundancy is defined as the number of peptides overlapping at each position.\n\n    Args:\n        df: DataFrame containing peptide information with 'start' and 'end' columns.\n        start: Column name for the start position.\n        end: Column name for the end position.\n\n    Returns:\n        A tuple containing:\n        - r_number: An array of positions from the minimum start to the maximum end.\n        - redundancy: An array of redundancy counts for each position in r_number.\n\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([start, end]).unique().sort(by=[start, end])\n    vmin, vmax = df[start][0], df[end][-1]\n\n    r_number = np.arange(vmin, vmax + 1, dtype=int)\n    redundancy = np.zeros_like(r_number, dtype=int)\n    for s, e in df.rows():\n        i0, i1 = np.searchsorted(r_number, (s, e))\n        redundancy[i0:i1] += 1\n\n    return r_number, redundancy\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.reconstruct_sequence","title":"<code>reconstruct_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Reconstruct the sequence form a dataframe of peptides with sequence information. The sequence is reconstructed by replacing the known sequence with the peptide sequences at the specified start and end positions.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>Starting sequence. Can be a string 'X' as placeholder.</p> required <code>n_term</code> <code>int</code> <p>The residue number of the N-terminal residue. This is typically 1, can be negative in case of purification tags.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>The reconstructed sequence.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef reconstruct_sequence(peptides: nw.DataFrame, known_sequence: str, n_term: int = 1) -&gt; str:\n    \"\"\"\n    Reconstruct the sequence form a dataframe of peptides with sequence information.\n    The sequence is reconstructed by replacing the known sequence with the peptide\n    sequences at the specified start and end positions.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: Starting sequence. Can be a string 'X' as placeholder.\n        n_term: The residue number of the N-terminal residue. This is typically 1, can be\n            negative in case of purification tags.\n\n    Returns:\n        The reconstructed sequence.\n    \"\"\"\n\n    reconstructed = list(known_sequence)\n    for start, end, sequence in peptides.select([\"start\", \"end\", \"sequence\"]).iter_rows():  # type: ignore\n        start_idx = start - n_term\n        assert end - start + 1 == len(sequence), (\n            f\"Length mismatch at {start}:{end} with sequence {sequence}\"\n        )\n\n        for i, aa in enumerate(sequence, start=start_idx):\n            reconstructed[i] = aa\n\n    return \"\".join(reconstructed)\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.verify_sequence","title":"<code>verify_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Verify the sequence of peptides against the given sequence.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>IntoFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>The original sequence to check against.</p> required <code>n_term</code> <code>int</code> <p>The number of N-terminal residues to consider.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[tuple[int, str, str]]</code> <p>A tuple containing the fixed sequence and a list of mismatches.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef verify_sequence(\n    peptides: IntoFrame, known_sequence: str, n_term: int = 1\n) -&gt; list[tuple[int, str, str]]:\n    \"\"\"\n    Verify the sequence of peptides against the given sequence.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: The original sequence to check against.\n        n_term: The number of N-terminal residues to consider.\n\n    Returns:\n        A tuple containing the fixed sequence and a list of mismatches.\n    \"\"\"\n\n    reconstructed_sequence = reconstruct_sequence(peptides, known_sequence, n_term)\n\n    mismatches = []\n    for r_number, (expected, found) in enumerate(\n        zip(known_sequence, reconstructed_sequence), start=n_term\n    ):\n        if expected != found:\n            mismatches.append((r_number, expected, found))\n\n    return mismatches\n</code></pre>"},{"location":"reference/web/components/","title":"components","text":""},{"location":"reference/web/components/#web.components.InputNumeric","title":"<code>InputNumeric(label, value=0, on_value=None, disabled=False, optional=False, continuous_update=False, clearable=False, classes=[], style=None, autofocus=False)</code>","text":"<p>Numeric input (float | integers).</p> <p>Basic example:</p> <pre><code>import solara\n\nint_value = solara.reactive(42)\ncontinuous_update = solara.reactive(True)\n\n@solara.component\ndef Page():\n    solara.Checkbox(label=\"Continuous update\", value=continuous_update)\n    solara.InputInt(\"Enter an integer number\", value=int_value, continuous_update=continuous_update.value)\n    with solara.Row():\n        solara.Button(\"Clear\", on_click=lambda: int_value.set(42))\n    solara.Markdown(f\"**You entered**: {int_value.value}\")\n</code></pre>"},{"location":"reference/web/components/#web.components.InputNumeric--arguments","title":"Arguments","text":"<ul> <li><code>label</code>: Label to display next to the slider.</li> <li><code>value</code>: The currently entered value.</li> <li><code>on_value</code>: Callback to call when the value changes.</li> <li><code>disabled</code>: Whether the input is disabled.</li> <li><code>optional</code>: Whether the value can be None.</li> <li><code>continuous_update</code>: Whether to call the <code>on_value</code> callback on every change or only when the input loses focus or the enter key is pressed.</li> <li><code>clearable</code>: Whether the input can be cleared.</li> <li><code>classes</code>: List of CSS classes to apply to the input.</li> <li><code>style</code>: CSS style to apply to the input.</li> <li><code>autofocus</code>: Determines if a component is to be autofocused or not (Default is False). Autofocus will occur either during page load, or when the component becomes visible (for example, dialog being opened). Only one component per page should have autofocus on each such event.</li> </ul> Source code in <code>hdxms_datasets/web/components.py</code> <pre><code>@solara.component\ndef InputNumeric(\n    label: str,\n    value: Union[None, num, solara.Reactive[num], solara.Reactive[Optional[num]]] = 0,\n    on_value: Union[None, Callable[[Optional[num]], None], Callable[[num], None]] = None,\n    disabled: bool = False,\n    optional: bool = False,\n    continuous_update: bool = False,\n    clearable: bool = False,\n    classes: list[str] = [],\n    style: Optional[Union[str, dict[str, str]]] = None,\n    autofocus: bool = False,\n):\n    \"\"\"Numeric input (float | integers).\n\n    Basic example:\n\n    ```solara\n    import solara\n\n    int_value = solara.reactive(42)\n    continuous_update = solara.reactive(True)\n\n    @solara.component\n    def Page():\n        solara.Checkbox(label=\"Continuous update\", value=continuous_update)\n        solara.InputInt(\"Enter an integer number\", value=int_value, continuous_update=continuous_update.value)\n        with solara.Row():\n            solara.Button(\"Clear\", on_click=lambda: int_value.set(42))\n        solara.Markdown(f\"**You entered**: {int_value.value}\")\n    ```\n\n    ## Arguments\n\n    * `label`: Label to display next to the slider.\n    * `value`: The currently entered value.\n    * `on_value`: Callback to call when the value changes.\n    * `disabled`: Whether the input is disabled.\n    * `optional`: Whether the value can be None.\n    * `continuous_update`: Whether to call the `on_value` callback on every change or only when the input loses focus or the enter key is pressed.\n    * `clearable`: Whether the input can be cleared.\n    * `classes`: List of CSS classes to apply to the input.\n    * `style`: CSS style to apply to the input.\n    * `autofocus`: Determines if a component is to be autofocused or not (Default is False). Autofocus will occur either during page load, or when the component becomes visible (for example, dialog being opened). Only one component per page should have autofocus on each such event.\n    \"\"\"\n\n    def str_to_num(value: Optional[str]):\n        if value:\n            try:\n                return int(value)\n            except ValueError:\n                try:\n                    return float(value)\n                except ValueError:\n                    raise ValueError(f\"Cannot convert {value} to a number\")\n        else:\n            if optional:\n                return None\n            else:\n                raise ValueError(\"Value cannot be empty\")\n\n    return _InputNumeric(\n        str_to_num,\n        label=label,\n        value=value,\n        on_value=on_value,\n        disabled=disabled,\n        continuous_update=continuous_update,\n        clearable=clearable,\n        classes=classes,\n        style=style,\n        autofocus=autofocus,\n    )\n</code></pre>"},{"location":"reference/web/models/","title":"models","text":""},{"location":"reference/web/state/","title":"state","text":""},{"location":"reference/web/state/#web.state.ListStore","title":"<code>ListStore</code>","text":"<p>               Bases: <code>Store[list[T]]</code></p> <p>baseclass for reactive list</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>class ListStore(Store[list[T]]):\n    \"\"\"baseclass for reactive list\"\"\"\n\n    def __init__(self, items: Optional[list[T]] = None):\n        super().__init__(items if items is not None else [])\n\n    def __len__(self):\n        return len(self.value)\n\n    def __getitem__(self, idx: int) -&gt; T:\n        return self.value[idx]\n\n    def __iter__(self):\n        return iter(self.value)\n\n    def get_item(self, idx: int, default: R = NO_DEFAULT) -&gt; T | R:\n        try:\n            return self._reactive.value[idx]\n        except IndexError:\n            if default is NO_DEFAULT:\n                raise IndexError(f\"Index {idx} is out of range\")\n            return default\n\n    def set(self, items: list[T]) -&gt; None:\n        self._reactive.value = items\n\n    def set_item(self, idx: int, item: T) -&gt; None:\n        new_items = self._reactive.value.copy()\n        if idx == len(new_items):\n            new_items.append(item)\n        elif idx &lt; len(new_items):\n            new_items[idx] = item\n        else:\n            raise IndexError(f\"Index {idx} is out of range\")\n        self._reactive.value = new_items\n\n    def append(self, item: T) -&gt; None:\n        self._reactive.value = [*self._reactive.value, item]\n\n    def extend(self, items: list[T]) -&gt; None:\n        new_value = self.value.copy()\n        new_value.extend(items)\n        self._reactive.value = new_value\n\n    def insert(self, idx: int, item: T) -&gt; None:\n        new_value = self.value.copy()\n        new_value.insert(idx, item)\n        self._reactive.value = new_value\n\n    def remove(self, item: T) -&gt; None:\n        self._reactive.value = [it for it in self.value if it != item]\n\n    def pop(self, idx: int) -&gt; T:\n        item = self.value[idx]\n        self._reactive.value = self.value[:idx] + self.value[idx + 1 :]\n        return item\n\n    def clear(self) -&gt; None:\n        self._reactive.value = []\n\n    def index(self, item: T) -&gt; int:\n        return self.value.index(item)\n\n    def update(self, idx: int, **kwargs):\n        new_value = self.value.copy()\n        updated_item = merge_state(new_value[idx], **kwargs)\n        new_value[idx] = updated_item\n        self._reactive.value = new_value\n\n    def count(self, item: T) -&gt; int:\n        return self.value.count(item)\n\n    def find_item(self, **kwargs) -&gt; Optional[T]:\n        \"\"\"find item in list by attributes\"\"\"\n        for item in self.value:\n            if all(getattr(item, k) == v for k, v in kwargs.items()):\n                return item\n        return None\n\n    def find_index(self, **kwargs) -&gt; Optional[int]:\n        \"\"\"find item in list by attributes\"\"\"\n        for idx, item in enumerate(self.value):\n            if all(getattr(item, k) == v for k, v in kwargs.items()):\n                return idx\n        return None\n</code></pre>"},{"location":"reference/web/state/#web.state.ListStore.find_index","title":"<code>find_index(**kwargs)</code>","text":"<p>find item in list by attributes</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>def find_index(self, **kwargs) -&gt; Optional[int]:\n    \"\"\"find item in list by attributes\"\"\"\n    for idx, item in enumerate(self.value):\n        if all(getattr(item, k) == v for k, v in kwargs.items()):\n            return idx\n    return None\n</code></pre>"},{"location":"reference/web/state/#web.state.ListStore.find_item","title":"<code>find_item(**kwargs)</code>","text":"<p>find item in list by attributes</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>def find_item(self, **kwargs) -&gt; Optional[T]:\n    \"\"\"find item in list by attributes\"\"\"\n    for item in self.value:\n        if all(getattr(item, k) == v for k, v in kwargs.items()):\n            return item\n    return None\n</code></pre>"},{"location":"reference/web/state/#web.state.PeptideStore","title":"<code>PeptideStore</code>","text":"<p>               Bases: <code>DictStore[str, ListStore[PeptideInfo]]</code></p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>class PeptideStore(DictStore[str, ListStore[PeptideInfo]]):\n    def add_peptide(self, state: str | None, peptide_type: PeptideType):  # TODO type ppetide_type\n        new_peptide = PeptideInfo(type=peptide_type)\n        self[state].append(new_peptide)\n\n    def update_peptide(self, state: str, peptide_idx: int, peptide: PeptideInfo):\n        new_peptides = self[state]\n        new_peptides.set_item(peptide_idx, peptide)\n        self.set_item(state, new_peptides)\n\n    def remove_peptide(self, state: str, peptide_idx: int):\n        new_peptides = self.value[state]\n        new_peptides.pop(peptide_idx)\n        self.set_item(state, new_peptides)\n\n    def add_state(self, state: str):\n        # append the new state to the dict\n        new_value = self.value | {state: ListStore[PeptideInfo]([])}\n        self.set(new_value)\n\n    def remove_state(self, state: str):\n        self.pop(state)\n\n    def validate(self) -&gt; dict[str, tuple[bool, str, str]]:\n        \"\"\"validate all peptides in the store\"\"\"\n        errors = {}\n        for state, peptides in self.items():\n            for idx, peptide in enumerate(peptides):\n                is_valid, error = peptide.validate()\n                if not is_valid:\n                    errors[state] = (idx, peptide.type, error)\n        return errors\n</code></pre>"},{"location":"reference/web/state/#web.state.PeptideStore.validate","title":"<code>validate()</code>","text":"<p>validate all peptides in the store</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>def validate(self) -&gt; dict[str, tuple[bool, str, str]]:\n    \"\"\"validate all peptides in the store\"\"\"\n    errors = {}\n    for state, peptides in self.items():\n        for idx, peptide in enumerate(peptides):\n            is_valid, error = peptide.validate()\n            if not is_valid:\n                errors[state] = (idx, peptide.type, error)\n    return errors\n</code></pre>"},{"location":"reference/web/upload_form/","title":"upload_form","text":""},{"location":"usage/loading/","title":"Loading Datasets","text":"<p>The <code>hdxms_datasets</code> package features a central <code>DataVault</code> object, which is used to fetch datasets from an online  database to a local cache dir, as well as parse those locally saved peptide sets into a narwhals <code>DataFrame</code>.</p>"},{"location":"usage/loading/#basic-usage","title":"Basic usage","text":"<pre><code># Creating a RemoveDataVault, specifying a chache dir, using the default remote database\nvault = RemoteDataVault(\n    cache_dir=\".cache\",\n)\nvault.get_index().to_native()\n\n#%%\n# Fetch a dataset by ID\nvault.fetch_dataset(\"1704204434_SecB_Krishnamurthy\")\n\n# Load the dataset\nds = vault.load_dataset(\"1704204434_SecB_Krishnamurthy\")\n\n# Print a string describing the states in the dataset\nprint(ds.describe())\n\n# Load ND control peptides as a narwhals DataFrame\nnd_control = ds.get_peptides(0, \"non_deuterated\").load()\n\n# # Load FD control peptides as a narwhals DataFrame\nfd_control = ds.get_peptides(0, \"fully_deuterated\").load()\n\n# Load experimental peptides as narwhals dataframe\npd_peptides = ds.get_peptides(0, \"partially_deuterated\").load()\npd_peptides\n# %%\n# Merge peptides, matching each partially dueterated peptide timepoint with nd/fd control uptake or mass\nmerged = merge_peptides(pd_peptides, nd_peptides=nd_control, fd_peptides=fd_control)\n\n# %%\n\n# compute d-uptake, max uptake, full deuteration uptake, RFU\nprocessed = compute_uptake_metrics(merged)\nprocessed.to_native()\n</code></pre> <p>The code above creates a <code>RemoteDataVault</code>, thereby creating a cache directory. Then the dataset <code>\"1704204434_SecB_Krishnamurthy\"</code> is fetched  from the database and stored in the cache dir.</p> <p>From here, HDX-MS data can be loaded and processed. </p>"}]}