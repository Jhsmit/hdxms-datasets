{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"HDXMS Datasets","text":"<p>Welcome to the HDXMS datasets repository. </p> <p>The <code>hdxms-datasets</code> package provides tools handling HDX-MS datasets.</p> <p>The package offers the following features:</p> <ul> <li>Defining datasets and their experimental metadata</li> <li>Verification of datasets and metadata</li> <li>Loading datasets from local or remote database</li> <li>Conversion of datasets from various formats (e.g., DynamX, HDExaminer) to a standardized format</li> <li>Propagation of standard deviations from replicates to fractional relative uptake values</li> </ul> <p>A database for open HDX datasets is set up at HDXMS DataBase</p>"},{"location":"#example-usage","title":"Example Usage","text":""},{"location":"#loading-datasets","title":"Loading datasets","text":"<pre><code>from hdxms_datasets import DataBase\n\ndb = DataBase('path/to/local_db')\ndataset = db.get_dataset('HDX_D9096080')\n\n# Protein identifier information\nprint(dataset.protein_identifiers.uniprot_entry_name)\n#&gt; 'SECB_ECOLI'\n\n# Access HDX states \nprint([state.name for state in dataset.states])\n#&gt; ['Tetramer', 'Dimer']\n\n# Get the sequence of the first state\nstate = dataset.states[0]\nprint(state.protein_state.sequence)\n#&gt; 'MSEQNNTEMTFQIQRIYT...'\n\n# Load peptides\npeptides = state.peptides[0]\n\n# Access peptide information\nprint(peptides.deuteration_type, peptides.pH, peptides.temperature)\n#&gt; DeuterationType.partially_deuterated 8.0 303.15\n\n# Load the peptide table as standardized narwhals DataFrame\ndf = peptides.load(\n    convert=True,  # convert column header names to open hdx stanard\n    aggregate=True, # aggregate centroids / uptake values across replicates\n)\n\nprint(df.columns)\n#&gt; ['start', 'end', 'sequence', 'state', 'exposure', 'centroid_mz', 'rt', 'rt_sd', 'uptake', ... \n</code></pre>"},{"location":"#define-and-process-datasets","title":"Define and process datasets","text":"<pre><code>from hdxms_datasets import ProteinState, Peptides, verify_sequence, merge_peptides, compute_uptake_metrics\n\n# Define the protein state\nprotein_state = ProteinState(\n    sequence=\"MSEQNNTEMTFQIQRIYTKDISFEAPNAPHVFQKDWQPEVKLDLDTASSQLADDVYEVVLRVTVTASLGEETAFLCEVQQGGIFSIAGIEGTQMAHCLGAYCPNILFPYARECITSMVSRGTFPQLNLAPVNFDALFMNYLQQQAGEGTEEHQDA\",\n    n_term=1,\n    c_term=155,\n    oligomeric_state=4,\n)\n\n# Define the partially deuterated peptides for the SecB state\npd_peptides = Peptides(\n    # path to the data file\n    data_file=data_dir / \"ecSecB_apo.csv\",\n    # specify the data format\n    data_format=PeptideFormat.DynamX_v3_state,\n    # specify the deuteration type (partially, fully or not deuterated)\n    deuteration_type=DeuterationType.partially_deuterated,\n    filters={\n        \"State\": \"SecB WT apo\",\n        # Optionally filter by exposure, leave out to include all exposures\n        \"Exposure\": [0.167, 0.5, 1.0, 10.0, 100.000008],\n    },\n    # pH read without corrections\n    pH=8.0,\n    # temperature of the exchange buffer\n    temperature=303.15,\n    # deuterium percentage of the exchange buffer\n    d_percentage=90.0,\n)\n\n# check for difference between the protein state sequence and the peptide sequences\nmismatches = verify_sequence(pd_peptides.load(), protein_state.sequence, n_term=protein_state.n_term)\nprint(mismatches)\n#&gt; [] # sequences match\n\n# Define the fully deuterated peptides for the SecB state\nfd_peptides = Peptides(\n    data_file=data_dir / \"ecSecB_apo.csv\",\n    data_format=PeptideFormat.DynamX_v3_state,\n    deuteration_type=DeuterationType.fully_deuterated,\n    filters={\n        \"State\": \"Full deuteration control\",\n        \"Exposure\": 0.167,\n    },\n)\n\n# merge both peptides together in a single dataframe\nmerged = merge_peptides([pd_peptides, fd_peptides])\nprint(merged.columns)\n#&gt; ['start', 'end', 'sequence', ... 'uptake', 'uptake_sd', 'fd_uptake', 'fd_uptake_sd']\n\n# compute uptake metrics for the merged peptides\n# this function computes uptake from centroid mass if not present\n# as well as fractional uptake\nprocessed = compute_uptake_metrics(merged)\nprint(processed.columns)\n#&gt; ['start', 'end', 'sequence', ... 'uptake', 'uptake_sd', 'fd_uptake', 'fd_uptake_sd', 'fractional_uptake', 'fractional_uptake_sd']\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ pip install hdxms-datasets\n</code></pre>"},{"location":"cli/","title":"Command Line Interface (CLI)","text":"<p>The <code>hdxms-datasets</code> package provides a command-line interface to help you create and manage HDX-MS datasets.</p>"},{"location":"cli/#installation","title":"Installation","text":"<p>First, install the package with the CLI dependencies:</p> <pre><code>pip install -e .\n</code></pre> <p>After installation, the <code>hdxms-datasets</code> command will be available in your terminal.</p>"},{"location":"cli/#commands","title":"Commands","text":""},{"location":"cli/#hdxms-datasets-create","title":"<code>hdxms-datasets create</code>","text":"<p>Create a new HDX-MS dataset with a unique ID and template script.</p> <p>Basic usage:</p> <pre><code>hdxms-datasets create\n</code></pre> <p>This will: 1. Generate a unique HDX dataset ID (e.g., <code>HDX_A1B2C3D4</code>) 2. Create a new directory in the current directory: <code>&lt;HDX_ID&gt;/</code> 3. Generate a template <code>create_dataset.py</code> script with configuration 4. Create a <code>data/</code> subdirectory for your raw data files 5. Generate a <code>README.md</code> with quick start instructions</p> <p>Options:</p> <ul> <li><code>--num-states, -n INTEGER</code>: Number of protein states (default: 1)</li> <li><code>--format, -f CHOICE</code>: Data format - OpenHDX, DynamX_v3_state, DynamX_v3_cluster, HDExaminer (default: OpenHDX)</li> <li><code>--ph FLOAT</code>: Experimental pH (default: 7.5)</li> <li><code>--temperature, -t FLOAT</code>: Temperature in Kelvin (default: 293.15)</li> <li><code>--database-dir, -d PATH</code>: Path to existing database directory to check for ID conflicts</li> <li><code>--help</code>: Show help message</li> </ul> <p>Examples:</p> <pre><code># Create with defaults (OpenHDX, 1 state, pH 7.5, 20\u00b0C)\nhdxms-datasets create\n\n# Create with custom parameters\nhdxms-datasets create --num-states 2 --format DynamX_v3_state --ph 8.0 --temperature 298.15\n\n# Using short flags\nhdxms-datasets create -n 3 -f HDExaminer --ph 7.0 -t 293.15\n\n# Check for ID conflicts with existing database\nhdxms-datasets create --database-dir ~/hdx-database/datasets\n</code></pre>"},{"location":"cli/#configuration-via-arguments","title":"Configuration via Arguments","text":"<p>All dataset configuration is specified via command-line arguments:</p> <ul> <li>Number of states (<code>--num-states</code>): How many different protein states you measured (default: 1)</li> <li>Data format (<code>--format</code>): Which software generated your data (default: OpenHDX)</li> <li><code>OpenHDX</code> - OpenHDX format</li> <li><code>DynamX_v3_state</code> - DynamX state files</li> <li><code>DynamX_v3_cluster</code> - DynamX cluster files  </li> <li><code>HDExaminer</code> - HDExaminer files</li> <li>pH (<code>--ph</code>): Experimental pH value (default: 7.5)</li> <li>Temperature (<code>--temperature</code>): Temperature in Kelvin (default: 293.15 K = 20\u00b0C)</li> </ul>"},{"location":"cli/#workflow-example","title":"Workflow Example","text":"<pre><code># Step 1: Create a new dataset with custom parameters\n$ hdxms-datasets create --num-states 2 --format DynamX_v3_state --ph 8.0\n\n\u2713 Generated new dataset ID: HDX_A1B2C3D4\n============================================================\n\u2713 Dataset template created successfully!\n============================================================\n\nDataset ID:     HDX_A1B2C3D4\nLocation:       C:\\Users\\username\\HDX_A1B2C3D4\nFormat:         DynamX_v3_state\nStates:         2\npH:             8.0\nTemperature:    293.15 K (20.0\u00b0C)\n\nNext steps:\n  1. cd HDX_A1B2C3D4\n  2. Place your data files in the data/ directory\n  3. Edit create_dataset.py with your specific information\n  4. python create_dataset.py\n\n# Step 2: Navigate to the new directory\n$ cd HDX_A1B2C3D4\n\n# Step 3: Copy your data files\n$ copy C:\\path\\to\\my\\data.csv data\\\n\n# Step 4: Edit the template script\n$ notepad create_dataset.py\n# Edit the file with your specific information:\n#   - Replace protein sequences\n#   - Update data file names\n#   - Add author information\n#   - Add publication details\n\n# Step 5: Run the script to create your dataset\n$ python create_dataset.py\n\u2713 Dataset submitted successfully with ID: HDX_A1B2C3D4\n  Dataset location: C:\\Users\\username\\HDX_A1B2C3D4\\dataset\\HDX_A1B2C3D4\n</code></pre>"},{"location":"cli/#generated-template-structure","title":"Generated Template Structure","text":"<p>After running <code>hdxms-datasets create</code>, you'll have:</p> <pre><code>HDX_A1B2C3D4/\n\u251c\u2500\u2500 create_dataset.py    # Template script to edit\n\u251c\u2500\u2500 README.md            # Quick start guide\n\u2514\u2500\u2500 data/                # Directory for your raw data files\n</code></pre> <p>The <code>create_dataset.py</code> template includes: - Clearly marked sections to edit - Inline comments explaining each field - List-based structure for protein states and peptides (flexible and easy to extend) - Pre-configured pH and temperature values from your command-line arguments - Example values to guide you - Automatic sequence verification - Dataset submission code</p> <p>Please note that this template is not exhaustive and other metadata fields may be used  depending on your dataset's requirements. </p>"},{"location":"cli/#future-commands-planned","title":"Future Commands (Planned)","text":"<p>The CLI is designed to be extensible. Future commands may include:</p> <ul> <li><code>hdxms-datasets validate</code>: Validate a dataset before submission</li> <li><code>hdxms-datasets upload</code>: Upload a dataset to a remote database</li> <li><code>hdxms-datasets export</code>: Export a dataset to different formats</li> </ul>"},{"location":"cli/#getting-help","title":"Getting Help","text":"<p>For more information about any command:</p> <pre><code>hdxms-datasets --help\nhdxms-datasets create --help\n</code></pre>"},{"location":"fields/","title":"Fields","text":"<p>This document describes the fields used in open-hdxms files. The fields are divided into required, optional, and calculated fields.</p> <p>Some fields can be both calculated from raw data (ie uptake) or provided directly</p>"},{"location":"fields/#start-int","title":"start (int)","text":"<p>residue number of the first amino acid in the peptide</p>"},{"location":"fields/#end-int","title":"end (int)","text":"<p>residue number of the last amino acid in the peptide</p>"},{"location":"fields/#sequence-str","title":"sequence (str)","text":"<p>fasta sequence of the peptide</p>"},{"location":"fields/#protein-str","title":"protein (str)","text":"<p>protein name or identifier</p> <p>HDExaminer name: Protein DynamX name: Protein</p>"},{"location":"fields/#state-str","title":"state (str)","text":"<p>state label</p> <p>DynamX state/cluster name: State HDExaminer name: Protein State hxms name: PROTEIN_STATE</p>"},{"location":"fields/#replicate-str","title":"replicate (str)","text":"<p>Label for the replicate DynamX cluster name: File HDExaminer name: Experiment</p>"},{"location":"fields/#exposure-float","title":"exposure (float)","text":"<p>Deuterium exposure time in seconds</p> <p>DynamX state/cluster name: Exposure HDExaminer name: Deut Time</p>"},{"location":"fields/#centroid_mass-str","title":"centroid_mass (str)","text":"<p>calculated mass of uncharged peptide derived from charge / centroid</p>"},{"location":"fields/#centroid_mass_sd-str","title":"centroid_mass_sd (str)","text":"<p>Standard deviation of the centroid mass value</p>"},{"location":"fields/#centroid_mz","title":"centroid_mz","text":"<p>HDExaminer name: Exp Cent DynamX name: ??</p>"},{"location":"fields/#centroid_mz_sd","title":"centroid_mz_sd","text":"<p>Standard deviation of the centroid m/z value</p>"},{"location":"fields/#rt","title":"rt","text":"<p>retention time  units unknown (minutes?)</p> <p>DynamX state/cluster name: RT HDExaminer name: Actual RT</p>"},{"location":"fields/#rt_sd-float","title":"rt_sd (float)","text":"<p>Standard deviation of the retention time value</p>"},{"location":"fields/#charge-int","title":"charge (int)","text":"<p>DynamX cluster name: z HDExaminer name: Charge</p>"},{"location":"fields/#intensity-float","title":"intensity (float)","text":"<p>HDExaminer name: Max Inty DynamX name?? is this max or mean intensity?</p>"},{"location":"fields/#optional-fields","title":"Optional fields:","text":"<p>These fields can be present in open-hdxms files, but can also be calculated from the other fields.</p>"},{"location":"fields/#max_uptake-int","title":"max_uptake (int)","text":"<p>Theoretical maximum deuterium uptake for the peptide. Equal to the number of  non proline residues. Not that back-exchange is not considered here, including back exchange of the N-terminal amide. </p>"},{"location":"fields/#uptake-float","title":"uptake (float)","text":"<p>Number of deuterium atoms incorporated into the peptide calculated from centroid mass, if available</p>"},{"location":"fields/#uptake_sd-float","title":"uptake_sd (float)","text":"<p>Standard deviation of the uptake value</p>"},{"location":"fields/#calculated-fields","title":"Calculated fields:","text":"<p>These fields are derived from other fields defined in the above sections.</p>"},{"location":"fields/#n_replicates","title":"n_replicates","text":"<p>added after data aggregation Total number of replicates that were aggregated together</p>"},{"location":"fields/#n_charges","title":"n_charges","text":"<p>Total number of different charged states that were aggregated together</p>"},{"location":"fields/#n_clusters","title":"n_clusters","text":"<p>added after data aggregation Total number of isotopic clusters that were aggregated together. When replicates include multiple isotopic clusters (different charged states), this value will be larger than n_replicates.</p>"},{"location":"fields/#frac_fd_control-float","title":"frac_fd_control (float)","text":"<p>Fractional deuterium uptake with respect to fully deuterated control sample</p>"},{"location":"fields/#frac_fd_control_sd-float","title":"frac_fd_control_sd (float)","text":"<p>Standard deviation of the fractional deuterium uptake with respect to fully deuterated control sample</p>"},{"location":"fields/#frac_max_uptake-float","title":"frac_max_uptake (float)","text":"<p>Fractional deuterium uptake with respect to the maximum possible uptake for the peptide</p>"},{"location":"fields/#frac_max_uptake_sd-float","title":"frac_max_uptake_sd (float)","text":"<p>Standard deviation of the fractional deuterium uptake with respect to the maximum possible uptake for the peptide</p>"},{"location":"hd_examiner_formats/","title":"HD examiner export format","text":"<p>There are multiple different file format formats of HD Examiner exported HDX-MS data published. This page provides an overview of the different formats in an attempt to assign HD examiner version numbers and export name / functionality to the different formats.</p> <p>See docs/hd_examiner_files for a collection of example files. </p> <p>It appears that there are three different export formats from HD examiner; 'all results', 'peptide pool results' and 'uptake summary table'. The latter two contain the same data but have different layouts.</p>"},{"location":"hd_examiner_formats/#list-of-found-hd-examiner-exported-data","title":"List of found HD examiner exported data","text":"<p>Below is a list of found HD examiner exported data files, with their source, columns and comments.</p>"},{"location":"hd_examiner_formats/#hdgraphix-pool-output","title":"HDgraphiX pool output","text":"<p>File: 'HDgraphiX Sample HDExaminer Pool.csv' Source: HDGraphix.</p> <p>Columns: The first line is a header with exposure times. </p> <p>The second line has the column names, starting with: 'State,Protein,Start,End,Sequence,Search RT,Charge,Max D,'</p> <p>Followed by repeating blocks of: 'Start RT,End RT,#D,%D,#D right,%D right,Score,Conf,'</p> <p>FD control is expected to be labelled as 'Full-D'</p> <p>Comments: Data itself has alternating empty lines?</p> <p>Reading: GitHub</p>"},{"location":"hd_examiner_formats/#hdgraphix-summary-output","title":"HDgraphiX summary output","text":"<p>File: 'HDgraphiX Sample HDExaminer Summary.csv' Source: HDGraphix.</p> <p>Columns: Protein State,Protein,Start,End,Sequence,Peptide Mass,RT (min),Deut Time (sec),maxD,Theor Uptake,#D,%D,Conf Interval (#D),#Rep,Confidence,Stddev,p</p> <p>Format: HD examiner summary FD control: 'Full-D'</p> <p>Comments: Data itself has alternating empty lines?</p>"},{"location":"hd_examiner_formats/#pflink-example-file-pool-peptide-results","title":"PFLink example file 'pool peptide results'","text":"<p>File: ecDHFR_tutorial.csv Source: https://huggingface.co/spaces/glasgow-lab/PFLink</p> <p>Columns:  Same as HDgrapix pool output</p> <p>Format: HD examiner peptide pool / HD examiner pool FD control: 'Full-D'</p> <p>Comments: This is similar to HDgraphix pool output, no alternating empty lines.</p> <p>Reading: Huggingface code</p>"},{"location":"hd_examiner_formats/#hd-examiner-v34-export","title":"HD Examiner v3.4 export","text":"<p>File: &lt; Not public &gt; Source: Data provided by Vladimir Sarpe from Trajan (not public)</p> <p>Columns: 'Protein State,Deut Time,Experiment,Start,End,Sequence,Charge,Search RT,Actual RT,# Spectra,Peak Width Da,m/z Shift Da,Max Inty,Exp Cent,Theor Cent,Score,Cent Diff,# Deut,Deut %,Confidence'</p> <p>'Deut Time' columns are in seconds, stored as strings formatted as '0s'. Format: HD examiner V3.4 (as currently used in <code>hdxms-datasets</code>)</p> <p>FD control: 'FD'</p> <p>Comments: Unknown name of export function used in HD Examiner v3.4, suspected it is an export of the 'Results Table'. Differences with tuttlelm 'all_results' and bryan et al (this, other):</p> <ul> <li>(Peak Width Da, Peak Width)</li> <li>(m/z Shift Da, m/z Shift)</li> </ul>"},{"location":"hd_examiner_formats/#tang-et-al-hd-examiner-export","title":"Tang et al HD examiner export","text":"<p>File: 'Tang_HDExaminer_layout_SLO.csv' Source: https://www.ebi.ac.uk/pride/archive/projects/PXD047461</p> <p>columns:  Protein;Protein State;Deut Time;Experiment;Start;End;Sequence;Charge;Search RT;Actual RT;# Spectra;Peak Width;m/z Shift;Max Inty;Exp Cent;Exp Cent 2;L/R Ratio;Theor Cent;Theor Cent 2;Theor L/R;Score;Cent Diff;# Deut;Deut %;Cent Diff 2;# Deut 2;Deut % 2;Confidence</p> <p>Comments: This is a ; separated file</p>"},{"location":"hd_examiner_formats/#ekstrom-et-al-hd-examiner-export","title":"Ekstr\u00f6m et al HD examiner export","text":"<p>File: 'HDX_S3.csv' Source: https://www.ebi.ac.uk/pride/archive/projects/PXD021266</p> <p>Columns: Protein State,Protein,Start,End,Sequence,Peptide Mass,RT (min),Deut Time (sec),maxD,Theor #D,#D,%D,Conf Interval (#D),#Rep,Confidence,Stddev,p</p> <p>Format: HD examiner summary file</p>"},{"location":"hd_examiner_formats/#burke-et-al-hd-examiner-export","title":"Burke et al HD examiner export","text":"<p>File: 'adjusted_ND_peptides.csv' Source: https://www.ebi.ac.uk/pride/archive/projects/PXD022172</p> <p>Columns: Start,End,Sequence,Charge,Search RT,Actual RT,# Spectra,Peak Width,m/z Shift,Max Inty,Exp Cent,Theor Cent,Score,Confidence</p> <p>Format: Unkown</p> <p>Looks like HD examiner but doesnt match previously seen columns</p>"},{"location":"hd_examiner_formats/#bryan-et-al-hd-examiner-export","title":"Bryan et al HD examiner export","text":"<p>File: 2020_ISB_Droplet_HXMS_Experiment_Data_Reduction_All_Results_Table_20220603.csv Source: https://www.ebi.ac.uk/pride/archive/projects/PXD034374</p> <p>Columns: Protein State,Deut Time,Experiment,Start,End,Sequence,Charge,Search RT,Actual RT,# Spectra,Peak Width,m/z Shift,Max Inty,Exp Cent,Theor Cent,Score,Cent Diff,# Deut,Deut %,Confidence</p> <p>Format: Unkown</p> <p>Looks like HD examiner but doesnt match previously seen columns</p>"},{"location":"hd_examiner_formats/#tuttlelm-all_results","title":"tuttlelm all_results","text":"<p>File: all_results.csv Source: PyHDX PR by Lisa Tuttle (https://github.com/Jhsmit/PyHDX/pull/350)</p> <p>Columns: Protein State,Deut Time,Experiment,Start,End,Sequence,Charge,Search RT,Actual RT,# Spectra,Peak Width,m/z Shift,Max Inty,Exp Cent,Theor Cent,Score,Cent Diff,# Deut,Deut %,Confidence</p> <p>Format: FD control: 'FD'</p> <p>Comments: Equal to Bryan et al HD examiner export not an aggregated format</p>"},{"location":"hd_examiner_formats/#tuttlelm-uptake_summary","title":"tuttlelm uptake_summary","text":"<p>File: uptake_summary.csv Source: PyHDX PR by Lisa Tuttle (https://github.com/Jhsmit/PyHDX/pull/350)</p> <p>Columns: Protein State,Protein,Start,End,Sequence,Peptide Mass,RT (min),Deut Time (sec),maxD,Theor Uptake #D,#D,%D,Conf Interval (#D),#Rep,Confidence,Stddev,p</p> <p>Format: (almost!) HD examiner summary file</p> <p>'Theor Uptake #D' instead of 'Theor #D'</p> <p>FD control: 'MAX' (older version)</p> <p>Comments:</p>"},{"location":"hd_examiner_formats/#kingfisher-hd-examiner-example","title":"Kingfisher HD examiner example","text":"<p>File: HDX export file test.csv Source: https://github.com/juan2089/Kingfisher-HDX/blob/Kingfisher-v1.1/www/HDX%20export%20file%20test.csv</p> <p>Columns: The first line is a header with exposure times. </p> <p>The second line has the column names, starting with: 'State,Protein,Start,End,Sequence,Search RT,Charge,Max D,'</p> <p>Followed by repeating blocks of: 'Start RT,End RT,#D,%D,#D right,%D right,Score,Conf,' Format: (almost!) HD examiner summary file</p> <p>This is a HD examiner 'peptide pool' file</p>"},{"location":"hd_examiner_formats/#hd-examiner-manual-on-exporting-data","title":"HD Examiner manual on exporting data","text":"<p>Peptide Pool Results / Uptake Summary Table To export the deuteration level table to a .csv file, switch to the Peptides View, then select a Peptide Pool or any peptide in that pool. Select \u201cTools\u201d, then \u201cExport\u201d, then \u201cPeptide Pool Results\u2026\u201d or \u201cUptake Summary Table\u2026\u201d or right-click on any Peptide Pool or peptide and select \u201cExport Peptide Pool Results\u2026\u201d or \u201cExport Uptake Summary Table\u2026\u201d. Specify a filename. HDExaminer will save the table to that file. The two tables contain the same data, but are formatted differently (one peptide per line versus one result per line). </p> <p>'All results' To export all tables to a .csv file, switch to the Analysis View, then select any experiment. Select \u201cTools\u201d, then \u201cExport\u201d, then \u201cAll Results Tables\u2026\u201d or right-click on the results table and select \u201cExport All Tables\u2026\u201d. Specify a filename. HDExaminer will save the combined tables to that file. </p> <p>Note on back-exchange correction</p> <p>In Masson et al., Nat Methods 16, 595\u2013602 (2019), the HDX community codified many recommendations for the design, analysis and reporting of HDX experiments. HDExaminer\u2019s data export features now adhere to the recommendations set forth in that paper. </p> <p>Note that Recommendation 3.1 of that paper states that reported #D results should not be back-exchange corrected. In light of this recommendation, the #D results reported in HDExaminer\u2019s Uptake Summary Table are not corrected for back exchange, regardless of your HDExaminer settings. This is a change in behavior from previous versions of HDExaminer. </p> <p>Unfortunately, its not known which verions of HDExaminer this change applies to and which files/formats are affected. </p>"},{"location":"install/","title":"Installation","text":"<pre><code>$ pip install hdxms-datasets\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>convert</li> <li>database</li> <li>expr</li> <li>formats</li> <li>migration<ul> <li>v020</li> </ul> </li> <li>models</li> <li>plot</li> <li>process</li> <li>reader</li> <li>stable<ul> <li>v020<ul> <li>backend</li> <li>convert</li> <li>datasets</li> <li>datavault</li> <li>expr</li> <li>formats</li> <li>plot</li> <li>process</li> <li>reader</li> <li>utils</li> </ul> </li> </ul> </li> <li>utils</li> <li>verification</li> <li>view</li> <li>web<ul> <li>components</li> <li>models</li> <li>state</li> <li>upload_form</li> </ul> </li> </ul>"},{"location":"reference/convert/","title":"convert","text":""},{"location":"reference/convert/#convert.cast_exposure","title":"<code>cast_exposure(df)</code>","text":"<p>Tries to cast the exposure column to float</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def cast_exposure(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Tries to cast the exposure column to float\"\"\"\n    try:\n        df = df.with_columns(nw.col(\"exposure\").str.strip_chars(\"s\").cast(nw.Float64))\n    except (InvalidOperationError, ValueError, AttributeError):\n        pass\n    return df\n</code></pre>"},{"location":"reference/convert/#convert.convert_rt","title":"<code>convert_rt(rt_str)</code>","text":"<p>Convert HDExaminer retention time string to float example: \"7.44-7.65\" -&gt; 7.545</p> <p>Lossy conversion</p> <p>This conversion loses information. The full range is not preserved. This was done such that retention time can be stored as float and thus be aggregated. Future versions may store the full range with additional <code>rt_min</code> and <code>rt_max</code> columns.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def convert_rt(rt_str: str) -&gt; float:\n    \"\"\"Convert HDExaminer retention time string to float\n    example: \"7.44-7.65\" -&gt; 7.545\n\n    !!! warning \"Lossy conversion\"\n        This conversion loses information. The full range is not preserved. This was done such that\n        retention time can be stored as float and thus be aggregated.\n        Future versions may store the full range with additional `rt_min` and `rt_max` columns.\n\n    \"\"\"\n    vmin, vmax = rt_str.split(\"-\")\n    mean = (float(vmin) + float(vmax)) / 2.0\n    return mean\n</code></pre>"},{"location":"reference/convert/#convert.from_dynamx_cluster","title":"<code>from_dynamx_cluster(dynamx_df)</code>","text":"<p>Convert a DynamX cluster DataFrame to OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_dynamx_cluster(dynamx_df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert a DynamX cluster DataFrame to OpenHDX format.\n    \"\"\"\n    column_mapping = {\n        \"State\": \"state\",\n        \"Exposure\": \"exposure\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Sequence\": \"sequence\",\n        \"File\": \"replicate\",\n        \"z\": \"charge\",\n        \"Center\": \"centroid_mz\",\n        \"Inten\": \"intensity\",\n        \"RT\": \"rt\",\n    }\n\n    column_order = list(column_mapping.values())\n    column_order.insert(column_order.index(\"charge\") + 1, \"centroid_mass\")\n\n    df = (\n        dynamx_df.rename(column_mapping)\n        .with_columns([centroid_mass, nw.col(\"exposure\") * 60.0])\n        .select(column_order)\n        .sort(by=[\"state\", \"exposure\", \"start\", \"end\", \"replicate\"])\n    )\n\n    return df\n</code></pre>"},{"location":"reference/convert/#convert.from_dynamx_state","title":"<code>from_dynamx_state(dynamx_df)</code>","text":"<p>Convert a DynamX state DataFrame to OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_dynamx_state(dynamx_df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert a DynamX state DataFrame to OpenHDX format.\n    \"\"\"\n    column_mapping = {\n        # TODO add Protein\n        \"State\": \"state\",\n        \"Exposure\": \"exposure\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Sequence\": \"sequence\",\n        \"Uptake\": \"uptake\",\n        \"Uptake SD\": \"uptake_sd\",\n        \"Center\": \"centroid_mz\",\n        \"RT\": \"rt\",\n        \"RT SD\": \"rt_sd\",\n    }\n\n    column_order = list(column_mapping.values())\n\n    df = (\n        dynamx_df.rename(column_mapping)\n        .with_columns([nw.col(\"exposure\") * 60.0])\n        .select(column_order)\n        .sort(by=[\"state\", \"exposure\", \"start\", \"end\"])\n    )\n\n    return df\n</code></pre>"},{"location":"reference/convert/#convert.from_hdexaminer_all_results","title":"<code>from_hdexaminer_all_results(hd_examiner_df, extra_columns=None)</code>","text":"<p>Convert an HDExaminer 'All results' exported DataFrame to OpenHDX format.</p> <p>To export as all results (from HDExaminer documentation):</p> <p>To export all tables to a .csv file, switch to the Analysis View, then select any experiment. Select \u201cTools\u201d, then \u201cExport\u201d, then \u201cAll Results Tables\u2026\u201d or right-click on the results table and select \u201cExport All Tables\u2026\u201d. Specify a filename. HDExaminer will save the combined tables to that file.</p> <p>Parameters:</p> Name Type Description Default <code>hd_examiner_df</code> <code>DataFrame</code> <p>DataFrame in HDExaminer format.</p> required <code>extra_columns</code> <code>list[str] | dict[str, str] | str | None</code> <p>Additional columns to include, either as a list/str of column name(s)            or a dictionary mapping original column names to new names.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame in OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_hdexaminer_all_results(\n    hd_examiner_df: nw.DataFrame,\n    extra_columns: list[str] | dict[str, str] | str | None = None,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert an HDExaminer 'All results' exported DataFrame to OpenHDX format.\n\n    To export as all results (from HDExaminer documentation):\n\n    To export all tables to a .csv file, switch to the Analysis View, then select any experiment.\n    Select \u201cTools\u201d, then \u201cExport\u201d, then \u201cAll Results Tables\u2026\u201d or right-click on the results table\n    and select \u201cExport All Tables\u2026\u201d. Specify a filename. HDExaminer will save the combined tables\n    to that file.\n\n    Args:\n        hd_examiner_df: DataFrame in HDExaminer format.\n        extra_columns: Additional columns to include, either as a list/str of column name(s)\n                       or a dictionary mapping original column names to new names.\n\n    Returns:\n        A DataFrame in OpenHDX format.\n\n    \"\"\"\n    from hdxms_datasets.reader import BACKEND\n\n    column_mapping = {\n        \"Protein State\": \"state\",\n        \"Deut Time\": \"exposure\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Sequence\": \"sequence\",\n        \"Experiment\": \"replicate\",\n        \"Charge\": \"charge\",\n        \"Exp Cent\": \"centroid_mz\",\n        \"Max Inty\": \"intensity\",\n    }\n\n    column_order = list(column_mapping.values())\n    column_order.insert(column_order.index(\"charge\") + 1, \"centroid_mass\")\n    column_order.append(\"rt\")\n\n    cols = _fmt_extra_columns(extra_columns)\n\n    column_mapping.update(cols)\n    column_order.extend(cols.values())\n\n    # TODO: parse to two columns, start_rt, end_rt\n    rt_values = [convert_rt(rt_str) for rt_str in hd_examiner_df[\"Actual RT\"]]\n    rt_series = nw.new_series(values=rt_values, name=\"rt\", backend=BACKEND)\n\n    df = (\n        hd_examiner_df.rename(column_mapping)\n        .with_columns([centroid_mass, rt_series])\n        .select(column_order)\n        .sort(\n            by=[\"state\", \"exposure\", \"start\", \"end\", \"replicate\"]\n        )  # TODO sort by protein first (if available), take from global var\n    )\n\n    return cast_exposure(df)\n</code></pre>"},{"location":"reference/convert/#convert.from_hdexaminer_peptide_pool","title":"<code>from_hdexaminer_peptide_pool(df)</code>","text":"<p>Convert from hd examiner peptide pool format to OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_hdexaminer_peptide_pool(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Convert from hd examiner peptide pool format to OpenHDX format.\"\"\"\n    column_mapping = {\n        \"State\": \"state\",\n        \"Exposure\": \"exposure\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Sequence\": \"sequence\",\n        \"Charge\": \"charge\",\n        \"#D\": \"uptake\",\n        \"Start RT\": \"start_rt\",\n        \"End RT\": \"end_rt\",\n        \"Search RT\": \"search_rt\",\n    }\n\n    df = df.rename(column_mapping)\n    column_order = list(column_mapping.values())\n\n    df = df.select(column_order)  # .sort(by=[\"state\", \"exposure\", \"start\", \"end\"])\n\n    return cast_exposure(df)\n</code></pre>"},{"location":"reference/convert/#convert.from_hdexaminer_uptake_summary","title":"<code>from_hdexaminer_uptake_summary(df)</code>","text":"<p>Convert from hd examiner uptake summary format to OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_hdexaminer_uptake_summary(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Convert from hd examiner uptake summary format to OpenHDX format.\"\"\"\n    column_mapping = {\n        \"Protein\": \"protein\",\n        \"Protein State\": \"state\",\n        \"Start\": \"start\",\n        \"End\": \"end\",\n        \"Deut Time (sec)\": \"exposure\",\n        #'Peptide Mass' ?,\n        \"Sequence\": \"sequence\",\n        \"#D\": \"uptake\",\n        \"RT (min)\": \"rt\",\n        \"#Rep\": \"n_replicates\",\n    }\n\n    df = df.rename(column_mapping)\n    column_order = list(column_mapping.values())\n\n    df = df.select(column_order)  # .sort(by=[\"state\", \"exposure\", \"start\", \"end\"])\n\n    return cast_exposure(df)\n</code></pre>"},{"location":"reference/convert/#convert.from_hxms","title":"<code>from_hxms(hxms_df, extra_columns='sequence')</code>","text":"<p>Convert an HXMS DataFrame to OpenHDX format.</p> <p>Parameters:</p> Name Type Description Default <code>hxms_df</code> <code>DataFrame</code> <p>DataFrame in HXMS format.</p> required <code>extra_columns</code> <code>list[str] | dict[str, str] | str | None</code> <p>Additional columns to include, either as a list/str of column name(s) or a dictionary mapping original column names to new names.</p> <code>'sequence'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame in OpenHDX format.</p> Source code in <code>hdxms_datasets/convert.py</code> <pre><code>def from_hxms(\n    hxms_df: nw.DataFrame,\n    extra_columns: list[str] | dict[str, str] | str | None = \"sequence\",\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert an HXMS DataFrame to OpenHDX format.\n\n    Args:\n        hxms_df: DataFrame in HXMS format.\n        extra_columns: Additional columns to include, either as a list/str of column name(s)\n            or a dictionary mapping original column names to new names.\n\n    Returns:\n        A DataFrame in OpenHDX format.\n\n    \"\"\"\n\n    column_mapping = {\n        \"START\": \"start\",\n        \"END\": \"end\",\n        \"REP\": \"replicate\",\n        \"TIME(Sec)\": \"exposure\",\n        \"UPTAKE\": \"uptake\",\n    }\n\n    column_order = list(column_mapping.values())\n    cols = _fmt_extra_columns(extra_columns)\n    column_mapping.update(cols)\n    column_order.extend(cols.values())\n\n    df = hxms_df.rename(column_mapping)\n    df = df.select(column_order).sort(by=[\"exposure\", \"start\", \"end\", \"replicate\"])\n\n    return df\n</code></pre>"},{"location":"reference/database/","title":"database","text":""},{"location":"reference/database/#database.DataBase","title":"<code>DataBase</code>","text":"Source code in <code>hdxms_datasets/database.py</code> <pre><code>class DataBase:\n    def __init__(self, database_dir: Path | str):\n        self.database_dir = Path(database_dir)\n        self.database_dir.mkdir(exist_ok=True, parents=True)\n\n    @property\n    def datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the cache dir\"\"\"\n        return [d.stem for d in self.database_dir.iterdir() if self.is_dataset(d)]\n\n    @staticmethod\n    def is_dataset(path: Path) -&gt; bool:\n        \"\"\"\n        Checks if the supplied path is a HDX-MS dataset.\n        \"\"\"\n\n        return (path / \"dataset.json\").exists()\n\n    def load_dataset(self, dataset_id: str) -&gt; HDXDataSet:\n        return load_dataset(self.database_dir / dataset_id)\n</code></pre>"},{"location":"reference/database/#database.DataBase.datasets","title":"<code>datasets</code>  <code>property</code>","text":"<p>List of available datasets in the cache dir</p>"},{"location":"reference/database/#database.DataBase.is_dataset","title":"<code>is_dataset(path)</code>  <code>staticmethod</code>","text":"<p>Checks if the supplied path is a HDX-MS dataset.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>@staticmethod\ndef is_dataset(path: Path) -&gt; bool:\n    \"\"\"\n    Checks if the supplied path is a HDX-MS dataset.\n    \"\"\"\n\n    return (path / \"dataset.json\").exists()\n</code></pre>"},{"location":"reference/database/#database.RemoteDataBase","title":"<code>RemoteDataBase</code>","text":"<p>               Bases: <code>DataBase</code></p> <p>A database for HDX-MS datasets, with the ability to fetch datasets from a remote repository.</p> <p>Parameters:</p> Name Type Description Default <code>database_dir</code> <code>Path | str</code> <p>Directory to store downloaded datasets.</p> required <code>remote_url</code> <code>str</code> <p>URL of the remote repository (default: DATABASE_URL).</p> <code>DATABASE_URL</code> Source code in <code>hdxms_datasets/database.py</code> <pre><code>class RemoteDataBase(DataBase):\n    \"\"\"\n    A database for HDX-MS datasets, with the ability to fetch datasets from a remote repository.\n\n    Args:\n        database_dir: Directory to store downloaded datasets.\n        remote_url: URL of the remote repository (default: DATABASE_URL).\n    \"\"\"\n\n    def __init__(\n        self,\n        database_dir: Path | str,\n        remote_url: str = DATABASE_URL,\n    ):\n        super().__init__(database_dir)\n        self.remote_url = remote_url\n\n        index_url = urljoin(DATABASE_URL, CATALOG_FILE)\n        response = requests.get(index_url)\n\n        # TODO keep catalogs on a per-url basis in a singleton\n        if response.ok:\n            df = read_csv(response.content)\n            self.datasets_catalog = df\n        else:\n            raise HTTPError(\n                index_url,\n                response.status_code,\n                \"Error fetching dataset index\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n    @property\n    def remote_datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the remote repository\"\"\"\n        return self.datasets_catalog[\"id\"].to_list()\n\n    @property\n    def local_datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the local database directory\"\"\"\n        return self.datasets\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all datasets from the local database directory\"\"\"\n        for dir in self.database_dir.iterdir():\n            shutil.rmtree(dir)\n\n    def fetch_dataset(self, data_id: str) -&gt; tuple[bool, str]:\n        \"\"\"\n        Download a dataset from the online repository to `database_dir`\n\n        Args:\n            data_id: The ID of the dataset to download.\n\n        Returns:\n            A tuple (success: bool, message: str):\n            - success: True if the dataset was successfully downloaded, False otherwise.\n            - message: A message indicating the result of the download.\n        \"\"\"\n\n        if data_id not in self.remote_datasets:\n            return False, f\"Dataset ID {data_id!r} not found in remote database.\"\n\n        json_url = urljoin(DATABASE_URL, data_id + \"/dataset.json\")\n        response = requests.get(json_url)\n\n        # confirm if the json is according to spec\n        try:\n            dataset = HDXDataSet.model_validate_json(\n                response.content,\n            )\n        except Exception as e:\n            return False, f\"Error validating dataset JSON: {e}\"\n\n        # create a list of all Path objects in the dataset plus the dataset.json file\n        data_files = list(set(extract_values_by_types(dataset, Path))) + [\n            Path(\"dataset.json\")\n        ]\n\n        # create the target directory to store the dataset\n        output_pth = self.database_dir / data_id\n        if output_pth.exists():\n            return False, \"Dataset already exists in the local database.\"\n        else:\n            output_pth.mkdir()\n\n        for data_file in data_files:\n            data_url = urljoin(DATABASE_URL, data_id + \"/\" + data_file.as_posix())\n\n            response = requests.get(data_url)\n            if response.ok:\n                # write the file to disk\n                fpath = output_pth / Path(data_file)\n                fpath.parent.mkdir(parents=True, exist_ok=True)\n                fpath.write_bytes(response.content)\n            else:\n                shutil.rmtree(output_pth)  # clean up partial download\n                return False, f\"Failed to download {data_file}: {response.status_code}\"\n\n        return True, \"\"\n</code></pre>"},{"location":"reference/database/#database.RemoteDataBase.local_datasets","title":"<code>local_datasets</code>  <code>property</code>","text":"<p>List of available datasets in the local database directory</p>"},{"location":"reference/database/#database.RemoteDataBase.remote_datasets","title":"<code>remote_datasets</code>  <code>property</code>","text":"<p>List of available datasets in the remote repository</p>"},{"location":"reference/database/#database.RemoteDataBase.clear","title":"<code>clear()</code>","text":"<p>Clear all datasets from the local database directory</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all datasets from the local database directory\"\"\"\n    for dir in self.database_dir.iterdir():\n        shutil.rmtree(dir)\n</code></pre>"},{"location":"reference/database/#database.RemoteDataBase.fetch_dataset","title":"<code>fetch_dataset(data_id)</code>","text":"<p>Download a dataset from the online repository to <code>database_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>The ID of the dataset to download.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>A tuple (success: bool, message: str):</p> <code>str</code> <ul> <li>success: True if the dataset was successfully downloaded, False otherwise.</li> </ul> <code>tuple[bool, str]</code> <ul> <li>message: A message indicating the result of the download.</li> </ul> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def fetch_dataset(self, data_id: str) -&gt; tuple[bool, str]:\n    \"\"\"\n    Download a dataset from the online repository to `database_dir`\n\n    Args:\n        data_id: The ID of the dataset to download.\n\n    Returns:\n        A tuple (success: bool, message: str):\n        - success: True if the dataset was successfully downloaded, False otherwise.\n        - message: A message indicating the result of the download.\n    \"\"\"\n\n    if data_id not in self.remote_datasets:\n        return False, f\"Dataset ID {data_id!r} not found in remote database.\"\n\n    json_url = urljoin(DATABASE_URL, data_id + \"/dataset.json\")\n    response = requests.get(json_url)\n\n    # confirm if the json is according to spec\n    try:\n        dataset = HDXDataSet.model_validate_json(\n            response.content,\n        )\n    except Exception as e:\n        return False, f\"Error validating dataset JSON: {e}\"\n\n    # create a list of all Path objects in the dataset plus the dataset.json file\n    data_files = list(set(extract_values_by_types(dataset, Path))) + [\n        Path(\"dataset.json\")\n    ]\n\n    # create the target directory to store the dataset\n    output_pth = self.database_dir / data_id\n    if output_pth.exists():\n        return False, \"Dataset already exists in the local database.\"\n    else:\n        output_pth.mkdir()\n\n    for data_file in data_files:\n        data_url = urljoin(DATABASE_URL, data_id + \"/\" + data_file.as_posix())\n\n        response = requests.get(data_url)\n        if response.ok:\n            # write the file to disk\n            fpath = output_pth / Path(data_file)\n            fpath.parent.mkdir(parents=True, exist_ok=True)\n            fpath.write_bytes(response.content)\n        else:\n            shutil.rmtree(output_pth)  # clean up partial download\n            return False, f\"Failed to download {data_file}: {response.status_code}\"\n\n    return True, \"\"\n</code></pre>"},{"location":"reference/database/#database.export_dataset","title":"<code>export_dataset(dataset, tgt_dir)</code>","text":"<p>Store a dataset to a target directory. This will copy the data files to a 'data' subdirectory and write the dataset JSON.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def export_dataset(dataset: HDXDataSet, tgt_dir: Path) -&gt; None:\n    \"\"\"\n    Store a dataset to a target directory.\n    This will copy the data files to a 'data' subdirectory and write the dataset JSON.\n    \"\"\"\n\n    # copy the dataset to update the paths\n    ds_copy = dataset.model_copy(deep=True)\n\n    data_dir = tgt_dir / \"data\"\n    data_dir.mkdir(exist_ok=True, parents=True)\n\n    # copy the sequence file\n    shutil.copy(\n        ds_copy.structure.data_file, data_dir / ds_copy.structure.data_file.name\n    )\n    # the the path to the copied file relative path\n    ds_copy.structure.data_file = Path(\"data\") / ds_copy.structure.data_file.name\n\n    # repeat for the peptides\n    for state in ds_copy.states:\n        for peptides in state.peptides:\n            shutil.copy(peptides.data_file, data_dir / peptides.data_file.name)\n            # update the path to the copied file relative path\n            peptides.data_file = Path(\"data\") / peptides.data_file.name\n\n    # write the dataset to a JSON file\n    s = ds_copy.model_dump_json(indent=2, exclude_none=True)\n    Path(tgt_dir / \"dataset.json\").write_text(s)\n</code></pre>"},{"location":"reference/database/#database.find_file_hash_matches","title":"<code>find_file_hash_matches(dataset, database_dir)</code>","text":"<p>Check if a new dataset matches an existing dataset in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def find_file_hash_matches(dataset: HDXDataSet, database_dir: Path) -&gt; list[str]:\n    \"\"\"\n    Check if a new dataset matches an existing dataset in the database directory.\n    \"\"\"\n    try:\n        catalog = nw.read_csv(\n            str(database_dir / \"datasets_catalog.csv\"), backend=BACKEND\n        )\n    except FileNotFoundError:\n        return []\n\n    assert dataset.file_hash is not None, \"Dataset must have a file hash.\"\n    matching_datasets = catalog.filter(nw.col(\"file_hash\") == dataset.file_hash[:16])\n\n    return matching_datasets[\"id\"].to_list()\n</code></pre>"},{"location":"reference/database/#database.generate_datasets_catalog","title":"<code>generate_datasets_catalog(database_dir)</code>","text":"<p>Generate an overview DataFrame of all datasets in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def generate_datasets_catalog(database_dir: Path) -&gt; nw.DataFrame:\n    \"\"\"\n    Generate an overview DataFrame of all datasets in the database directory.\n    \"\"\"\n    records = []\n    for ds_id in list_datasets(database_dir):\n        ds_path = database_dir / ds_id / \"dataset.json\"\n        if ds_path.exists():\n            dataset = HDXDataSet.model_validate_json(\n                ds_path.read_text(), context={\"dataset_root\": database_dir / ds_id}\n            )\n            records.append(\n                {\n                    \"id\": ds_id,\n                    \"description\": dataset.description,\n                    \"author\": dataset.metadata.authors[0].last_name,\n                    \"doi\": dataset.metadata.publication.doi\n                    if dataset.metadata.publication\n                    else None,\n                    \"created_date\": dataset.metadata.created_date,\n                    \"uniprot_accession_number\": dataset.protein_identifiers.uniprot_accession_number,\n                    \"file_hash\": dataset.file_hash,\n                }\n            )\n\n    df = nw.from_dict(records_to_dict(records), backend=BACKEND)\n\n    return df\n</code></pre>"},{"location":"reference/database/#database.list_datasets","title":"<code>list_datasets(database_dir)</code>","text":"<p>List all valid dataset IDs in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def list_datasets(database_dir: Path) -&gt; list[str]:\n    \"\"\"\n    List all valid dataset IDs in the database directory.\n    \"\"\"\n\n    return [p.stem for p in database_dir.iterdir() if valid_id(p.stem)]\n</code></pre>"},{"location":"reference/database/#database.load_dataset","title":"<code>load_dataset(pth)</code>","text":"<p>Load a dataset from a JSON file, .zip file or directory.</p> <p>Parameters:</p> Name Type Description Default <code>pth</code> <code>Path | str</code> <p>Path to a dataset.json file, a .zip file containing a dataset, or a directory.</p> required <p>Returns:</p> Type Description <code>HDXDataSet</code> <p>The loaded HDXDataSet.</p> Note <p>When loading from a .zip file, the contents are extracted to a temporary directory that persists for the lifetime of the program.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def load_dataset(pth: Path | str) -&gt; HDXDataSet:\n    \"\"\"\n    Load a dataset from a JSON file, .zip file or directory.\n\n    Args:\n        pth: Path to a dataset.json file, a .zip file containing a dataset, or a directory.\n\n    Returns:\n        The loaded HDXDataSet.\n\n    Note:\n        When loading from a .zip file, the contents are extracted to a temporary directory\n        that persists for the lifetime of the program.\n    \"\"\"\n\n    pth = Path(pth)\n\n    # Handle .zip files by extracting to a temp directory\n    if pth.is_file() and pth.suffix.lower() == \".zip\":\n        temp_dir = Path(tempfile.mkdtemp(prefix=\"hdxms_dataset_\"))\n\n        with zipfile.ZipFile(pth, \"r\") as zip_ref:\n            zip_ref.extractall(temp_dir)\n\n        return load_dataset(temp_dir)  # recursively load from the extracted directory\n\n    elif pth.is_file() and pth.suffix.lower() == \".json\":\n        dataset_root = pth.parent\n        json_pth = pth\n    elif pth.is_dir():\n        # find the dataset.json file in the extracted contents\n        json_files = list(pth.rglob(\"dataset.json\"))\n        if not json_files:\n            raise FileNotFoundError(\n                \"No dataset.json file found in the extracted zip contents.\"\n            )\n        if len(json_files) &gt; 1:\n            raise ValueError(\n                \"Multiple dataset.json files found in the extracted zip contents.\"\n            )\n\n        json_pth = json_files[0]\n        dataset_root = json_pth.parent\n    else:\n        raise ValueError(\"Path must be a .zip file, .json file or directory.\")\n\n    dataset = HDXDataSet.model_validate_json(\n        json_pth.read_text(), context={\"dataset_root\": dataset_root}\n    )\n    return dataset\n</code></pre>"},{"location":"reference/database/#database.mint_new_dataset_id","title":"<code>mint_new_dataset_id(current_ids=KNOWN_HDX_IDS)</code>","text":"<p>Mint a new dataset ID that does not conflict with existing IDs in the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def mint_new_dataset_id(current_ids: set[str] = KNOWN_HDX_IDS) -&gt; str:\n    \"\"\"\n    Mint a new dataset ID that does not conflict with existing IDs in the database directory.\n    \"\"\"\n    while True:\n        new_id = f\"HDX_{uuid.uuid4().hex[:8].upper()}\"\n        if new_id not in current_ids:\n            return new_id\n</code></pre>"},{"location":"reference/database/#database.populate_known_ids","title":"<code>populate_known_ids(database_dir, append=True)</code>","text":"<p>Populate the KNOWN_HDX_IDS set with existing dataset IDs from the database directory.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def populate_known_ids(database_dir: Path, append=True) -&gt; None:\n    \"\"\"\n    Populate the KNOWN_HDX_IDS set with existing dataset IDs from the database directory.\n    \"\"\"\n    global KNOWN_HDX_IDS\n    if append:\n        KNOWN_HDX_IDS.update(list_datasets(database_dir))\n    else:\n        KNOWN_HDX_IDS = set(list_datasets(database_dir))\n</code></pre>"},{"location":"reference/database/#database.submit_dataset","title":"<code>submit_dataset(dataset, database_dir, allow_mint_new_id=False, check_existing=True, verify=True, strict=True)</code>","text":"<p>Submit a dataset to a local HDX-MS database.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>HDXDataSet</code> <p>The HDXDataSet to submit.</p> required <code>database_dir</code> <code>Path</code> <p>The directory where the dataset will be stored.</p> required <code>allow_mint_new_id</code> <code>bool</code> <p>If True, allows minting a new dataset ID if it is already present in the database.</p> <code>False</code> <code>check_existing</code> <code>bool</code> <p>If True, checks if the dataset already exists in the database.</p> <code>True</code> <code>verify</code> <code>bool</code> <p>If True, verifies the dataset before submission.</p> <code>True</code> <code>strict</code> <code>bool</code> <p>If True, performs strict verification of the dataset.</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>A tuple (success: bool, message: str):</p> <code>str</code> <ul> <li>success: True if the dataset was successfully submitted, False otherwise.</li> </ul> <code>tuple[bool, str]</code> <ul> <li>message: A message indicating the result of the submission.</li> </ul> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def submit_dataset(\n    dataset: HDXDataSet,\n    database_dir: Path,\n    allow_mint_new_id: bool = False,\n    check_existing: bool = True,\n    verify: bool = True,\n    strict: bool = True,\n) -&gt; tuple[bool, str]:\n    \"\"\"\n    Submit a dataset to a local HDX-MS database.\n\n    Args:\n        dataset: The HDXDataSet to submit.\n        database_dir: The directory where the dataset will be stored.\n        allow_mint_new_id: If True, allows minting a new dataset ID if it is already present in the database.\n        check_existing: If True, checks if the dataset already exists in the database.\n        verify: If True, verifies the dataset before submission.\n        strict: If True, performs strict verification of the dataset.\n\n    Returns:\n        A tuple (success: bool, message: str):\n        - success: True if the dataset was successfully submitted, False otherwise.\n        - message: A message indicating the result of the submission.\n\n    \"\"\"\n\n    # copy the datasets in case we need to update the ID\n    dataset_copy = dataset.model_copy(deep=True)\n\n    if verify:\n        verify_dataset(dataset_copy, strict=strict)\n\n    if not database_dir.is_absolute():\n        raise ValueError(\"Database directory must be an absolute path.\")\n\n    # check if the uniprot ID is already there,\n    # although there could be multiple states with the same uniprot ID\n    # this is a quick check to avoid duplicates\n    if check_existing:\n        matches = find_file_hash_matches(dataset_copy, database_dir)\n        if matches:\n            if len(matches) == 1:\n                msg = (\n                    f\"Dataset matches an existing dataset in the database: {matches[0]}\"\n                )\n            else:\n                msg = f\"Dataset matches existing datasets in the database: {', '.join(matches)}\"\n            return False, msg\n\n    # mint a new ID if not provided\n    existing_ids = set(list_datasets(database_dir))\n    if dataset_copy.hdx_id in existing_ids:\n        if allow_mint_new_id:\n            dataset_id = mint_new_dataset_id(existing_ids)\n            dataset_copy.hdx_id = dataset_id\n        else:\n            return (\n                False,\n                f\"Dataset ID {dataset_copy.hdx_id} already exists in the database.\",\n            )\n    else:\n        dataset_id = dataset_copy.hdx_id\n\n    # TODO this check is now superfluous\n    if not valid_id(dataset_id):\n        raise ValueError(\n            f\"Invalid dataset ID: {dataset_id}. \"\n            \"A valid ID starts with 'HDX_' followed by 8 uppercase alphanumeric characters.\"\n        )\n\n    # create the target directory\n    tgt_dir = database_dir / dataset_id\n    export_dataset(dataset_copy, tgt_dir)\n\n    # update the catalogue\n    # TODO: update instead of regenerate\n    # TODO: lockfile? https://github.com/harlowja/fasteners\n    # TODO: at the moment this also reads all the datasets again\n    # generate_datasets_catalog(database_dir, save_csv=True)\n\n    return True, dataset_id\n</code></pre>"},{"location":"reference/database/#database.valid_id","title":"<code>valid_id(dataset_id)</code>","text":"<p>Check if the dataset ID is valid. A valid ID starts with 'HDX_' followed by 8 uppercase alphanumeric characters.</p> Source code in <code>hdxms_datasets/database.py</code> <pre><code>def valid_id(dataset_id: str) -&gt; bool:\n    \"\"\"\n    Check if the dataset ID is valid.\n    A valid ID starts with 'HDX_' followed by 8 uppercase alphanumeric characters.\n    \"\"\"\n    return (\n        bool(dataset_id)\n        and dataset_id.startswith(\"HDX_\")\n        and len(dataset_id) == 12\n        and dataset_id[4:].isalnum()\n    )\n</code></pre>"},{"location":"reference/expr/","title":"expr","text":"<p>Narwhals expression used to compute common HDX-MS metrics.</p> <p>Exported metrics from this module are:</p> <ul> <li>centroid_mass</li> <li>max_uptake</li> <li>uptake</li> <li>uptake_sd</li> <li>fd_uptake</li> <li>fd_uptake_sd</li> <li>frac_fd_control</li> <li>frac_fd_control_sd</li> <li>frac_max_uptake</li> <li>frac_max_uptake_sd</li> </ul>"},{"location":"reference/formats/","title":"formats","text":""},{"location":"reference/formats/#formats.FormatSpec","title":"<code>FormatSpec</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Specification for a HDX data format</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Name of the format.</p> required <code>returned_columns</code> <p>List of columns returned by .read(). May return additional columns depending on the format.</p> required <code>filter_columns</code> <p>List of columns that can be used to filter data.</p> required <code>is_valid_file</code> <p>Function to check if a file is valid for this format.</p> required <code>reader</code> <p>Function to read a file into a DataFrame.</p> required <code>converter</code> <p>Function to convert a DataFrame to OpenHDX format.</p> required <code>aggregated</code> <p>Whether the format is aggregated, not aggregated, or None if it depends on the data.</p> required <code>doc</code> <p>Optional documentation string.</p> required Source code in <code>hdxms_datasets/formats.py</code> <pre><code>class FormatSpec(ABC):\n    \"\"\"Specification for a HDX data format\n\n    Args:\n        name: Name of the format.\n        returned_columns: List of columns returned by .read(). May return\n            additional columns depending on the format.\n        filter_columns: List of columns that can be used to filter data.\n        is_valid_file: Function to check if a file is valid for this format.\n        reader: Function to read a file into a DataFrame.\n        converter: Function to convert a DataFrame to OpenHDX format.\n        aggregated: Whether the format is aggregated, not aggregated, or None if\n            it depends on the data.\n        doc: Optional documentation string.\n\n    \"\"\"\n\n    doc: str = \"\"\n    returned_columns: list[str]\n    filter_columns: list[str] = []\n    aggregated: bool | None = None\n\n    def __init_subclass__(cls):\n        \"\"\"Register format in global registry.\"\"\"\n\n        required_class_attrs = [\"returned_columns\"]\n        for attr in required_class_attrs:\n            if not hasattr(cls, attr):\n                raise NotImplementedError(\n                    f\"Class attribute '{attr}' must be defined in subclass '{cls.__name__}'\"\n                )\n\n        if cls.__name__ in FMT_REGISTRY:\n            warnings.warn((f\"Format '{cls.__name__}' is already registered. Overwriting.\"))\n        FMT_REGISTRY[cls.__name__] = cls\n\n    @classmethod\n    @abstractmethod\n    def read(cls, path: Path) -&gt; nw.DataFrame:\n        \"\"\"Read the data to a dataframe using the format's reader.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def convert(cls, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"Convert DataFrame to OpenHDX format.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def valid_file(cls, path: Path) -&gt; bool:\n        \"\"\"Default format identification based on file extension.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def load(cls, path: Path, convert=True) -&gt; nw.DataFrame:\n        \"\"\"Load and convert a file to OpenHDX format.\"\"\"\n        df = cls.read(path)\n        if convert:\n            df = cls.convert(df)\n        return df\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.__init_subclass__","title":"<code>__init_subclass__()</code>","text":"<p>Register format in global registry.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>def __init_subclass__(cls):\n    \"\"\"Register format in global registry.\"\"\"\n\n    required_class_attrs = [\"returned_columns\"]\n    for attr in required_class_attrs:\n        if not hasattr(cls, attr):\n            raise NotImplementedError(\n                f\"Class attribute '{attr}' must be defined in subclass '{cls.__name__}'\"\n            )\n\n    if cls.__name__ in FMT_REGISTRY:\n        warnings.warn((f\"Format '{cls.__name__}' is already registered. Overwriting.\"))\n    FMT_REGISTRY[cls.__name__] = cls\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.convert","title":"<code>convert(df)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Convert DataFrame to OpenHDX format.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>@classmethod\n@abstractmethod\ndef convert(cls, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Convert DataFrame to OpenHDX format.\"\"\"\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.load","title":"<code>load(path, convert=True)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Load and convert a file to OpenHDX format.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>@classmethod\n@abstractmethod\ndef load(cls, path: Path, convert=True) -&gt; nw.DataFrame:\n    \"\"\"Load and convert a file to OpenHDX format.\"\"\"\n    df = cls.read(path)\n    if convert:\n        df = cls.convert(df)\n    return df\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.read","title":"<code>read(path)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Read the data to a dataframe using the format's reader.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>@classmethod\n@abstractmethod\ndef read(cls, path: Path) -&gt; nw.DataFrame:\n    \"\"\"Read the data to a dataframe using the format's reader.\"\"\"\n</code></pre>"},{"location":"reference/formats/#formats.FormatSpec.valid_file","title":"<code>valid_file(path)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Default format identification based on file extension.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>@classmethod\n@abstractmethod\ndef valid_file(cls, path: Path) -&gt; bool:\n    \"\"\"Default format identification based on file extension.\"\"\"\n</code></pre>"},{"location":"reference/formats/#formats.HDExaminer_all_results_with_units","title":"<code>HDExaminer_all_results_with_units</code>","text":"<p>               Bases: <code>HDExaminer_all_results</code></p> <p>There are some 'all results' files out there which have a variation on the standard columns where the units are incuded in the column names: - Peak Width &gt; Peak Width Da - m/z Shift &gt; m/z Shift Da</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>class HDExaminer_all_results_with_units(HDExaminer_all_results):\n    \"\"\"\n    There are some 'all results' files out there which have a variation on the standard columns\n    where the units are incuded in the column names:\n    - Peak Width &gt; Peak Width Da\n    - m/z Shift &gt; m/z Shift Da\n    \"\"\"\n\n    returned_columns = [\n        \"Protein State\",\n        \"Deut Time\",\n        \"Experiment\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"Charge\",\n        \"Search RT\",\n        \"Actual RT\",\n        \"# Spectra\",\n        \"Peak Width Da\",\n        \"m/z Shift Da\",\n        \"Max Inty\",\n        \"Exp Cent\",\n        \"Theor Cent\",\n        \"Score\",\n        \"Cent Diff\",\n        \"# Deut\",\n        \"Deut %\",\n        \"Confidence\",\n    ]\n</code></pre>"},{"location":"reference/formats/#formats.HDExaminer_peptide_pool","title":"<code>HDExaminer_peptide_pool</code>","text":"<p>               Bases: <code>FormatSpec</code></p> <p>HDExaminer Peptide Pool format specification</p> <p>This file consists of an inital block of 8 columns (first 8 in returned_columns), followed by a repeating number of typically 8 columns per exposure (the last 8 in returned_columns). The repeating columns blocks might have 6 columns for FD control blocks. These columns are:</p> <p>['Start RT', 'End RT', '#D', '%Max D', 'Score', 'Conf']</p> <p>The first line in this file is header with exposure times, in seconds formatted as '10s', or 'Full-D' for the full deuterated control.</p> <p>Reading the file returns an additional \"Exposure\" column with values derived from the header line.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>class HDExaminer_peptide_pool(FormatSpec):\n    \"\"\"HDExaminer Peptide Pool format specification\n\n    This file consists of an inital block of 8 columns (first 8 in returned_columns),\n    followed by a repeating number of typically 8 columns per exposure (the last 8 in returned_columns).\n    The repeating columns blocks might have 6 columns for FD control blocks. These columns are:\n\n    &gt;&gt; ['Start RT', 'End RT', '#D', '%Max D', 'Score', 'Conf']\n\n    The first line in this file is header with exposure times, in seconds formatted as '10s', or 'Full-D'\n    for the full deuterated control.\n\n    Reading the file returns an additional \"Exposure\" column with values derived from the header line.\n\n    \"\"\"\n\n    returned_columns = [\n        \"State\",\n        \"Protein\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"Search RT\",\n        \"Charge\",\n        \"Max D\",\n        \"Start RT\",\n        \"End RT\",\n        \"#D\",\n        \"%D\",\n        \"#D right\",\n        \"%D right\",\n        \"Score\",\n        \"Conf\",\n        \"Exposure\",\n    ]\n\n    filter_columns = [\"Protein\", \"State\", \"Exposure\"]\n    aggregated = False\n\n    @classmethod\n    def read(cls, path: Path) -&gt; nw.DataFrame:\n        return read_hdexaminer_peptide_pool(path)\n\n    @classmethod\n    def convert(cls, df: nw.DataFrame) -&gt; nw.DataFrame:\n        return from_hdexaminer_peptide_pool(df)\n\n    @classmethod\n    def valid_file(cls, path: Path) -&gt; bool:\n        columns = read_columns(path, line=1)\n        return set(cls.returned_columns[:-1]).issubset(set(columns))\n</code></pre>"},{"location":"reference/formats/#formats.HDExaminer_uptake_summary","title":"<code>HDExaminer_uptake_summary</code>","text":"<p>               Bases: <code>FormatSpec</code></p> <p>HDExaminer Uptake Summary Table format specification.</p> <p>This is the output of HD Examiner's \"Uptake Summary Table\" report.</p> <p>The resulting table is aggregated, ie only one row per peptide/timepoint.</p> <p>Full deuterated control is labelled as 'MAX', exposure times are in seconds, e.g. '10', '100', etc.</p> <p>Typically includes nondeuterated control as '0'.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>class HDExaminer_uptake_summary(FormatSpec):\n    \"\"\"HDExaminer Uptake Summary Table format specification.\n\n    This is the output of HD Examiner's \"Uptake Summary Table\" report.\n\n    The resulting table is aggregated, ie only one row per peptide/timepoint.\n\n    Full deuterated control is labelled as 'MAX', exposure times are in seconds,\n    e.g. '10', '100', etc.\n\n    Typically includes nondeuterated control as '0'.\n\n    \"\"\"\n\n    returned_columns = [\n        \"Protein State\",\n        \"Protein\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"Peptide Mass\",\n        \"RT (min)\",\n        \"Deut Time (sec)\",\n        \"maxD\",\n        \"Theor Uptake #D\",\n        \"#D\",\n        \"%D\",\n        \"Conf Interval (#D)\",\n        \"#Rep\",\n        \"Confidence\",\n        \"Stddev\",\n        \"p\",\n    ]\n    filter_columns = [\"Protein\", \"Protein State\", \"Deut Time (sec)\"]\n    aggregated = True\n\n    @classmethod\n    def read(cls, path: Path) -&gt; nw.DataFrame:\n        return read_csv(path)\n\n    @classmethod\n    def convert(cls, df: nw.DataFrame) -&gt; nw.DataFrame:\n        return from_hdexaminer_uptake_summary(df)\n\n    @classmethod\n    def valid_file(cls, path: Path) -&gt; bool:\n        columns = read_columns(path)\n        return set(cls.returned_columns).issubset(set(columns))\n</code></pre>"},{"location":"reference/formats/#formats.identify_format","title":"<code>identify_format(path)</code>","text":"<p>Identify format from file path by reading a sample of the data.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>def identify_format(path: Path) -&gt; type[FormatSpec]:\n    \"\"\"Identify format from file path by reading a sample of the data.\"\"\"\n\n    for fmt in FMT_REGISTRY.values():\n        if fmt.valid_file(path):\n            return fmt\n    raise ValueError(f\"Could not identify format for file: {path}\")\n</code></pre>"},{"location":"reference/formats/#formats.is_aggregated","title":"<code>is_aggregated(df)</code>","text":"<p>Checks if a open-hdx formatted DataFrame is aggregated.</p> <p>A DataFrame is considered aggregated if it containns only one replicate or replicates are already averaged, ie if there is only one entry per unique combination of protein, state, start, end, and exposure.</p> Source code in <code>hdxms_datasets/formats.py</code> <pre><code>def is_aggregated(df: nw.DataFrame) -&gt; bool:\n    \"\"\"Checks if a open-hdx formatted DataFrame is aggregated.\n\n    A DataFrame is considered aggregated if it containns only one replicate or\n    replicates are already averaged, ie if there is only one entry per\n    unique combination of protein, state, start, end, and exposure.\n\n    \"\"\"\n    identifier_columns = [\"protein\", \"state\", \"start\", \"end\", \"exposure\"]\n    by = set(identifier_columns) &amp; set(df.columns)\n    unique = df.unique(subset=list(by))\n    return len(unique) == len(df)\n</code></pre>"},{"location":"reference/models/","title":"models","text":""},{"location":"reference/models/#models.DataRepository","title":"<code>DataRepository</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about the data repository where the source data is published</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class DataRepository(BaseModel):\n    \"\"\"Information about the data repository where the source data is published\"\"\"\n\n    name: Annotated[str, Field(..., description=\"Repository name\")]  # ie Pride, Zenodo,\n    url: Annotated[Optional[HttpUrl], Field(None, description=\"Repository URL\")]\n    identifier: Annotated[Optional[str], Field(None, description=\"Repository entry identifier\")]\n    doi: Annotated[Optional[str], Field(None, description=\"Repository DOI\")]\n    description: Annotated[Optional[str], Field(None, description=\"Repository description\")]\n</code></pre>"},{"location":"reference/models/#models.DeuterationType","title":"<code>DeuterationType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Experimental Deuteration Type of the peptide</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class DeuterationType(str, Enum):\n    \"\"\"Experimental Deuteration Type of the peptide\"\"\"\n\n    partially_deuterated = \"partially_deuterated\"\n    fully_deuterated = \"fully_deuterated\"\n    non_deuterated = \"non_deuterated\"\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet","title":"<code>HDXDataSet</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>HDX-MS dataset containing multiple states</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class HDXDataSet(BaseModel):\n    \"\"\"HDX-MS dataset containing multiple states\"\"\"\n\n    hdx_id: Annotated[\n        str, Field(default_factory=id_factory, description=\"HDX-MS dataset identifier\")\n    ]\n\n    # Basic information\n    description: Annotated[Optional[str], Field(None, description=\"Dataset description\")]\n\n    states: list[State] = Field(description=\"List of HDX states in the dataset\")\n    structure: Annotated[Structure, Field(description=\"Structural model file path\")]\n    protein_identifiers: Annotated[\n        ProteinIdentifiers, Field(description=\"Protein identifiers (UniProt, etc.)\")\n    ]\n    metadata: Annotated[DatasetMetadata, Field(description=\"Dataset metadata\")]\n    file_hash: Annotated[\n        Optional[str], Field(None, init=False, description=\"Hash of the files in the dataset\")\n    ]\n\n    @model_validator(mode=\"after\")\n    def validate_hdx_id(self):\n        \"\"\"Validate hdx_id format: 'HDX_' followed by 8 uppercase alphanumeric chars (e.g. HDX_3BAE2080).\"\"\"\n        from hdxms_datasets.database import valid_id\n\n        if not valid_id(self.hdx_id):\n            raise ValueError(\n                \"hdx_id must match pattern 'HDX_XXXXXXXX' where X are uppercase letters or digits, e.g. 'HDX_3BAE2080'\"\n            )\n        return self\n\n    @model_validator(mode=\"after\")\n    def compute_file_hash(self):\n        \"\"\"Compute a hash of the dataset based on its data files\"\"\"\n        if any(not p.exists() for p in self.data_files):\n            self.file_hash = None\n            return self\n\n        self.file_hash = self.hash_files()[:16]  # Shorten to 16 characters\n\n        return self\n\n    @model_validator(mode=\"after\")\n    def verify_unique_state_names(self):\n        \"\"\"Ensure that all state names are unique within the dataset\"\"\"\n        state_names = [state.name for state in self.states]\n        if len(state_names) != len(set(state_names)):\n            raise ValueError(\"State names must be unique within the dataset.\")\n        return self\n\n    def hash_files(self) -&gt; str:\n        return hash_files(self.data_files)  # Ensure files are sorted and hashed consistently\n\n    def validate_file_integrity(self) -&gt; bool:\n        \"\"\"Match hash of files with the stored hash\"\"\"\n        if self.file_hash is None:\n            return False\n        current_hash = self.hash_files()\n        return current_hash.startswith(self.file_hash)\n\n    def get_state(self, state: str | int) -&gt; State:\n        \"\"\"Get a specific state by name or index\"\"\"\n        if isinstance(state, int):\n            return self.states[state]\n        elif isinstance(state, str):\n            for s in self.states:\n                if s.name == state:\n                    return s\n        raise ValueError(f\"State '{state}' not found in dataset.\")\n\n    @property\n    def data_files(self) -&gt; list[Path]:\n        \"\"\"List of all data files in the dataset\"\"\"\n        return sorted(set(extract_values_by_types(self, Path)))\n\n    @classmethod\n    def from_json(\n        cls,\n        json_str: str,\n        dataset_root: Optional[Path] = None,\n    ) -&gt; HDXDataSet:\n        \"\"\"Load dataset from JSON string\n\n        Args:\n            json_str: JSON string representing the dataset\n            dataset_root: Optional root directory to resolve relative paths\n\n        Returns:\n            HDXDataSet instance.\n\n        \"\"\"\n        if dataset_root is None:\n            context = {}\n        else:\n            context = {\"dataset_root\": dataset_root}\n        return cls.model_validate_json(json_str, context=context)\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.data_files","title":"<code>data_files</code>  <code>property</code>","text":"<p>List of all data files in the dataset</p>"},{"location":"reference/models/#models.HDXDataSet.compute_file_hash","title":"<code>compute_file_hash()</code>","text":"<p>Compute a hash of the dataset based on its data files</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef compute_file_hash(self):\n    \"\"\"Compute a hash of the dataset based on its data files\"\"\"\n    if any(not p.exists() for p in self.data_files):\n        self.file_hash = None\n        return self\n\n    self.file_hash = self.hash_files()[:16]  # Shorten to 16 characters\n\n    return self\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.from_json","title":"<code>from_json(json_str, dataset_root=None)</code>  <code>classmethod</code>","text":"<p>Load dataset from JSON string</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>JSON string representing the dataset</p> required <code>dataset_root</code> <code>Optional[Path]</code> <p>Optional root directory to resolve relative paths</p> <code>None</code> <p>Returns:</p> Type Description <code>HDXDataSet</code> <p>HDXDataSet instance.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    json_str: str,\n    dataset_root: Optional[Path] = None,\n) -&gt; HDXDataSet:\n    \"\"\"Load dataset from JSON string\n\n    Args:\n        json_str: JSON string representing the dataset\n        dataset_root: Optional root directory to resolve relative paths\n\n    Returns:\n        HDXDataSet instance.\n\n    \"\"\"\n    if dataset_root is None:\n        context = {}\n    else:\n        context = {\"dataset_root\": dataset_root}\n    return cls.model_validate_json(json_str, context=context)\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.get_state","title":"<code>get_state(state)</code>","text":"<p>Get a specific state by name or index</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def get_state(self, state: str | int) -&gt; State:\n    \"\"\"Get a specific state by name or index\"\"\"\n    if isinstance(state, int):\n        return self.states[state]\n    elif isinstance(state, str):\n        for s in self.states:\n            if s.name == state:\n                return s\n    raise ValueError(f\"State '{state}' not found in dataset.\")\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.validate_file_integrity","title":"<code>validate_file_integrity()</code>","text":"<p>Match hash of files with the stored hash</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def validate_file_integrity(self) -&gt; bool:\n    \"\"\"Match hash of files with the stored hash\"\"\"\n    if self.file_hash is None:\n        return False\n    current_hash = self.hash_files()\n    return current_hash.startswith(self.file_hash)\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.validate_hdx_id","title":"<code>validate_hdx_id()</code>","text":"<p>Validate hdx_id format: 'HDX_' followed by 8 uppercase alphanumeric chars (e.g. HDX_3BAE2080).</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_hdx_id(self):\n    \"\"\"Validate hdx_id format: 'HDX_' followed by 8 uppercase alphanumeric chars (e.g. HDX_3BAE2080).\"\"\"\n    from hdxms_datasets.database import valid_id\n\n    if not valid_id(self.hdx_id):\n        raise ValueError(\n            \"hdx_id must match pattern 'HDX_XXXXXXXX' where X are uppercase letters or digits, e.g. 'HDX_3BAE2080'\"\n        )\n    return self\n</code></pre>"},{"location":"reference/models/#models.HDXDataSet.verify_unique_state_names","title":"<code>verify_unique_state_names()</code>","text":"<p>Ensure that all state names are unique within the dataset</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@model_validator(mode=\"after\")\ndef verify_unique_state_names(self):\n    \"\"\"Ensure that all state names are unique within the dataset\"\"\"\n    state_names = [state.name for state in self.states]\n    if len(state_names) != len(set(state_names)):\n        raise ValueError(\"State names must be unique within the dataset.\")\n    return self\n</code></pre>"},{"location":"reference/models/#models.PeptideFormat","title":"<code>PeptideFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Format of the peptide data</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class PeptideFormat(str, Enum):\n    \"\"\"Format of the peptide data\"\"\"\n\n    DynamX_v3_state = \"DynamX_v3_state\"\n    DynamX_v3_cluster = \"DynamX_v3_cluster\"\n    DynamX_vx_state = \"DynamX_vx_state\"\n    HDExaminer_all_results = \"HDExaminer_all_results\"\n    HDExaminer_all_results_with_units = \"HDExaminer_all_results_with_units\"\n    HDExaminer_peptide_pool = \"HDExaminer_peptide_pool\"\n    HDExaminer_uptake_summary = \"HDExaminer_uptake_summary\"\n    HXMS = \"HXMS\"\n    OpenHDX = \"OpenHDX\"\n\n    def get_format(self) -&gt; type[FormatSpec]:\n        \"\"\"Get the FormatSpec for this format\"\"\"\n        from hdxms_datasets.formats import FMT_REGISTRY\n\n        fmt = FMT_REGISTRY.get(self.value)\n        if fmt is None:\n            raise ValueError(f\"FormatSpec not found for format: {self.value}\")\n        return fmt\n\n    @classmethod\n    def identify(cls, path: Path) -&gt; PeptideFormat | None:\n        \"\"\"Identify format from DataFrame\"\"\"\n        from hdxms_datasets.formats import identify_format\n\n        fmt = identify_format(path)\n        if fmt:\n            return cls(fmt.__name__)\n        return None\n</code></pre>"},{"location":"reference/models/#models.PeptideFormat.get_format","title":"<code>get_format()</code>","text":"<p>Get the FormatSpec for this format</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def get_format(self) -&gt; type[FormatSpec]:\n    \"\"\"Get the FormatSpec for this format\"\"\"\n    from hdxms_datasets.formats import FMT_REGISTRY\n\n    fmt = FMT_REGISTRY.get(self.value)\n    if fmt is None:\n        raise ValueError(f\"FormatSpec not found for format: {self.value}\")\n    return fmt\n</code></pre>"},{"location":"reference/models/#models.PeptideFormat.identify","title":"<code>identify(path)</code>  <code>classmethod</code>","text":"<p>Identify format from DataFrame</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>@classmethod\ndef identify(cls, path: Path) -&gt; PeptideFormat | None:\n    \"\"\"Identify format from DataFrame\"\"\"\n    from hdxms_datasets.formats import identify_format\n\n    fmt = identify_format(path)\n    if fmt:\n        return cls(fmt.__name__)\n    return None\n</code></pre>"},{"location":"reference/models/#models.Peptides","title":"<code>Peptides</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about HDX-MS peptides</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class Peptides(BaseModel):\n    \"\"\"Information about HDX-MS peptides\"\"\"\n\n    data_file: DataFilePath\n    data_format: Annotated[PeptideFormat, Field(description=\"Data format (e.g., OpenHDX)\")]\n    deuteration_type: Annotated[\n        DeuterationType, Field(description=\"Type of the peptide (e.g., fully_deuterated)\")\n    ]\n    filters: Annotated[\n        dict[str, ValueType | list[ValueType]],\n        Field(default_factory=dict, description=\"Filters applied to the data\"),\n        AfterValidator(validate_nonfinite_numbers_recursive),\n        PlainSerializer(serialize_nonfinite_numbers_recursive),\n    ]\n    pH: Annotated[\n        Optional[float], Field(description=\"pH (read, uncorrected) of the experiment\")\n    ] = None\n    temperature: Annotated[Optional[float], Field(description=\"Temperature in Kelvin\")] = None\n    d_percentage: Annotated[Optional[float], Field(description=\"Deuteration percentage\")] = None\n    ionic_strength: Annotated[Optional[float], Field(description=\"Ionic strength in Molar\")] = None\n\n    structure_mapping: Annotated[\n        StructureMapping, Field(description=\"Structure mapping information\")\n    ] = StructureMapping()\n\n    def load(\n        self,\n        convert: bool = True,\n        aggregate: bool | None = None,\n        sort_rows: bool = True,\n        sort_columns: bool = True,\n        drop_null: bool = True,\n    ) -&gt; nw.DataFrame:\n        \"\"\"Load the peptides from the data file\n\n        Args:\n            convert: Whether to convert the data to a standard format.\n            aggregate: Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.\n            sort_rows: Whether to sort the rows.\n            sort_columns: Whether to sort the columns in a standard order.\n            drop_null: Whether to drop columns that are entirely null.\n\n        \"\"\"\n        if self.data_file.exists():\n            from hdxms_datasets.process import load_peptides\n\n            return load_peptides(\n                self,\n                convert=convert,\n                aggregate=aggregate,\n                sort_rows=sort_rows,\n                sort_columns=sort_columns,\n                drop_null=drop_null,\n            )\n        else:\n            raise FileNotFoundError(f\"Data file {self.data_file} does not exist.\")\n</code></pre>"},{"location":"reference/models/#models.Peptides.load","title":"<code>load(convert=True, aggregate=None, sort_rows=True, sort_columns=True, drop_null=True)</code>","text":"<p>Load the peptides from the data file</p> <p>Parameters:</p> Name Type Description Default <code>convert</code> <code>bool</code> <p>Whether to convert the data to a standard format.</p> <code>True</code> <code>aggregate</code> <code>bool | None</code> <p>Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.</p> <code>None</code> <code>sort_rows</code> <code>bool</code> <p>Whether to sort the rows.</p> <code>True</code> <code>sort_columns</code> <code>bool</code> <p>Whether to sort the columns in a standard order.</p> <code>True</code> <code>drop_null</code> <code>bool</code> <p>Whether to drop columns that are entirely null.</p> <code>True</code> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def load(\n    self,\n    convert: bool = True,\n    aggregate: bool | None = None,\n    sort_rows: bool = True,\n    sort_columns: bool = True,\n    drop_null: bool = True,\n) -&gt; nw.DataFrame:\n    \"\"\"Load the peptides from the data file\n\n    Args:\n        convert: Whether to convert the data to a standard format.\n        aggregate: Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.\n        sort_rows: Whether to sort the rows.\n        sort_columns: Whether to sort the columns in a standard order.\n        drop_null: Whether to drop columns that are entirely null.\n\n    \"\"\"\n    if self.data_file.exists():\n        from hdxms_datasets.process import load_peptides\n\n        return load_peptides(\n            self,\n            convert=convert,\n            aggregate=aggregate,\n            sort_rows=sort_rows,\n            sort_columns=sort_columns,\n            drop_null=drop_null,\n        )\n    else:\n        raise FileNotFoundError(f\"Data file {self.data_file} does not exist.\")\n</code></pre>"},{"location":"reference/models/#models.ProteinIdentifiers","title":"<code>ProteinIdentifiers</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>General protein information</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class ProteinIdentifiers(BaseModel):\n    \"\"\"General protein information\"\"\"\n\n    uniprot_accession_number: Annotated[Optional[str], Field(None, description=\"UniProt ID\")] = None\n    uniprot_entry_name: Annotated[Optional[str], Field(None, description=\"UniProt entry name\")] = (\n        None\n    )\n    protein_name: Annotated[Optional[str], Field(None, description=\"Recommended protein name\")] = (\n        None\n    )\n</code></pre>"},{"location":"reference/models/#models.ProteinState","title":"<code>ProteinState</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Protein information for a specific state</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class ProteinState(BaseModel):\n    \"\"\"Protein information for a specific state\"\"\"\n\n    sequence: Annotated[str | list[str], Field(description=\"Amino acid sequence\")]\n    n_term: Annotated[int, Field(description=\"N-terminal residue number\")]\n    c_term: Annotated[int, Field(description=\"C-terminal residue number\")]\n    mutations: Annotated[Optional[list[str]], Field(description=\"List of mutations\")] = None\n    oligomeric_state: Annotated[Optional[int], Field(description=\"Oligomeric state\")] = None\n    ligand: Annotated[Optional[str], Field(description=\"Bound ligand information\")] = None\n\n    @model_validator(mode=\"after\")\n    def check_sequence(self):\n        if len(self.sequence) != self.c_term - self.n_term + 1:\n            raise ValueError(\n                f\"Sequence length does not match N-term and C-term residue numbers, found: {len(self.sequence)}, expected: {self.c_term - self.n_term + 1}\"\n            )\n        return self\n</code></pre>"},{"location":"reference/models/#models.Publication","title":"<code>Publication</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Publication information</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class Publication(BaseModel):\n    \"\"\"Publication information\"\"\"\n\n    title: str\n    authors: Optional[List[str]] = None\n    journal: Optional[str] = None\n    year: Optional[int] = None\n    doi: Optional[str] = None\n    pmid: Optional[str] = None\n    url: Optional[str] = None\n</code></pre>"},{"location":"reference/models/#models.State","title":"<code>State</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about HDX-MS state</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class State(BaseModel):\n    \"\"\"Information about HDX-MS state\"\"\"\n\n    name: Annotated[str, Field(description=\"State name\")]\n    peptides: list[Peptides] = Field(..., description=\"List of peptides in this state\")\n    description: Annotated[str, Field(description=\"State description\")] = \"\"  # TODO max length?\n    protein_state: ProteinState = Field(..., description=\"Protein information for this state\")\n</code></pre>"},{"location":"reference/models/#models.Structure","title":"<code>Structure</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Structural model file information</p> <p>Residues or protein chains may have different numbering/labels depending on if they are the assigned labels by the author of the structure ('auth') or renumbered by the RCSB PDB.</p> <p>If your HDX data uses the author numbering/labels, set <code>auth_residue_numbers</code> and/or <code>auth_chain_labels</code> to True.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class Structure(BaseModel):\n    \"\"\"Structural model file information\n\n    Residues or protein chains may have different numbering/labels depending on if they are\n    the assigned labels by the author of the structure ('auth') or renumbered by the RCSB PDB.\n\n    If your HDX data uses the author numbering/labels, set `auth_residue_numbers` and/or\n    `auth_chain_labels` to True.\n    \"\"\"\n\n    data_file: DataFilePath\n    format: Annotated[str, Field(description=\"Format of the structure file (e.g., PDB, mmCIF)\")]\n    description: Annotated[Optional[str], Field(description=\"Description of the structure\")] = None\n\n    # source database identifiers\n    pdb_id: Annotated[Optional[str], Field(None, description=\"RCSB PDB ID\")] = None\n    alphafold_id: Annotated[Optional[str], Field(None, description=\"AlphaFold ID\")] = None\n\n    def pdbemolstar_custom_data(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Returns a dictionary with custom data for PDBeMolstar visualization.\n        \"\"\"\n\n        if self.format in [\"bcif\"]:\n            binary = True\n        else:\n            binary = False\n\n        if self.data_file.is_file():\n            data = self.data_file.read_bytes()\n        else:\n            raise ValueError(f\"Path {self.data_file} is not a file.\")\n\n        return {\n            \"data\": data,\n            \"format\": self.format,\n            \"binary\": binary,\n        }\n\n    def get_auth_residue_mapping(self) -&gt; dict[tuple[str, str], tuple[str, str]]:\n        \"\"\"Create a mapping from author residue numbers to RCSB residue numbers.\"\"\"\n\n        if self.label_auth_mapping is not None:\n            return self.label_auth_mapping\n\n        else:\n            if self.format.lower() not in [\"cif\", \"mmcif\"]:\n                raise ValueError(\"Author residue number mapping is only supported for mmCIF files.\")\n\n            mapping = residue_number_mapping(self.data_file)\n            self.label_auth_mapping = mapping\n            return mapping\n\n    def to_biopython(self) -&gt; BioStructure:\n        \"\"\"Load the structure using Biopython\"\"\"\n        try:\n            from Bio.PDB.PDBParser import PDBParser\n            from Bio.PDB.MMCIFParser import MMCIFParser\n        except ImportError:\n            raise ImportError(\"Biopython is required to load structures.\")\n\n        if self.format.lower() in [\"pdb\"]:\n            parser = PDBParser(QUIET=True)\n        elif self.format.lower() in [\"cif\", \"mmcif\"]:\n            parser = MMCIFParser(QUIET=True)\n        else:\n            raise ValueError(f\"Unsupported structure format: {self.format}\")\n\n        structure = parser.get_structure(self.pdb_id or \"structure\", self.data_file)\n        assert structure is not None\n\n        return structure\n</code></pre>"},{"location":"reference/models/#models.Structure.get_auth_residue_mapping","title":"<code>get_auth_residue_mapping()</code>","text":"<p>Create a mapping from author residue numbers to RCSB residue numbers.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def get_auth_residue_mapping(self) -&gt; dict[tuple[str, str], tuple[str, str]]:\n    \"\"\"Create a mapping from author residue numbers to RCSB residue numbers.\"\"\"\n\n    if self.label_auth_mapping is not None:\n        return self.label_auth_mapping\n\n    else:\n        if self.format.lower() not in [\"cif\", \"mmcif\"]:\n            raise ValueError(\"Author residue number mapping is only supported for mmCIF files.\")\n\n        mapping = residue_number_mapping(self.data_file)\n        self.label_auth_mapping = mapping\n        return mapping\n</code></pre>"},{"location":"reference/models/#models.Structure.pdbemolstar_custom_data","title":"<code>pdbemolstar_custom_data()</code>","text":"<p>Returns a dictionary with custom data for PDBeMolstar visualization.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def pdbemolstar_custom_data(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Returns a dictionary with custom data for PDBeMolstar visualization.\n    \"\"\"\n\n    if self.format in [\"bcif\"]:\n        binary = True\n    else:\n        binary = False\n\n    if self.data_file.is_file():\n        data = self.data_file.read_bytes()\n    else:\n        raise ValueError(f\"Path {self.data_file} is not a file.\")\n\n    return {\n        \"data\": data,\n        \"format\": self.format,\n        \"binary\": binary,\n    }\n</code></pre>"},{"location":"reference/models/#models.Structure.to_biopython","title":"<code>to_biopython()</code>","text":"<p>Load the structure using Biopython</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def to_biopython(self) -&gt; BioStructure:\n    \"\"\"Load the structure using Biopython\"\"\"\n    try:\n        from Bio.PDB.PDBParser import PDBParser\n        from Bio.PDB.MMCIFParser import MMCIFParser\n    except ImportError:\n        raise ImportError(\"Biopython is required to load structures.\")\n\n    if self.format.lower() in [\"pdb\"]:\n        parser = PDBParser(QUIET=True)\n    elif self.format.lower() in [\"cif\", \"mmcif\"]:\n        parser = MMCIFParser(QUIET=True)\n    else:\n        raise ValueError(f\"Unsupported structure format: {self.format}\")\n\n    structure = parser.get_structure(self.pdb_id or \"structure\", self.data_file)\n    assert structure is not None\n\n    return structure\n</code></pre>"},{"location":"reference/models/#models.StructureMapping","title":"<code>StructureMapping</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Maps peptide HDX-MS data to a structure</p> <p>Residue numbers can be mapped from HDX-MS data to a structure using either an residue number offset or a specific dictionary mapping.</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>class StructureMapping(BaseModel):\n    \"\"\"Maps peptide HDX-MS data to a structure\n\n    Residue numbers can be mapped from HDX-MS data to a structure using either an residue number\n    offset or a specific dictionary mapping.\n\n    \"\"\"\n\n    entity_id: Annotated[Optional[str], Field(None, description=\"Entity identifier\")] = None\n    chain: Annotated[Optional[list[str]], Field(None, description=\"Chain identifiers\")] = None\n    residue_offset: Annotated[int, Field(None, description=\"Residue number offset to apply\")] = 0\n    mapping: dict[int, int] = Field(default_factory=dict, description=\"Residue number mapping\")\n\n    auth_residue_numbers: Annotated[\n        bool, Field(default=False, description=\"Use author residue numbers\")\n    ] = False\n    auth_chain_labels: Annotated[\n        bool, Field(default=False, description=\"Use author chain labels\")\n    ] = False\n\n    def map(self, residue_number: int) -&gt; int:\n        \"\"\"Map a residue number using the mapping dictionary and offset\"\"\"\n        if self.residue_offset:\n            return residue_number + self.residue_offset\n        elif self.mapping:\n            return self.mapping.get(residue_number, residue_number)\n        else:\n            return residue_number\n</code></pre>"},{"location":"reference/models/#models.StructureMapping.map","title":"<code>map(residue_number)</code>","text":"<p>Map a residue number using the mapping dictionary and offset</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def map(self, residue_number: int) -&gt; int:\n    \"\"\"Map a residue number using the mapping dictionary and offset\"\"\"\n    if self.residue_offset:\n        return residue_number + self.residue_offset\n    elif self.mapping:\n        return self.mapping.get(residue_number, residue_number)\n    else:\n        return residue_number\n</code></pre>"},{"location":"reference/models/#models.extract_values_by_types","title":"<code>extract_values_by_types(obj, target_types)</code>","text":"<p>Recursively extract all values of specified type(s) from a nested structure. This function can handle Pydantic models, lists, tuples, sets, and dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Pydantic model instance or any nested structure</p> required <code>target_types</code> <code>Type | tuple[Type, ...]</code> <p>Single type or tuple of types to search for</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>List of all values matching any of the target types</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def extract_values_by_types(obj: Any, target_types: Type | tuple[Type, ...]) -&gt; list[Any]:\n    \"\"\"\n    Recursively extract all values of specified type(s) from a nested structure.\n    This function can handle Pydantic models, lists, tuples, sets, and dictionaries.\n\n    Args:\n        obj: Pydantic model instance or any nested structure\n        target_types: Single type or tuple of types to search for\n\n    Returns:\n        List of all values matching any of the target types\n    \"\"\"\n    values = []\n\n    # Normalize target_types to tuple\n    if not isinstance(target_types, tuple):\n        target_types = (target_types,)\n\n    # Check if current object is of any target type\n    if isinstance(obj, target_types):\n        values.append(obj)\n\n    elif isinstance(obj, BaseModel):\n        # Iterate through all field values in the Pydantic model\n        for field_name, field_value in obj.__dict__.items():\n            values.extend(extract_values_by_types(field_value, target_types))\n\n    elif isinstance(obj, (list, tuple, set)):\n        # Handle sequences\n        for item in obj:\n            values.extend(extract_values_by_types(item, target_types))\n\n    elif isinstance(obj, dict):\n        # Handle dictionaries (both keys and values)\n        for key, value in obj.items():\n            values.extend(extract_values_by_types(key, target_types))\n            values.extend(extract_values_by_types(value, target_types))\n\n    return values\n</code></pre>"},{"location":"reference/models/#models.hash_files","title":"<code>hash_files(data_files)</code>","text":"<p>Compute a hash of all data files in the dataset</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def hash_files(data_files: Iterable[Path]) -&gt; str:\n    \"\"\"Compute a hash of all data files in the dataset\"\"\"\n    hash_obj = hashlib.sha256()\n    files = sorted(data_files, key=lambda p: p.as_posix())  # Sort to ensure consistent order\n    for f in files:\n        if f.suffix in TEXT_FILE_FORMATS:\n            content = f.read_text(encoding=\"utf-8\").replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n            hash_obj.update(content.encode(\"utf-8\"))\n        else:\n            hash_obj.update(f.read_bytes())\n\n    return hash_obj.hexdigest()\n</code></pre>"},{"location":"reference/models/#models.id_factory","title":"<code>id_factory()</code>","text":"<p>Factory function to generate a new dataset ID</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def id_factory() -&gt; str:\n    \"\"\"Factory function to generate a new dataset ID\"\"\"\n    from hdxms_datasets.database import mint_new_dataset_id\n\n    return mint_new_dataset_id()\n</code></pre>"},{"location":"reference/models/#models.residue_number_mapping","title":"<code>residue_number_mapping(cif_path, chain=True, residue=True)</code>","text":"<p>Create a mapping from author residue numbers to RCSB residue numbers from an mmCIF file.</p> <p>Parameters:</p> Name Type Description Default <code>cif_path</code> <code>Path</code> <p>Path to the mmCIF file.</p> required <code>chain</code> <p>Whether to include chain mapping.</p> <code>True</code> <code>residue</code> <p>Whether to include residue number mapping.</p> <code>True</code> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def residue_number_mapping(\n    cif_path: Path, chain=True, residue=True\n) -&gt; dict[tuple[str, str], tuple[str, str]]:\n    \"\"\"Create a mapping from author residue numbers to RCSB residue numbers from an mmCIF file.\n\n    Args:\n        cif_path: Path to the mmCIF file.\n        chain: Whether to include chain mapping.\n        residue: Whether to include residue number mapping.\n\n\n    \"\"\"\n    try:\n        from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n    except ImportError:\n        raise ImportError(\"Biopython is required for residue number mapping from mmCIF files.\")\n\n    mm = MMCIF2Dict(cif_path)\n\n    label_asym = mm[\"_atom_site.label_asym_id\"]\n    if chain:\n        auth_asym = mm.get(\"_atom_site.auth_asym_id\", label_asym)\n    else:\n        auth_asym = label_asym\n\n    label_seq = mm.get(\"_atom_site.label_seq_id\", [])\n    if residue:\n        auth_seq = mm.get(\"_atom_site.auth_seq_id\", label_seq)\n    else:\n        auth_seq = label_seq\n\n    # maps author chain/residue numbers to PDB chain/residue numbers\n    mapping = {\n        (a_asym, a_seq): (l_asym, l_seq)\n        for l_asym, l_seq, a_asym, a_seq in zip(label_asym, label_seq, auth_asym, auth_seq)\n        if l_asym != a_asym or l_seq != a_seq  # dont include identical mappings\n    }\n\n    return mapping\n</code></pre>"},{"location":"reference/models/#models.serialize_datafile_path","title":"<code>serialize_datafile_path(x, info)</code>","text":"<p>Pydantic serializer to convert paths to relative paths based on context</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def serialize_datafile_path(x: Path, info: ValidationInfo) -&gt; str:\n    \"\"\"Pydantic serializer to convert paths to relative paths based on context\"\"\"\n    context = info.context\n    if context and \"dataset_root\" in context:\n        relpath = x.relative_to(Path(context[\"dataset_root\"]))\n        return relpath.as_posix()\n    return x.as_posix()\n</code></pre>"},{"location":"reference/models/#models.validate_datafile_path","title":"<code>validate_datafile_path(x, info)</code>","text":"<p>Pydantic validator to resolve relative paths based on context</p> Source code in <code>hdxms_datasets/models.py</code> <pre><code>def validate_datafile_path(x: Path, info: ValidationInfo):\n    \"\"\"Pydantic validator to resolve relative paths based on context\"\"\"\n    context = info.context\n    if context and \"dataset_root\" in context and not x.is_absolute():\n        root = Path(context[\"dataset_root\"])\n        x = root / x\n    return x\n</code></pre>"},{"location":"reference/plot/","title":"plot","text":""},{"location":"reference/plot/#plot.find_wrap","title":"<code>find_wrap(peptides, margin=4, step=5, wrap_limit=200)</code>","text":"<p>Find the minimum wrap value for a given list of intervals.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>Dataframe with columns 'start' and 'end' representing intervals.</p> required <code>margin</code> <code>int</code> <p>The margin applied to the wrap value. Defaults to 4.</p> <code>4</code> <code>step</code> <code>int</code> <p>The increment step for the wrap value. Defaults to 5.</p> <code>5</code> <code>wrap_limit</code> <code>int</code> <p>The maximum allowed wrap value. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <code>int</code> <p>The minimum wrap value that does not overlap with any intervals.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def find_wrap(\n    peptides: pl.DataFrame,\n    margin: int = 4,\n    step: int = 5,\n    wrap_limit: int = 200,\n) -&gt; int:\n    \"\"\"\n    Find the minimum wrap value for a given list of intervals.\n\n    Args:\n        peptides: Dataframe with columns 'start' and 'end' representing intervals.\n        margin: The margin applied to the wrap value. Defaults to 4.\n        step: The increment step for the wrap value. Defaults to 5.\n        wrap_limit: The maximum allowed wrap value. Defaults to 200.\n\n    Returns:\n        The minimum wrap value that does not overlap with any intervals.\n    \"\"\"\n    wrap = step\n\n    while True:\n        peptides_y = peptides.with_columns(\n            (pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"y\") % wrap)\n        )\n\n        no_overlaps = True\n        for name, df in peptides_y.group_by(\"y\", maintain_order=True):\n            overlaps = (np.array(df[\"end\"]) + 1 + margin)[:-1] &gt;= np.array(df[\"start\"])[1:]\n            if np.any(overlaps):\n                no_overlaps = False\n                break\n                # return wrap\n\n        wrap += step\n        if wrap &gt; wrap_limit:\n            return wrap_limit  # Return the maximum wrap limit if no valid wrap found\n        elif no_overlaps:\n            return wrap\n</code></pre>"},{"location":"reference/plot/#plot.peptide_rectangles","title":"<code>peptide_rectangles(peptides, wrap=None)</code>","text":"<p>Given a DataFrame with 'start' and 'end' columns, each describing a peptide range, this function computes the corresponding rectangle coordinates for visualization.</p> <p>Typicall used for Altair plotting. The rectangles will be stacked vertically based on the <code>wrap</code> parameter. Horizontally, each rectangle spans from <code>start - 0.5</code> to <code>end + 0.5</code>.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information with 'start' and 'end' columns.</p> required <code>wrap</code> <code>int | None</code> <p>The number of peptides to stack vertically before wrapping to the next row.   If <code>None</code>, the function will compute an optimal wrap value to avoid overlaps.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with columns 'x', 'x2', 'y', and 'y2' representing the rectangle coordinates.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def peptide_rectangles(peptides: pl.DataFrame, wrap: int | None = None) -&gt; pl.DataFrame:\n    \"\"\"\n    Given a DataFrame with 'start' and 'end' columns, each describing a peptide range,\n    this function computes the corresponding rectangle coordinates for visualization.\n\n    Typicall used for Altair plotting. The rectangles will be stacked vertically based on the `wrap` parameter.\n    Horizontally, each rectangle spans from `start - 0.5` to `end + 0.5`.\n\n    Args:\n        peptides: DataFrame containing peptide information with 'start' and 'end' columns.\n        wrap: The number of peptides to stack vertically before wrapping to the next row.\n              If `None`, the function will compute an optimal wrap value to avoid overlaps.\n\n    Returns:\n        A DataFrame with columns 'x', 'x2', 'y', and 'y2' representing the rectangle coordinates.\n\n    \"\"\"\n    wrap = find_wrap(peptides, step=1) if wrap is None else wrap\n    columns = [\n        (pl.col(\"start\") - 0.5).alias(\"x\"),\n        (pl.col(\"end\") + 0.5).alias(\"x2\"),\n        (wrap - (pl.col(\"idx\") % wrap)).alias(\"y\"),\n    ]\n\n    rectangles = (\n        peptides[\"start\", \"end\"]\n        .with_row_index(\"idx\")\n        .with_columns(columns)\n        .with_columns((pl.col(\"y\") - 1).alias(\"y2\"))\n    )\n\n    return rectangles\n</code></pre>"},{"location":"reference/plot/#plot.plot_peptides","title":"<code>plot_peptides(peptides, value='value', value_sd=None, colormap='viridis', domain=None, bad_color='#8c8c8c', N=256, label=None, width='container', height=350, wrap=None, fill_nan=True)</code>","text":"<p>Create an altair chart visualizing peptides as colored rectangles.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information with 'start', 'end', and <code>value</code> columns.</p> required <code>value</code> <code>str</code> <p>The column name in <code>peptides</code> to use for coloring the rectangles.</p> <code>'value'</code> <code>value_sd</code> <code>str | None</code> <p>Optional column name for standard deviation of <code>value</code>, used in tooltips.</p> <code>None</code> <code>colormap</code> <code>str | Colormap</code> <p>Colormap to use for coloring the rectangles. Can be a string or a Colormap object.</p> <code>'viridis'</code> <code>domain</code> <code>tuple[float | None, float | None] | None</code> <p>Tuple specifying the (min, max) values for the colormap. If <code>None</code>, uses min and max of <code>value</code>.</p> <code>None</code> <code>bad_color</code> <code>str</code> <p>Color to use for invalid or NaN values.</p> <code>'#8c8c8c'</code> <code>N</code> <code>int</code> <p>Number of discrete colors to generate from the colormap.</p> <code>256</code> <code>label</code> <code>str | None</code> <p>Label for the color legend. If <code>None</code>, uses a title-cased version of <code>value</code>.</p> <code>None</code> <code>width</code> <code>str | int</code> <p>Width of the chart. Can be an integer or 'container' for responsive width.</p> <code>'container'</code> <code>height</code> <code>str | int</code> <p>Height of the chart in pixels.</p> <code>350</code> <code>wrap</code> <code>int | None</code> <p>Number of peptides to stack vertically before wrapping to the next row. If <code>None</code>, computes an optimal wrap value.</p> <code>None</code> <code>fill_nan</code> <code>bool</code> <p>Whether to fill NaN values in <code>peptides</code> with None to avoid serialization issues.</p> <code>True</code> <p>Returns:</p> Type Description <code>Chart</code> <p>An Altair Chart object visualizing the peptides.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def plot_peptides(\n    peptides: pl.DataFrame,\n    value: str = \"value\",\n    value_sd: str | None = None,\n    colormap: str | Colormap = \"viridis\",\n    domain: tuple[float | None, float | None] | None = None,\n    bad_color: str = \"#8c8c8c\",\n    N: int = 256,\n    label: str | None = None,\n    width: str | int = \"container\",\n    height: str | int = 350,\n    wrap: int | None = None,\n    fill_nan: bool = True,\n) -&gt; alt.Chart:\n    \"\"\"\n    Create an altair chart visualizing peptides as colored rectangles.\n\n    Args:\n        peptides: DataFrame containing peptide information with 'start', 'end', and `value` columns.\n        value: The column name in `peptides` to use for coloring the rectangles.\n        value_sd: Optional column name for standard deviation of `value`, used in tooltips.\n        colormap: Colormap to use for coloring the rectangles. Can be a string or a Colormap object.\n        domain: Tuple specifying the (min, max) values for the colormap. If `None`, uses min and max of `value`.\n        bad_color: Color to use for invalid or NaN values.\n        N: Number of discrete colors to generate from the colormap.\n        label: Label for the color legend. If `None`, uses a title-cased version of `value`.\n        width: Width of the chart. Can be an integer or 'container' for responsive width.\n        height: Height of the chart in pixels.\n        wrap: Number of peptides to stack vertically before wrapping to the next row. If `None`, computes an optimal wrap value.\n        fill_nan: Whether to fill NaN values in `peptides` with None to avoid serialization issues.\n\n    Returns:\n        An Altair Chart object visualizing the peptides.\n\n    \"\"\"\n\n    if not unique_peptides(peptides):\n        raise ValueError(\"Peptides must be unique by 'start' and 'end' columns.\")\n\n    if fill_nan:\n        # nan values can cause problems in serialization\n        peptides = peptides.fill_nan(None)\n\n    value_sd = value_sd or f\"{value}_sd\"\n    colormap = Colormap(colormap) if isinstance(colormap, str) else colormap\n    if domain is None:\n        domain = (None, None)\n    vmin = domain[0] if domain[0] is not None else peptides[value].min()  # type: ignore\n    vmax = domain[1] if domain[1] is not None else peptides[value].max()  # type: ignore\n\n    scale = alt.Scale(domain=(vmin, vmax), range=colormap.to_altair(N=N))  # type: ignore\n    label = label or value.replace(\"_\", \" \").title()\n\n    if value_sd in peptides.columns:\n        tooltip_value = []\n        for v, v_sd in zip(peptides[value], peptides[value_sd]):\n            if v is not None and v_sd is not None:\n                tooltip_value.append(\n                    f\"{v:.2f} \\u00b1 {v_sd:.2f}\"  # type: ignore\n                )\n            else:\n                tooltip_value.append(\"NaN\")\n    else:\n        tooltip_value = [f\"{value:.2f}\" if value is not None else \"\" for value in peptides[value]]\n\n    rectangles = peptide_rectangles(peptides, wrap=wrap)\n    peptide_source = peptides.join(rectangles, on=[\"start\", \"end\"], how=\"left\").with_columns(\n        pl.col(value), pl.Series(tooltip_value).alias(\"tooltip_value\")\n    )\n\n    invalid = {\"color\": {\"value\": bad_color}}\n    peptide_chart = (\n        alt.Chart(peptide_source)\n        .mark_rect(\n            stroke=\"black\",\n        )\n        .encode(\n            x=alt.X(\"x:Q\", title=\"Residue Number\"),\n            y=alt.Y(\"y:Q\", title=\"\", axis=alt.Axis(ticks=False, domain=False, labels=False)),\n            x2=alt.X2(\"x2:Q\"),\n            y2=alt.Y2(\"y2:Q\"),\n            tooltip=[\n                alt.Tooltip(\"idx:Q\", title=\"Index\"),\n                alt.Tooltip(\"start:Q\", title=\"Start\"),\n                alt.Tooltip(\"end:Q\", title=\"End\"),\n                alt.Tooltip(\"sequence:N\", title=\"Sequence\"),\n                alt.Tooltip(\"tooltip_value:N\", title=label),\n            ],\n            color=alt.Color(f\"{value}:Q\", scale=scale, title=label),\n        )\n        .configure_scale(invalid=invalid)\n    )\n\n    return peptide_chart.properties(height=height, width=width)\n</code></pre>"},{"location":"reference/plot/#plot.unique_peptides","title":"<code>unique_peptides(df)</code>","text":"<p>Checks if all peptides in the DataFrame are unique. Needs to have columns 'start' and 'end' marking peptide intervals (inclusive).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing peptide information.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all peptides are unique, otherwise <code>False</code>.</p> Source code in <code>hdxms_datasets/plot.py</code> <pre><code>def unique_peptides(df: pl.DataFrame) -&gt; bool:\n    \"\"\"\n    Checks if all peptides in the DataFrame are unique.\n    Needs to have columns 'start' and 'end' marking peptide intervals (inclusive).\n\n    Args:\n        df: DataFrame containing peptide information.\n\n    Returns:\n        `True` if all peptides are unique, otherwise `False`.\n\n    \"\"\"\n\n    return len(df) == len(df.unique(subset=[\"start\", \"end\"]))\n</code></pre>"},{"location":"reference/process/","title":"process","text":""},{"location":"reference/process/#process.aggregate","title":"<code>aggregate(df)</code>","text":"<p>Aggregate replicates by intensity-weighted average. Columns which are intensity-weighted averaged are: uptake, centroid_mz, centroid_mass, rt, if present.</p> <p>If no intensity column is present, replicates are averaged with equal weights. All other columns are pass through if they are unique, otherwise set to <code>None</code>. Also adds n_replicates, n_charges, and n_clusters columns.</p> <pre><code>n_replicates: Number of replicates averaged, based on the unique number of values in\n    the 'replicate' column\nn_charges: Number of unique charged states averaged together\nn_clusters: Total number of isotopic clusters averaged together regardless of whether\n    they are from replicate experiments or different charged states.\n</code></pre> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def aggregate(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Aggregate replicates by intensity-weighted average.\n    Columns which are intensity-weighted averaged are: uptake, centroid_mz, centroid_mass, rt, if present.\n\n    If no intensity column is present, replicates are averaged with equal weights.\n    All other columns are pass through if they are unique, otherwise set to `None`.\n    Also adds n_replicates, n_charges, and n_clusters columns.\n\n        n_replicates: Number of replicates averaged, based on the unique number of values in\n            the 'replicate' column\n        n_charges: Number of unique charged states averaged together\n        n_clusters: Total number of isotopic clusters averaged together regardless of whether\n            they are from replicate experiments or different charged states.\n\n    \"\"\"\n\n    # group by these columns if present\n    by = [\"protein\", \"state\", \"start\", \"end\", \"exposure\"]\n    group_by_columns = [col for col in by if col in df.columns]\n\n    # these must be unique before aggregating makes sense\n    # TODO: we can group also by these columns to avoid this requirement\n    # unique_columns = [\"protein\", \"state\"]\n    # for col in unique_columns:\n    #     if col in df.columns:\n    #         assert df[col].n_unique() == 1, f\"Column {col} must be unique before aggregating.\"\n\n    # columns which are intesity weighed averaged\n    # TODO global variable\n    candidate_columns = [\"uptake\", \"centroid_mz\", \"centroid_mass\", \"rt\"]\n    intensity_wt_avg_columns = [col for col in candidate_columns if col in df.columns]\n\n    if \"intensity\" not in df.columns:\n        df = df.with_columns(nw.lit(1.0).alias(\"intensity\"))\n\n    output_columns = df.columns[:]\n\n    for col in intensity_wt_avg_columns:\n        col_idx = output_columns.index(col)\n        output_columns.insert(col_idx + 1, f\"{col}_sd\")\n\n    if \"replicate\" in df.columns:\n        output_columns += [\"n_replicates\"]\n    if \"charge\" in df.columns:\n        output_columns += [\"n_charges\"]\n    output_columns += [\"n_clusters\"]\n\n    excluded = {\"intensity\"}\n    output = {k: [] for k in output_columns if k not in excluded}\n    groups = df.group_by(group_by_columns)\n\n    # TODO: if we don't have an intensity column, we can do a normal aggregate\n    # instead of needing a for loop\n    for group_values, df_group in groups:\n        record = {col: val for col, val in zip(group_by_columns, group_values)}\n        # record[\"start\"] = start\n        # record[\"end\"] = end\n        # record[\"exposure\"] = exposure\n        if \"charge\" in df.columns:\n            record[\"n_charges\"] = df_group[\"charge\"].n_unique()\n        if \"replicate\" in df.columns:\n            record[\"n_replicates\"] = df_group[\"replicate\"].n_unique()\n        record[\"n_clusters\"] = len(df_group)\n\n        # add intensity-weighted average columns\n        for col in intensity_wt_avg_columns:\n            val = ufloat_stats(df_group[col], df_group[\"intensity\"])\n            record[col] = val.nominal_value\n            record[f\"{col}_sd\"] = val.std_dev\n\n        # add other columns, taking the first value if unique, otherwise None\n        other_columns = set(df.columns) - record.keys() - excluded\n        for col in other_columns:\n            if df_group[col].n_unique() == 1:\n                record[col] = df_group[col][0]\n            else:\n                record[col] = None\n\n        # add record to output\n        assert output.keys() == record.keys()\n        for k in record:\n            output[k].append(record[k])\n\n    agg_df = nw.from_dict(output, backend=BACKEND)\n\n    return agg_df\n</code></pre>"},{"location":"reference/process/#process.aggregate_columns","title":"<code>aggregate_columns(df, columns, by=['start', 'end', 'exposure'])</code>","text":"<p>Aggregate the specified columns by intensity-weighted average. The dataframe must have a column named 'intensity' for weighting.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to aggregate.</p> required <code>columns</code> <code>list[str]</code> <p>List of columns to aggregate.</p> required <code>by</code> <code>list[str]</code> <p>List of columns to group by.</p> <code>['start', 'end', 'exposure']</code> Source code in <code>hdxms_datasets/process.py</code> <pre><code>@nw.narwhalify\ndef aggregate_columns(\n    df: nw.DataFrame, columns: list[str], by: list[str] = [\"start\", \"end\", \"exposure\"]\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Aggregate the specified columns by intensity-weighted average.\n    The dataframe must have a column named 'intensity' for weighting.\n\n    Args:\n        df: DataFrame to aggregate.\n        columns: List of columns to aggregate.\n        by: List of columns to group by.\n\n    \"\"\"\n    groups = df.group_by(by)\n    output = {k: [] for k in by}\n    for col in columns:\n        output[col] = []\n        output[f\"{col}_sd\"] = []\n\n    for (start, end, exposure), df_group in groups:\n        output[\"start\"].append(start)\n        output[\"end\"].append(end)\n        output[\"exposure\"].append(exposure)\n\n        for col in columns:\n            val = ufloat_stats(df_group[col], df_group[\"intensity\"])\n            output[col].append(val.nominal_value)\n            output[f\"{col}_sd\"].append(val.std_dev)\n\n    agg_df = nw.from_dict(output, backend=BACKEND)\n    return agg_df\n</code></pre>"},{"location":"reference/process/#process.apply_filters","title":"<code>apply_filters(df, **filters)</code>","text":"<p>Apply filters to the DataFrame based on the provided keyword arguments. Each keyword corresponds to a column name, and the value can be a single value or a list of values.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to filter.</p> required <code>**filters</code> <code>ValueType | list[ValueType]</code> <p>Column-value pairs to filter the DataFrame.</p> <code>{}</code> <p>Returns:     Filtered DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def apply_filters(df: nw.DataFrame, **filters: ValueType | list[ValueType]) -&gt; nw.DataFrame:\n    \"\"\"\n    Apply filters to the DataFrame based on the provided keyword arguments.\n    Each keyword corresponds to a column name, and the value can be a single value or a list of values.\n\n    Args:\n        df: The DataFrame to filter.\n        **filters: Column-value pairs to filter the DataFrame.\n    Returns:\n        Filtered DataFrame.\n    \"\"\"\n    exprs = []\n    for col, val in filters.items():\n        if isinstance(val, list):\n            expr = nw.col(col).is_in(val)\n        else:\n            expr = nw.col(col) == val\n        exprs.append(expr)\n    if not exprs:\n        return df\n    f_expr = reduce(and_, exprs)\n    return df.filter(f_expr)\n</code></pre>"},{"location":"reference/process/#process.compute_uptake_metrics","title":"<code>compute_uptake_metrics(df, exception='ignore')</code>","text":"<p>Tries to add columns to computed from other columns the DataFrame. Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to add columns to.</p> required <code>exception</code> <code>Literal['raise', 'warn', 'ignore']</code> <p>How to handle exceptions when adding columns. Options are 'raise', 'warn', 'ignore'.</p> <code>'ignore'</code> <p>Returns:     DataFrame with added columns.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def compute_uptake_metrics(\n    df: nw.DataFrame, exception: Literal[\"raise\", \"warn\", \"ignore\"] = \"ignore\"\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Tries to add columns to computed from other columns the DataFrame.\n    Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.\n\n    Args:\n        df: DataFrame to add columns to.\n        exception: How to handle exceptions when adding columns. Options are 'raise', 'warn', 'ignore'.\n    Returns:\n        DataFrame with added columns.\n\n    \"\"\"\n    all_columns = {\n        \"max_uptake\": hdx_expr.max_uptake,\n        \"uptake\": hdx_expr.uptake,\n        \"uptake_sd\": hdx_expr.uptake_sd,\n        \"fd_uptake\": hdx_expr.fd_uptake,\n        \"fd_uptake_sd\": hdx_expr.fd_uptake_sd,\n        \"frac_fd_control\": hdx_expr.frac_fd_control,\n        \"frac_fd_control_sd\": hdx_expr.frac_fd_control_sd,\n        \"frac_max_uptake\": hdx_expr.frac_max_uptake,\n        \"frac_max_uptake_sd\": hdx_expr.frac_max_uptake_sd,\n    }\n\n    for col, expr in all_columns.items():\n        if col not in df.columns:\n            try:\n                df = df.with_columns(expr)\n            except Exception as e:\n                if exception == \"raise\":\n                    raise e\n                elif exception == \"warn\":\n                    warnings.warn(f\"Failed to add column {col}: {e}\")\n                elif exception == \"ignore\":\n                    pass\n                else:\n                    raise ValueError(\"Invalid exception handling option\")\n\n    return df\n</code></pre>"},{"location":"reference/process/#process.drop_null_columns","title":"<code>drop_null_columns(df)</code>","text":"<p>Drop columns that are all null from the DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def drop_null_columns(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Drop columns that are all null from the DataFrame.\"\"\"\n    all_null_columns = [col for col in df.columns if df[col].is_null().all()]\n    return df.drop(all_null_columns)\n</code></pre>"},{"location":"reference/process/#process.dynamx_cluster_to_state","title":"<code>dynamx_cluster_to_state(cluster_data, nd_exposure=0.0)</code>","text":"<p>Convert dynamx cluster data to state data. Must contain only a single state.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_data</code> <code>DataFrame</code> <p>DataFrame containing dynamx cluster data.</p> required <code>nd_exposure</code> <code>float</code> <p>Exposure time for non-deuterated control.</p> <code>0.0</code> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def dynamx_cluster_to_state(cluster_data: nw.DataFrame, nd_exposure: float = 0.0) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert dynamx cluster data to state data.\n    Must contain only a single state.\n\n    Args:\n        cluster_data: DataFrame containing dynamx cluster data.\n        nd_exposure: Exposure time for non-deuterated control.\n\n    \"\"\"\n\n    assert len(cluster_data[\"state\"].unique()) == 1, \"Multiple states found in data\"\n\n    # determine undeuterated masses per peptide\n    nd_data = cluster_data.filter(nw.col(\"exposure\") == nd_exposure)\n    nd_peptides: list[tuple[int, int]] = sorted(\n        {(start, end) for start, end in zip(nd_data[\"start\"], nd_data[\"end\"])}\n    )\n\n    # create a dict of non-deuterated masses\n    peptides_nd_mass = {}\n    for p in nd_peptides:\n        start, end = p\n        df_nd_peptide = nd_data.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end))\n\n        masses = df_nd_peptide[\"z\"] * (df_nd_peptide[\"center\"] - PROTON_MASS)\n        nd_mass = ufloat_stats(masses, df_nd_peptide[\"inten\"])\n\n        peptides_nd_mass[p] = nd_mass\n\n    groups = cluster_data.group_by([\"start\", \"end\", \"exposure\"])\n    unique_columns = [\n        \"end\",\n        \"exposure\",\n        \"fragment\",\n        \"maxuptake\",\n        \"mhp\",\n        \"modification\",\n        \"protein\",\n        \"sequence\",\n        \"start\",\n        \"state\",\n        \"stop\",\n    ]\n\n    # Determine uptake and uptake_sd for each peptide/exposure by\n    # subtracting the non-deuterated mass from the observed mass\n    records = []\n    for (start, end, exposure), df_group in groups:\n        record = {col: df_group[col][0] for col in unique_columns}\n\n        rt = ufloat_stats(df_group[\"rt\"], df_group[\"inten\"])\n        record[\"rt\"] = rt.nominal_value\n        record[\"rt_sd\"] = rt.std_dev\n\n        # state data 'center' is mass as if |charge| would be 1\n        center = ufloat_stats(\n            df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS) + PROTON_MASS, df_group[\"inten\"]\n        )\n        record[\"center\"] = center.nominal_value\n        record[\"center_sd\"] = center.std_dev\n\n        masses = df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS)\n        exp_mass = ufloat_stats(masses, df_group[\"inten\"])\n\n        if (start, end) in peptides_nd_mass:\n            uptake = exp_mass - peptides_nd_mass[(start, end)]\n            record[\"uptake\"] = uptake.nominal_value\n            record[\"uptake_sd\"] = uptake.std_dev\n        else:\n            record[\"uptake\"] = None\n            record[\"uptake_sd\"] = None\n\n        records.append(record)\n\n    d = records_to_dict(records)\n    df = nw.from_dict(d, backend=BACKEND)\n\n    if set(df.columns) == set(STATE_DATA_COLUMN_ORDER):\n        df = df[STATE_DATA_COLUMN_ORDER]\n\n    return df\n</code></pre>"},{"location":"reference/process/#process.left_join","title":"<code>left_join(df_left, df_right, select_columns, prefix, include_sd=True)</code>","text":"<p>Left join two DataFrames on start, end, selecting   and the specified column.</p> <p>Parameters:</p> Name Type Description Default <code>df_left</code> <code>DataFrame</code> <p>Left DataFrame.</p> required <code>df_right</code> <code>DataFrame</code> <p>Right DataFrame.</p> required <code>select_columns</code> <code>list[str]</code> <p>Column names to select from the right dataframe.</p> required <code>prefix</code> <code>str</code> <p>Prefix to add to the joined columns from the right DataFrame.</p> required <code>include_sd</code> <code>bool</code> <p>Whether to include the standard deviation column (column_sd) from the right DataFrame, if available</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Merged DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def left_join(\n    df_left: nw.DataFrame,\n    df_right: nw.DataFrame,\n    select_columns: list[str],\n    prefix: str,\n    include_sd: bool = True,\n) -&gt; nw.DataFrame:\n    \"\"\"Left join two DataFrames on start, end, selecting\n      and the specified column.\n\n    Args:\n        df_left: Left DataFrame.\n        df_right: Right DataFrame.\n        select_columns: Column names to select from the right dataframe.\n        prefix: Prefix to add to the joined columns from the right DataFrame.\n        include_sd: Whether to include the standard deviation column (column_sd) from the right DataFrame, if available\n\n    Returns:\n        Merged DataFrame.\n\n    \"\"\"\n    select = [nw.col(\"start\"), nw.col(\"end\")]\n    for column in select_columns:\n        select.append(nw.col(column).alias(f\"{prefix}_{column}\"))\n        if include_sd and f\"{column}_sd\" in df_right.columns:\n            select.append(nw.col(f\"{column}_sd\").alias(f\"{prefix}_{column}_sd\"))\n\n    merge = df_left.join(\n        df_right.select(select),\n        on=[\"start\", \"end\"],\n        how=\"left\",  # 'left' join ensures all rows from pd_peptides are kept\n    )\n\n    return merge\n</code></pre>"},{"location":"reference/process/#process.load_peptides","title":"<code>load_peptides(peptides, base_dir=Path.cwd(), convert=True, aggregate=None, sort_rows=True, sort_columns=True, drop_null=True)</code>","text":"<p>Load peptides from the data file and return a Narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>Peptides</code> <p>Peptides object containing metadata and file path.</p> required <code>base_dir</code> <code>Path</code> <p>Base directory to resolve relative file paths. Defaults to the current working directory.</p> <code>cwd()</code> <code>convert</code> <code>bool</code> <p>Whether to convert the data to a standard format.</p> <code>True</code> <code>aggregate</code> <code>bool | None</code> <p>Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.</p> <code>None</code> <code>sort_rows</code> <code>bool</code> <p>Whether to sort the rows.</p> <code>True</code> <code>sort_columns</code> <code>bool</code> <p>Whether to sort the columns in a standard order.</p> <code>True</code> <code>drop_null</code> <code>bool</code> <p>Whether to drop columns that are entirely null.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Narwhals DataFrame containing the loaded peptide data.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def load_peptides(\n    peptides: Peptides,\n    base_dir: Path = Path.cwd(),\n    convert: bool = True,\n    aggregate: bool | None = None,\n    sort_rows: bool = True,\n    sort_columns: bool = True,\n    drop_null: bool = True,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Load peptides from the data file and return a Narwhals DataFrame.\n\n    Args:\n        peptides: Peptides object containing metadata and file path.\n        base_dir: Base directory to resolve relative file paths. Defaults to the current working directory.\n        convert: Whether to convert the data to a standard format.\n        aggregate: Whether to aggregate the data. If None, will aggregate if the data is not already aggregated.\n        sort_rows: Whether to sort the rows.\n        sort_columns: Whether to sort the columns in a standard order.\n        drop_null: Whether to drop columns that are entirely null.\n\n    Returns:\n        A Narwhals DataFrame containing the loaded peptide data.\n\n    \"\"\"\n\n    # Resolve the data file path\n    if peptides.data_file.is_absolute():\n        data_path = peptides.data_file\n    else:\n        data_path = base_dir / peptides.data_file\n\n    from hdxms_datasets.formats import FMT_REGISTRY, is_aggregated\n\n    format_spec = FMT_REGISTRY.get(peptides.data_format)\n    assert format_spec is not None, f\"Unknown format: {peptides.data_format}\"\n\n    df = format_spec.read(data_path)\n\n    from hdxms_datasets import process\n\n    df = process.apply_filters(df, **peptides.filters)\n\n    if not convert and sort_rows:\n        warnings.warn(\"Cannot sort rows without conversion. Sorting will be skipped.\")\n        sort_rows = False\n\n    if not convert and sort_columns:\n        warnings.warn(\"Cannot sort columns without conversion. Sorting will be skipped.\")\n        sort_columns = False\n\n    if convert:\n        df = format_spec.convert(df)\n\n    peptides_are_aggregated = format_spec.aggregated or is_aggregated(df)\n\n    if callable(format_spec.aggregated):\n        peptides_are_aggregated = format_spec.aggregated(df)\n    else:\n        peptides_are_aggregated = format_spec.aggregated\n\n    # if aggregation is not specified, by default aggregate if the data is not already aggregated\n    if aggregate is None:\n        aggregate = not peptides_are_aggregated\n\n    if aggregate and peptides_are_aggregated:\n        warnings.warn(\"Data format is pre-aggregated. Aggregation will be skipped.\")\n        aggregate = False\n\n    if not convert and aggregate:\n        warnings.warn(\"Cannot aggregate data without conversion. Aggregation will be skipped.\")\n        aggregate = False\n\n    if aggregate:\n        df = process.aggregate(df)\n\n    if drop_null:\n        df = process.drop_null_columns(df)\n\n    if sort_rows:\n        df = process.sort_rows(df)\n\n    if sort_columns:\n        df = process.sort_columns(df)\n\n    return df\n</code></pre>"},{"location":"reference/process/#process.merge_peptide_tables","title":"<code>merge_peptide_tables(partially_deuterated, non_deuterated=None, fully_deuterated=None, select_columns=None)</code>","text":"<p>Merges peptide tables from different deuteration types into a single DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>partially_deuterated</code> <code>DataFrame</code> <p>DataFrame containing partially deuterated peptides. Must be provided.</p> required <code>select_columns</code> <code>Optional[list[str]]</code> <p>Column names to select from the controls. If None, 'centroid_mass' and'uptake' are used, if present</p> <code>None</code> <code>non_deuterated</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame containing non-deuterated peptides.</p> <code>None</code> <code>fully_deuterated</code> <code>Optional[DataFrame]</code> <p>Optional DataFrame containing fully deuterated peptides.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Merged DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def merge_peptide_tables(\n    partially_deuterated: nw.DataFrame,\n    non_deuterated: Optional[nw.DataFrame] = None,\n    fully_deuterated: Optional[nw.DataFrame] = None,\n    select_columns: Optional[list[str]] = None,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Merges peptide tables from different deuteration types into a single DataFrame.\n\n    Args:\n        partially_deuterated: DataFrame containing partially deuterated peptides. Must be provided.\n        select_columns: Column names to select from the controls. If None, 'centroid_mass' and'uptake' are used, if present\n        non_deuterated: Optional DataFrame containing non-deuterated peptides.\n        fully_deuterated: Optional DataFrame containing fully deuterated peptides.\n\n    Returns:\n        Merged DataFrame.\n\n    \"\"\"\n\n    available_controls = [\n        (prefix, df)\n        for prefix, df in [(\"nd\", non_deuterated), (\"fd\", fully_deuterated)]\n        if df is not None\n    ]\n    if len(available_controls) == 0:\n        raise ValueError(\n            \"At least one control (non_deuterated or fully_deuterated) must be provided.\"\n        )\n    common_columns = reduce(set.intersection, (set(df.columns) for _, df in available_controls))\n\n    if select_columns is None:\n        candidates = [\"centroid_mass\", \"uptake\"]\n        select_columns = [col for col in candidates if col in common_columns]\n\n    output = partially_deuterated\n    _names = {\"fd\": \"Fully Deuterated\", \"nd\": \"Non Deuterated\"}\n    for prefix, df in available_controls:\n        assert peptides_are_unique(df), f\"{_names[prefix]} peptides must be unique.\"\n        output = left_join(output, df, select_columns=select_columns, prefix=prefix)\n\n    return output\n</code></pre>"},{"location":"reference/process/#process.merge_peptides","title":"<code>merge_peptides(peptides, base_dir=Path.cwd())</code>","text":"<p>Merge peptide tables from different deuteration types into a single DataFrame. This function is used to match control measurements to a set of partially deuterated peptides.</p> <p>Supports non-deuterated (nd) and fully deuterated peptides (fd) as controls. The column used in the merge is 'centroid_mass' if present, otherwise 'uptake'. Merged columns are prefixed with 'nd_' or 'fd_'.</p> When to use merge_peptide_tables vs left_join <ul> <li>Use <code>merge_peptide_tables</code> to merge already loaded peptide dataframes.</li> <li>Use <code>left_join</code> to merge peptide dataframes with other controls / data types.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>list[Peptides]</code> <p>List of Peptides objects to merge. Must contain one partially deuterated peptide.</p> required <code>base_dir</code> <code>Path</code> <p>Base directory to resolve relative paths in Peptides data_file.</p> <code>cwd()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Merged DataFrame.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def merge_peptides(peptides: list[Peptides], base_dir: Path = Path.cwd()) -&gt; nw.DataFrame:\n    \"\"\"Merge peptide tables from different deuteration types into a single DataFrame.\n    This function is used to match control measurements to a set of partially deuterated peptides.\n\n    Supports non-deuterated (nd) and fully deuterated peptides (fd) as controls.\n    The column used in the merge is 'centroid_mass' if present, otherwise 'uptake'. Merged columns are prefixed\n    with 'nd_' or 'fd_'.\n\n    ??? tip \"When to use merge_peptide_tables vs left_join\"\n        - Use `merge_peptide_tables` to merge already loaded peptide dataframes.\n        - Use `left_join` to merge peptide dataframes with other controls / data types.\n\n    Args:\n        peptides: List of Peptides objects to merge. Must contain one partially deuterated peptide.\n        base_dir: Base directory to resolve relative paths in Peptides data_file.\n\n    Returns:\n        Merged DataFrame.\n\n    \"\"\"\n    peptide_types = {p.deuteration_type for p in peptides}\n    if not peptides:\n        raise ValueError(\"No peptides provided for merging.\")\n\n    if len(peptide_types) != len(peptides):\n        raise ValueError(\n            \"Multiple peptides of the same type found. Please ensure unique deuteration types.\"\n        )\n\n    if DeuterationType.partially_deuterated not in peptide_types:\n        raise ValueError(\"Partially deuterated peptide is required for uptake metrics calculation.\")\n\n    loaded_peptides = {\n        p.deuteration_type.value: load_peptides(p, base_dir=base_dir) for p in peptides\n    }\n\n    merged = merge_peptide_tables(**loaded_peptides, select_columns=None)\n    return merged\n</code></pre>"},{"location":"reference/process/#process.sort_columns","title":"<code>sort_columns(df, columns=OPEN_HDX_COLUMNS)</code>","text":"<p>Sorts the DataFrame columns to match the specified order.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to sort.</p> required <code>columns</code> <code>list[str]</code> <p>List of columns in the desired order. Columns not in this list will be placed at the end.</p> <code>OPEN_HDX_COLUMNS</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns sorted.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def sort_columns(df: nw.DataFrame, columns: list[str] = OPEN_HDX_COLUMNS) -&gt; nw.DataFrame:\n    \"\"\"Sorts the DataFrame columns to match the specified order.\n\n    Args:\n        df: DataFrame to sort.\n        columns: List of columns in the desired order. Columns not in this list will be placed at the end.\n\n    Returns:\n        DataFrame with columns sorted.\n\n    \"\"\"\n    matching_columns = [col for col in columns if col in df.columns]\n    other_columns = [col for col in df.columns if col not in matching_columns]\n\n    assert set(df.columns) == set(matching_columns + other_columns)\n\n    return df[matching_columns + other_columns]\n</code></pre>"},{"location":"reference/process/#process.sort_rows","title":"<code>sort_rows(df)</code>","text":"<p>Sorts the DataFrame by state, exposure, start, end, file.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def sort_rows(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Sorts the DataFrame by state, exposure, start, end, file.\"\"\"\n    all_by = [\"state\", \"exposure\", \"start\", \"end\", \"replicate\"]\n    by = [col for col in all_by if col in df.columns]\n    return df.sort(by=by)\n</code></pre>"},{"location":"reference/process/#process.ufloat_stats","title":"<code>ufloat_stats(array, weights)</code>","text":"<p>Calculate the weighted mean and standard deviation.</p> Source code in <code>hdxms_datasets/process.py</code> <pre><code>def ufloat_stats(array, weights) -&gt; Variable:\n    \"\"\"Calculate the weighted mean and standard deviation.\"\"\"\n    weighted_stats = DescrStatsW(array, weights=weights, ddof=0)\n    return ufloat(weighted_stats.mean, weighted_stats.std)\n</code></pre>"},{"location":"reference/reader/","title":"reader","text":"<p>Module for loading various HDX-MS formats.</p>"},{"location":"reference/reader/#reader.deduplicate_name","title":"<code>deduplicate_name(name)</code>","text":"<p>Deduplicate column name by removing trailing '_duplicated_xx</p> Source code in <code>hdxms_datasets/reader.py</code> <pre><code>def deduplicate_name(name: str):\n    \"\"\"Deduplicate column name by removing trailing '_duplicated_xx\"\"\"\n    import re\n\n    return re.sub(r\"_duplicated_\\d+$\", \"\", name)\n</code></pre>"},{"location":"reference/reader/#reader.get_backend","title":"<code>get_backend()</code>","text":"<p>Returns the backend used for data handling.</p> Source code in <code>hdxms_datasets/reader.py</code> <pre><code>def get_backend():\n    \"\"\"\n    Returns the backend used for data handling.\n    \"\"\"\n    try:\n        import polars  # NOQA: F401 # type: ignore[import]\n\n        return \"polars\"\n    except ImportError:\n        pass\n\n    try:\n        import pandas  # NOQA: F401 # type: ignore[import]\n\n        return \"pandas\"\n    except ImportError:\n        pass\n\n    try:\n        import modin  # NOQA: F401 # type: ignore[import]\n\n        return \"modin\"\n    except ImportError:\n        pass\n\n    try:\n        import pyarrow  # NOQA: F401 # type: ignore[import]\n\n        return \"pyarrow\"\n    except ImportError:\n        pass\n\n    raise ImportError(\"No suitable backend found. Please install pandas, polars, pyarrow or modin.\")\n</code></pre>"},{"location":"reference/reader/#reader.hxms_line_generator","title":"<code>hxms_line_generator(source)</code>","text":"<p>Generate lines from an HXMS file.</p> Source code in <code>hdxms_datasets/reader.py</code> <pre><code>def hxms_line_generator(source: Path) -&gt; Iterator[str]:\n    \"\"\"\n    Generate lines from an HXMS file.\n    \"\"\"\n    with source.open(\"r\", encoding=\"utf-8\") as fh:\n        for line in fh:\n            yield line.rstrip(\"\\r\\n\")\n</code></pre>"},{"location":"reference/reader/#reader.parse_hxms_lines","title":"<code>parse_hxms_lines(lines, read_content=True)</code>","text":"<p>Parse the different sections of an HXMS file.</p> Returns a dictionary with keys <ul> <li>\"HEADER\": list of header lines</li> <li>\"METADATA\": dict of metadata key-value pairs</li> <li>\"REMARK\": dict of remark key-value pairs</li> <li>\"DATA\": Narwhals DataFrame containing the HXMS data (if read_content is True)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>lines</code> <code>Iterable[str]</code> <p>An iterable of lines from the HXMS file.</p> required <p>Returns:</p> Type Description <code>HXMSResult</code> <p>A dictionary containing the parsed information.</p> Source code in <code>hdxms_datasets/reader.py</code> <pre><code>def parse_hxms_lines(lines: Iterable[str], read_content: bool = True) -&gt; HXMSResult:\n    \"\"\"Parse the different sections of an HXMS file.\n\n    Returns a dictionary with keys:\n        - \"HEADER\": list of header lines\n        - \"METADATA\": dict of metadata key-value pairs\n        - \"REMARK\": dict of remark key-value pairs\n        - \"DATA\": Narwhals DataFrame containing the HXMS data (if read_content is True)\n\n    Args:\n        lines: An iterable of lines from the HXMS file.\n\n    Returns:\n        A dictionary containing the parsed information.\n\n    \"\"\"\n    result: HXMSResult = {\n        \"HEADER\": [],\n        \"METADATA\": {},\n        \"REMARK\": {},\n    }\n    columns = []\n    line_iter = iter(lines)\n    for line in line_iter:\n        if line.startswith(\"HEADER\"):\n            content = line.lstrip(\"HEADER\").strip()\n            result[\"HEADER\"].append(content)\n        elif line.startswith(\"METADATA\"):\n            name, *raw_content = line.strip().split(\" \")\n            content = [item for item in raw_content if item]\n            if len(content) == 2:\n                key, value = content\n                result[\"METADATA\"][key] = value\n        elif line.startswith(\"REMARK\"):\n            name, *raw_content = line.strip().split(\" \")\n            content = [item for item in raw_content if item]\n            if len(content) == 2:\n                key, value = content\n                result[\"REMARK\"][key] = value\n        elif line.startswith(\"TITLE_TP\") and not read_content:\n            return result\n        elif line.startswith(\"TITLE_TP\") and read_content:\n            columns = _line_content(line)\n            break\n\n    # the rest of the lines are data lines\n    df = _parse_hxms_TP_lines(line_iter, sequence=result[\"METADATA\"][\"PROTEIN_SEQUENCE\"])\n    result[\"DATA\"] = df\n\n    # check read columns against expected columns\n    if columns:\n        expected_columns = list(HXMS_SCHEMA)[: len(columns)]\n        if columns != expected_columns:\n            warnings.warn(\n                f\"Columns in HXMS file do not match expected columns. \"\n                f\"Found: {columns}, Expected: {expected_columns}\"\n            )\n\n    return result\n</code></pre>"},{"location":"reference/reader/#reader.read_csv","title":"<code>read_csv(source, **kwargs)</code>","text":"<p>Read a CSV file and return a Narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | StringIO | bytes</code> <p>Source object representing the CSV data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Narwhals DataFrame containing the CSV data.</p> Source code in <code>hdxms_datasets/reader.py</code> <pre><code>def read_csv(source: Path | StringIO | bytes, **kwargs) -&gt; nw.DataFrame:\n    \"\"\"\n    Read a CSV file and return a Narwhals DataFrame.\n\n    Args:\n        source: Source object representing the CSV data.\n\n    Returns:\n        A Narwhals DataFrame containing the CSV data.\n\n    \"\"\"\n\n    if isinstance(source, Path):\n        return nw.read_csv(source.as_posix(), backend=BACKEND, **kwargs)\n    elif isinstance(source, bytes):\n        import polars as pl\n\n        return nw.from_native(pl.read_csv(source), **kwargs)\n    elif isinstance(source, StringIO):\n        try:\n            import polars as pl\n\n            return nw.from_native(pl.read_csv(source), **kwargs)\n        except ImportError:\n            pass\n        try:\n            import pandas as pd\n\n            return nw.from_native(pd.read_csv(source), **kwargs)  # type: ignore\n        except ImportError:\n            raise ValueError(\"No suitable backend found for reading file-like objects\")\n    else:\n        raise TypeError(f\"Source must be a Path, bytes, or file-like object, got: {type(source)}\")\n</code></pre>"},{"location":"reference/reader/#reader.read_hdexaminer_peptide_pool","title":"<code>read_hdexaminer_peptide_pool(source)</code>","text":"<p>Read an HDX-Examiner peptide pool file and return a Narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | StringIO</code> <p>Source object representing the HDX-Examiner peptide pool data.</p> required Source code in <code>hdxms_datasets/reader.py</code> <pre><code>def read_hdexaminer_peptide_pool(source: Path | StringIO) -&gt; nw.DataFrame:\n    \"\"\"\n    Read an HDX-Examiner peptide pool file and return a Narwhals DataFrame.\n\n    Args:\n        source: Source object representing the HDX-Examiner peptide pool data.\n\n    \"\"\"\n\n    # read the data and header\n    if isinstance(source, StringIO):\n        try:\n            import polars as pl\n\n            df = nw.from_native(pl.read_csv(source, skip_rows=1, has_header=True))\n        except ImportError:\n            import pandas as pd\n\n            df = nw.from_native(pd.read_csv(source, skiprows=[0]))\n\n        source.seek(0)\n        exposure_line = source.readline()\n        header_line = source.readline()\n\n    else:\n        kwargs = {\n            \"polars\": {\"skip_rows\": 1, \"has_header\": True},\n            \"pandas\": {\"skiprows\": [0]},\n        }\n        if BACKEND not in kwargs:\n            raise ValueError(f\"Unsupported backend: {BACKEND}\")\n        df = nw.read_csv(source.as_posix(), backend=BACKEND, **kwargs[BACKEND])\n        with open(source, \"r\") as fh:\n            exposure_line = fh.readline()\n            header_line = fh.readline()\n\n    exposure_columns = exposure_line.strip().split(\",\")\n    header_columns = header_line.strip().split(\",\")\n\n    found_schema = df[:, 0:8].schema\n    if found_schema != HDEXAMINER_PEPTIDE_POOL_INITIAL_SCHEMA:\n        raise ValueError(\"HDX-Examiner peptide pool file has an unexpected columns schema.\")\n\n    # find indices of exposure markers in header\n    has_entry_with_end = [i for i, col in enumerate(exposure_columns) if col] + [\n        len(exposure_columns)\n    ]\n\n    num_blocks = len(has_entry_with_end) - 1\n    initial_df = nw.concat([df[:, :8]] * num_blocks, how=\"vertical\")\n\n    has_entry_with_end = [i for i, col in enumerate(exposure_columns) if col] + [\n        len(exposure_columns)\n    ]\n\n    output = defaultdict(list)\n    for i, j in zip(has_entry_with_end[:-1], has_entry_with_end[1:]):\n        exposure = exposure_columns[i]\n        found_columns = header_columns[i:j]\n\n        # iterate over the expected columns, extract the series and append to output\n        # for missing columns, create a series of nulls\n        for col, dtype in HDEXAMINER_PEPTIDE_POOL_REPEATED_SCHEMA.items():\n            if col in found_columns:\n                column_index = found_columns.index(col) + i\n                frame = df[:, column_index].cast(dtype).alias(col).to_frame()\n            else:\n                c = itertools.repeat(None, len(df))\n                frame = nw.Series.from_iterable(\n                    name=col, values=c, dtype=dtype, backend=BACKEND\n                ).to_frame()\n\n            output[col].append(frame)\n\n        c = itertools.repeat(exposure, len(df))\n        frame = nw.Series.from_iterable(\n            name=\"Exposure\", values=c, dtype=dtype, backend=BACKEND\n        ).to_frame()\n        output[\"Exposure\"].append(frame)\n\n    # combine all 1-column frames first vertically and then horizontally with initial_df\n    concatenated = {k: nw.concat(v, how=\"vertical\") for k, v in output.items()}\n    final_output = nw.concat([initial_df, *concatenated.values()], how=\"horizontal\")\n\n    return final_output\n</code></pre>"},{"location":"reference/reader/#reader.read_hxms","title":"<code>read_hxms(source, returns='HXMSResult')</code>","text":"<pre><code>read_hxms(\n    source: Path | IO | bytes,\n    returns: Literal[\"HXMSResult\"],\n) -&gt; HXMSResult\n</code></pre><pre><code>read_hxms(\n    source: Path | IO | bytes, returns: Literal[\"DataFrame\"]\n) -&gt; nw.DataFrame\n</code></pre> <p>Read an HXMS file and return a HXMSResult or Narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | IO | bytes</code> <p>Source object representing the HXMS data.</p> required <p>Returns:</p> Type Description <code>HXMSResult | DataFrame</code> <p>A Narwhals DataFrame containing the HXMS data.</p> Source code in <code>hdxms_datasets/reader.py</code> <pre><code>def read_hxms(\n    source: Path | IO | bytes,\n    returns: Literal[\"HXMSResult\", \"DataFrame\"] = \"HXMSResult\",\n) -&gt; HXMSResult | nw.DataFrame:\n    \"\"\"\n    Read an HXMS file and return a HXMSResult or Narwhals DataFrame.\n\n    Args:\n        source: Source object representing the HXMS data.\n\n    Returns:\n        A Narwhals DataFrame containing the HXMS data.\n    \"\"\"\n\n    lines = _hxms_splitlines(source)\n    # TODO make generator based on input type\n    line_gen = iter(lines)\n\n    # first get column names\n    result = parse_hxms_lines(line_gen, read_content=True)\n\n    if returns == \"HXMSResult\":\n        return result\n    elif returns == \"DataFrame\":\n        assert \"DATA\" in result, \"No data found in HXMS file\"\n        return result[\"DATA\"]\n    else:\n        raise ValueError(f\"Unsupported returns value: {returns!r}\")\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#utils.FrameSlicer","title":"<code>FrameSlicer</code>","text":"<p>               Bases: <code>Generic[IntoFrameT]</code></p> <p>Wrap a DataFrame and allow indexing by column values (sorted).</p> Example <p>s = FrameSlicer(df, col=\"exposure\") first_df = s[0] # filtered dataframe where col == first unique value three = s[0:3]  # filtered dataframe where col in first three unique values</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>class FrameSlicer(Generic[IntoFrameT]):\n    \"\"\"Wrap a DataFrame and allow indexing by column values (sorted).\n\n    Example:\n        s = FrameSlicer(df, col=\"exposure\")\n        first_df = s[0] # filtered dataframe where col == first unique value\n        three = s[0:3]  # filtered dataframe where col in first three unique values\n    \"\"\"\n\n    def __init__(self, df: IntoFrameT, col: str):\n        self._df = nw.from_native(df)\n        self._col = col\n\n        # get unique values as Python list\n        raw_vals = sorted(self._df[self._col].unique().to_list())\n        self._values: list[object] = raw_vals\n\n    def __len__(self) -&gt; int:\n        return len(self._values)\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    def __getitem__(self, key: Union[int, slice]) -&gt; IntoFrameT:\n        slice_result = self._values[key]\n\n        if isinstance(slice_result, list):\n            return self._df.filter(nw.col(self._col).is_in(slice_result)).to_native()\n        else:\n            return self._df.filter(nw.col(self._col) == slice_result).to_native()\n</code></pre>"},{"location":"reference/utils/#utils.contiguous_peptides","title":"<code>contiguous_peptides(df)</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing contiguous regions.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef contiguous_peptides(df: IntoFrame) -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing contiguous regions.\n    \"\"\"\n    # cast to ensure df is a narwhals DataFrame\n    df = cast(nw.DataFrame, df).select([\"start\", \"end\"]).unique().sort(by=[\"start\", \"end\"])\n\n    regions = []\n    current_start, current_end = None, 0\n\n    for start_val, end_val in df.select([nw.col(\"start\"), nw.col(\"end\")]).iter_rows(named=False):\n        if current_start is None:\n            # Initialize the first region\n            current_start, current_end = start_val, end_val\n        elif start_val &lt;= current_end + 1:  # Check for contiguity\n            # Extend the current region\n            current_end = max(current_end, end_val)\n        else:\n            # Save the previous region and start a new one\n            regions.append((current_start, current_end))\n            current_start, current_end = start_val, end_val\n\n    # Don't forget to add the last region\n    if current_start is not None:\n        regions.append((current_start, current_end))\n\n    return regions\n</code></pre>"},{"location":"reference/utils/#utils.diff_sequence","title":"<code>diff_sequence(a, b)</code>","text":"<p>Compute the similarity ratio between two sequences.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def diff_sequence(a: str, b: str) -&gt; float:\n    \"\"\"\n    Compute the similarity ratio between two sequences.\n    \"\"\"\n    return difflib.SequenceMatcher(None, a, b).ratio()\n</code></pre>"},{"location":"reference/utils/#utils.get_peptides_by_type","title":"<code>get_peptides_by_type(peptides, deuteration_type)</code>","text":"<p>Get peptides of a specific deuteration type.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def get_peptides_by_type(\n    peptides: list[Peptides], deuteration_type: DeuterationType\n) -&gt; Optional[Peptides]:\n    \"\"\"Get peptides of a specific deuteration type.\"\"\"\n    matching_peptides = [p for p in peptides if p.deuteration_type == deuteration_type]\n    if not matching_peptides:\n        return None\n    if len(matching_peptides) &gt; 1:\n        return None\n    return matching_peptides[0]\n</code></pre>"},{"location":"reference/utils/#utils.non_overlapping_peptides","title":"<code>non_overlapping_peptides(df)</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing non-overlapping peptides.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef non_overlapping_peptides(\n    df: IntoFrame,\n) -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing non-overlapping peptides.\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([\"start\", \"end\"]).unique().sort(by=[\"start\", \"end\"])\n\n    regions = df.rows()\n    out = [regions[0]]\n    for start_val, end_val in regions[1:]:\n        if start_val &gt; out[-1][1]:\n            out.append((start_val, end_val))\n        else:\n            continue\n\n    return out\n</code></pre>"},{"location":"reference/utils/#utils.peptide_redundancy","title":"<code>peptide_redundancy(df)</code>","text":"<p>Compute the redundancy of peptides in a DataFrame based on their start and end positions. Redundancy is defined as the number of peptides overlapping at each position.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrame</code> <p>DataFrame containing peptide information with 'start' and 'end' columns.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A tuple containing:</p> <code>ndarray</code> <ul> <li>r_number: An array of positions from the minimum start to the maximum end.</li> </ul> <code>tuple[ndarray, ndarray]</code> <ul> <li>redundancy: An array of redundancy counts for each position in r_number.</li> </ul> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef peptide_redundancy(df: IntoFrame) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the redundancy of peptides in a DataFrame based on their start and end positions.\n    Redundancy is defined as the number of peptides overlapping at each position.\n\n    Args:\n        df: DataFrame containing peptide information with 'start' and 'end' columns.\n\n    Returns:\n        A tuple containing:\n        - r_number: An array of positions from the minimum start to the maximum end.\n        - redundancy: An array of redundancy counts for each position in r_number.\n\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([\"start\", \"end\"]).unique().sort(by=[\"start\", \"end\"])\n    vmin, vmax = df[\"start\"][0], df[\"end\"][-1]\n\n    r_number = np.arange(vmin, vmax + 1, dtype=int)\n    redundancy = np.zeros_like(r_number, dtype=int)\n    for s, e in df.rows():\n        i0, i1 = np.searchsorted(r_number, (s, e))\n        redundancy[i0:i1] += 1\n\n    return r_number, redundancy\n</code></pre>"},{"location":"reference/utils/#utils.peptides_are_unique","title":"<code>peptides_are_unique(peptides_df)</code>","text":"<p>Check if the peptides in the dataframe are unique.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def peptides_are_unique(peptides_df: nw.DataFrame) -&gt; bool:\n    \"\"\"Check if the peptides in the dataframe are unique.\"\"\"\n    unique_peptides = peptides_df.select([\"start\", \"end\"]).unique()\n    return len(unique_peptides) == len(peptides_df)\n</code></pre>"},{"location":"reference/utils/#utils.reconstruct_sequence","title":"<code>reconstruct_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Reconstruct the sequence form a dataframe of peptides with sequence information. The sequence is reconstructed by replacing the known sequence with the peptide sequences at the specified start and end positions.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>Starting sequence. Can be a string 'X' as placeholder.</p> required <code>n_term</code> <code>int</code> <p>The residue number of the N-terminal residue. This is typically 1, can be negative in case of purification tags.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>The reconstructed sequence.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef reconstruct_sequence(\n    peptides: nw.DataFrame,\n    known_sequence: str,\n    n_term: int = 1,\n) -&gt; str:\n    \"\"\"\n    Reconstruct the sequence form a dataframe of peptides with sequence information.\n    The sequence is reconstructed by replacing the known sequence with the peptide\n    sequences at the specified start and end positions.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: Starting sequence. Can be a string 'X' as placeholder.\n        n_term: The residue number of the N-terminal residue. This is typically 1, can be\n            negative in case of purification tags.\n\n    Returns:\n        The reconstructed sequence.\n    \"\"\"\n\n    reconstructed = list(known_sequence)\n    for start_, end_, sequence_ in peptides.select([\"start\", \"end\", \"sequence\"]).iter_rows():  # type: ignore\n        start_idx = start_ - n_term\n        assert end_ - start_ + 1 == len(sequence_), (\n            f\"Length mismatch at {start_}:{end_} with sequence {sequence_}\"\n        )\n\n        for i, aa in enumerate(sequence_, start=start_idx):\n            reconstructed[i] = aa\n\n    return \"\".join(reconstructed)\n</code></pre>"},{"location":"reference/utils/#utils.records_to_dict","title":"<code>records_to_dict(records)</code>","text":"<p>Convert a list of records to a dictionary.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def records_to_dict(records: list[dict[str, Any]]) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert a list of records to a dictionary.\n    \"\"\"\n    output = defaultdict(list)\n    for record in records:\n        for key, value in record.items():\n            output[key].append(value)\n\n    return dict(output)\n</code></pre>"},{"location":"reference/utils/#utils.slice_exposure","title":"<code>slice_exposure(df)</code>","text":"<p>Factory returning FrameSlicer for df using column 'exposure'.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>def slice_exposure(df: IntoFrameT) -&gt; FrameSlicer[IntoFrameT]:\n    \"\"\"Factory returning FrameSlicer for df using column 'exposure'.\"\"\"\n    return FrameSlicer(df, col=\"exposure\")\n</code></pre>"},{"location":"reference/utils/#utils.verify_sequence","title":"<code>verify_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Verify the sequence of peptides against the given sequence.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>IntoFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>The original sequence to check against.</p> required <code>n_term</code> <code>int</code> <p>The number of N-terminal residues to consider.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[tuple[int, str, str]]</code> <p>A tuple containing the fixed sequence and a list of mismatches.</p> Source code in <code>hdxms_datasets/utils.py</code> <pre><code>@nw.narwhalify\ndef verify_sequence(\n    peptides: IntoFrame,\n    known_sequence: str,\n    n_term: int = 1,\n) -&gt; list[tuple[int, str, str]]:\n    \"\"\"\n    Verify the sequence of peptides against the given sequence.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: The original sequence to check against.\n        n_term: The number of N-terminal residues to consider.\n\n    Returns:\n        A tuple containing the fixed sequence and a list of mismatches.\n    \"\"\"\n\n    reconstructed_sequence = reconstruct_sequence(peptides, known_sequence, n_term)\n\n    mismatches = []\n    for r_number, (expected, found) in enumerate(\n        zip(known_sequence, reconstructed_sequence), start=n_term\n    ):\n        if expected != found:\n            mismatches.append((r_number, expected, found))\n\n    return mismatches\n</code></pre>"},{"location":"reference/verification/","title":"verification","text":""},{"location":"reference/verification/#verification.build_structure_peptides_comparison","title":"<code>build_structure_peptides_comparison(structure, peptides)</code>","text":"<p>Compares residue numbering and identity between a structure and peptides.</p> <p>Applies any residue offset defined in the peptides' structure mapping.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame merging structure and peptide residue information.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def build_structure_peptides_comparison(\n    structure: Structure,\n    peptides: Peptides,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Compares residue numbering and identity between a structure and peptides.\n\n    Applies any residue offset defined in the peptides' structure mapping.\n\n    Returns:\n        A DataFrame merging structure and peptide residue information.\n    \"\"\"\n    structure_df = residue_df_from_structure(structure, peptides.structure_mapping)\n    residue_df = residue_df_from_peptides(peptides)\n\n    import polars as pl\n\n    # apply residue offset to peptides\n    # doesnt support 'mapping' dicts yet\n    if peptides.structure_mapping.mapping:\n        raise NotImplementedError(\"Custom residue mapping is not supported yet.\")\n\n    residue_df = residue_df.with_columns(\n        (pl.col(\"resi\") + peptides.structure_mapping.residue_offset).cast(str).alias(\"resi\")\n    )\n\n    chains = (\n        peptides.structure_mapping.chain\n        if peptides.structure_mapping.chain is not None\n        else structure_df[\"chain\"].unique().to_list()\n    )\n\n    # supplement the residue_df with all chains\n    # multi-chain peptides are expected to correspond to homomultimers\n    import polars as pl\n\n    residue_df_chain = pl.concat(\n        [residue_df.with_columns(pl.lit(ch).alias(\"chain\")) for ch in chains]\n    )\n\n    merged = residue_df_chain.join(\n        structure_df,\n        on=[\"chain\", \"resi\"],\n        how=\"left\",\n    )\n\n    return merged\n</code></pre>"},{"location":"reference/verification/#verification.compare_structure_peptides","title":"<code>compare_structure_peptides(structure, peptides, returns='dict')</code>","text":"<pre><code>compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"dict\"] = \"dict\",\n) -&gt; CompareSummary\n</code></pre><pre><code>compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"df\"],\n) -&gt; pl.DataFrame\n</code></pre><pre><code>compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"both\"],\n) -&gt; tuple[CompareSummary, pl.DataFrame]\n</code></pre> <p>Compares structure and peptide data.</p> <p>Returned dataframe has the following columns: resi (str): Residue number from peptides resn: (str): One letter amino acid code from peptides resn_TLA: (str): Three letter amino acid code from peptides chain (str): Chain identifier resn_TLA_right: (str): Three letter amino acid code from structure (null if no match)</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def compare_structure_peptides(\n    structure: Structure,\n    peptides: Peptides,\n    returns: Literal[\"dict\", \"df\", \"both\"] = \"dict\",\n) -&gt; pl.DataFrame | CompareSummary | tuple[CompareSummary, pl.DataFrame]:\n    \"\"\"Compares structure and peptide data.\n\n    Returned dataframe has the following columns:\n    resi (str): Residue number from peptides\n    resn: (str): One letter amino acid code from peptides\n    resn_TLA: (str): Three letter amino acid code from peptides\n    chain (str): Chain identifier\n    resn_TLA_right: (str): Three letter amino acid code from structure (null if no match)\n\n    \"\"\"\n\n    df = build_structure_peptides_comparison(structure, peptides)\n\n    if returns == \"dict\":\n        return summarize_compare_table(df)\n    elif returns == \"df\":\n        return df\n    elif returns == \"both\":\n        return summarize_compare_table(df), df\n    else:\n        raise ValueError(f\"Invalid returns value: {returns!r}\")\n</code></pre>"},{"location":"reference/verification/#verification.datafiles_exist","title":"<code>datafiles_exist(dataset)</code>","text":"<p>Check if the data files for all peptides and structures in the dataset exist.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def datafiles_exist(dataset: HDXDataSet) -&gt; bool:\n    \"\"\"\n    Check if the data files for all peptides and structures in the dataset exist.\n    \"\"\"\n    for state in dataset.states:\n        for peptides in state.peptides:\n            if not peptides.data_file.exists():\n                return False\n    if not dataset.structure.data_file.exists():\n        return False\n    return True\n</code></pre>"},{"location":"reference/verification/#verification.residue_df_from_peptides","title":"<code>residue_df_from_peptides(peptides)</code>","text":"<p>Create a dataframe from the peptides with resi, resn_TLA.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>Peptides</code> <p>Peptides object</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with columns: resi: residue number (int) resn: one letter amino acid code (str) resn_TLA: three letter amino acid code (str)</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def residue_df_from_peptides(peptides: Peptides) -&gt; pl.DataFrame:\n    \"\"\"Create a dataframe from the peptides with resi, resn_TLA.\n\n    Args:\n        peptides: Peptides object\n\n    Returns:\n        DataFrame with columns:\n            resi: residue number (int)\n            resn: one letter amino acid code (str)\n            resn_TLA: three letter amino acid code (str)\n\n\n    \"\"\"\n    from Bio.Data import IUPACData\n    import polars as pl\n\n    one_to_three = IUPACData.protein_letters_1to3\n\n    peptide_df = peptides.load().to_native()\n\n    start, end = peptide_df[\"start\"].min(), peptide_df[\"end\"].max()\n    residues = range(start, end + 1)\n    known_sequence = \"X\" * len(residues)\n    sequence = reconstruct_sequence(peptide_df, known_sequence, n_term=start)\n\n    residue_df = (\n        pl.DataFrame({\"resi\": residues, \"resn\": list(sequence)})\n        .filter(pl.col(\"resn\") != \"X\")\n        .with_columns(\n            [\n                pl.col(\"resi\"),\n                pl.col(\"resn\").replace_strict(one_to_three).str.to_uppercase().alias(\"resn_TLA\"),\n            ]\n        )\n    )\n\n    return residue_df\n</code></pre>"},{"location":"reference/verification/#verification.residue_df_from_structure","title":"<code>residue_df_from_structure(structure, mapping=StructureMapping())</code>","text":"<p>Create a dataframe from the structure with chain, resi, resn_TLA</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def residue_df_from_structure(\n    structure: Structure, mapping: StructureMapping = StructureMapping()\n) -&gt; pl.DataFrame:\n    \"\"\"Create a dataframe from the structure with chain, resi, resn_TLA\"\"\"\n    if structure.format not in [\"cif\", \"mmcif\"]:\n        raise ValueError(f\"Unsupported structure format: {structure.format}\")\n\n    # create the reference structure amino acid dataframe\n    from Bio.PDB.MMCIF2Dict import MMCIF2Dict\n    from Bio.Data import IUPACData\n    import polars as pl\n\n    mm = MMCIF2Dict(structure.data_file)\n\n    one_to_three = IUPACData.protein_letters_1to3\n    AA_codes = [a.upper() for a in list(one_to_three.values())]\n\n    # create a dataframe with chain, resi, resn_TLA from structure\n    chain_name = \"label_asym_id\" if not mapping.auth_chain_labels else \"auth_asym_id\"\n    resn_name = \"label_seq_id\" if not mapping.auth_residue_numbers else \"auth_seq_id\"\n\n    structure_df = (\n        pl.DataFrame(\n            {\n                \"chain\": mm[\"_atom_site.\" + chain_name],\n                \"resi\": mm[\"_atom_site.\" + resn_name],\n                \"resn_TLA\": [s.upper() for s in mm[\"_atom_site.label_comp_id\"]],\n            }\n        )\n        .unique()\n        .filter(pl.col(\"resn_TLA\").is_in(AA_codes))\n    )\n\n    return structure_df\n</code></pre>"},{"location":"reference/verification/#verification.residue_offset_optimization","title":"<code>residue_offset_optimization(structure, peptides, search_range=(-100, 100))</code>","text":"<p>Optimize residue offset to maximize matched residues between structure and peptides. Ignores current offset on the peptides' structure mapping.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Structure</code> <p>Structure object</p> required <code>peptides</code> <code>Peptides</code> <p>Peptides object</p> required <code>search_range</code> <code>tuple[int, int]</code> <p>Tuple of (min_offset, max_offset) to search</p> <code>(-100, 100)</code> <p>Returns:</p> Type Description <code>int</code> <p>The optimal residue offset as an integer.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def residue_offset_optimization(\n    structure: Structure,\n    peptides: Peptides,\n    search_range: tuple[int, int] = (-100, 100),\n) -&gt; int:\n    \"\"\"\n    Optimize residue offset to maximize matched residues between structure and peptides.\n    Ignores current offset on the peptides' structure mapping.\n\n    Args:\n        structure: Structure object\n        peptides: Peptides object\n        search_range: Tuple of (min_offset, max_offset) to search\n\n    Returns:\n        The optimal residue offset as an integer.\n    \"\"\"\n\n    import polars as pl\n\n    structure_df = residue_df_from_structure(structure, peptides.structure_mapping)\n    residue_df = residue_df_from_peptides(peptides)\n    residue_df = pl.concat(\n        [\n            residue_df.with_columns(pl.lit(ch).alias(\"chain\"))\n            for ch in peptides.structure_mapping.chain or []\n        ]\n    )\n\n    best_offset = search_range[0]\n    prev_result = -float(\"inf\")\n    for offset in range(*search_range):\n        residue_df_with_offset = residue_df.with_columns(\n            (pl.col(\"resi\").cast(int) + offset).cast(str).alias(\"resi\")\n        )\n\n        merged = residue_df_with_offset.join(\n            structure_df,\n            on=[\"chain\", \"resi\"],\n            how=\"left\",\n        )\n\n        matched = merged.drop_nulls(subset=[\"resn_TLA_right\"]).filter(\n            pl.col(\"resn_TLA\") == pl.col(\"resn_TLA_right\")\n        )\n\n        result = matched.height\n        if result &gt; prev_result:\n            best_offset = offset\n            prev_result = result\n\n    return best_offset\n</code></pre>"},{"location":"reference/verification/#verification.summarize_compare_table","title":"<code>summarize_compare_table(df)</code>","text":"<p>Derive the metrics from the merged table.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def summarize_compare_table(df: pl.DataFrame) -&gt; CompareSummary:\n    \"\"\"Derive the metrics from the merged table.\"\"\"\n    # rows with a structure match (adjust column names to your schema)\n    import polars as pl\n\n    matched = df.drop_nulls(subset=[\"resn_TLA_right\"])\n    identical = matched.filter(pl.col(\"resn_TLA\") == pl.col(\"resn_TLA_right\"))\n\n    return {\n        \"total_residues\": df.height,\n        \"matched_residues\": matched.height,\n        \"identical_residues\": identical.height,\n    }\n</code></pre>"},{"location":"reference/verification/#verification.verify_dataset","title":"<code>verify_dataset(dataset, strict=True)</code>","text":"<p>Verify the integrity of the dataset by checking sequences and data files.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def verify_dataset(dataset: HDXDataSet, strict: bool = True):\n    \"\"\"Verify the integrity of the dataset by checking sequences and data files.\"\"\"\n    verify_peptides(dataset)\n    if not datafiles_exist(dataset):\n        raise ValueError(\"Missing datafiles\")\n\n    if dataset.file_hash is None:\n        raise ValueError(\"Dataset file hash is not set\")\n\n    if strict:\n        verify_version(dataset)\n</code></pre>"},{"location":"reference/verification/#verification.verify_peptides","title":"<code>verify_peptides(dataset)</code>","text":"<p>Verify that all peptide sequences match the protein sequence in the dataset states.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def verify_peptides(dataset: HDXDataSet):\n    \"\"\"Verify that all peptide sequences match the protein sequence in the dataset states.\"\"\"\n    for state in dataset.states:\n        sequence = state.protein_state.sequence\n        for i, peptides in enumerate(state.peptides):\n            try:\n                peptide_table = peptides.load()\n            except Exception as e:\n                raise ValueError(f\"State: {state.name}, Peptides[{i}] failed to load: {e}\")\n            try:\n                mismatches = verify_sequence(\n                    peptide_table, sequence, n_term=state.protein_state.n_term\n                )\n            except IndexError as e:\n                raise IndexError(f\"State: {state.name}, Peptides[{i}] has an index error: {e}\")\n            if mismatches:\n                raise ValueError(\n                    f\"State: {state.name}, Peptides[{i}] does not match protein sequence, mismatches: {mismatches}\"\n                )\n</code></pre>"},{"location":"reference/verification/#verification.verify_version","title":"<code>verify_version(dataset)</code>","text":"<p>Verify that the dataset was created with a pep 440 compliant version.</p> Source code in <code>hdxms_datasets/verification.py</code> <pre><code>def verify_version(dataset: HDXDataSet):\n    \"\"\"Verify that the dataset was created with a pep 440 compliant version.\"\"\"\n    ver_str = dataset.metadata.package_version\n\n    v = Version(ver_str)  # type: ignore\n\n    if v.pre:\n        raise ValueError(\n            f\"A pre-release version of `hdxms-datasets` was used to create this dataset: {ver_str}\"\n        )\n    if v.dev:\n        raise ValueError(\n            f\"A development version of `hdxms-datasets` was used to create this dataset: {ver_str}\"\n        )\n    if v.local:\n        raise ValueError(\n            f\"A local version of `hdxms-datasets` was used to create this dataset: {ver_str}\"\n        )\n</code></pre>"},{"location":"reference/view/","title":"view","text":""},{"location":"reference/view/#view.StructureView","title":"<code>StructureView</code>","text":"Source code in <code>hdxms_datasets/view.py</code> <pre><code>class StructureView:\n    def __init__(\n        self,\n        structure: Structure,\n        mapping: StructureMapping = StructureMapping(),\n        hide_water=True,\n        **kwargs: dict,\n    ):\n        \"\"\"\n        Initialize the PDBeMolstar visualization namespace.\n        Can uses a `StructureMapping` which relates peptides to the structure\n\n        Args:\n            structure: The structure to visualize.\n            mapping: Optional structure mapping information.\n            **kwargs: Additional keyword arguments for customization.\n        \"\"\"\n        self.structure = structure\n        self.mapping = mapping\n\n        from ipymolstar import PDBeMolstar\n\n        self.view = PDBeMolstar(\n            custom_data=self.structure.pdbemolstar_custom_data(),\n            hide_water=hide_water,\n            **kwargs,\n        )\n\n    def show(self):\n        return self.view\n\n    def highlight(self, resi: int) -&gt; StructureView:\n        \"\"\"\n        Highlights a residue in the structure.\n\n        Args:\n            resi: Residue number to highlight.\n\n        Returns:\n            The updated StructureView object.\n        \"\"\"\n        param = self.get_query_param(resi)\n        data = self._augment_chain([param])\n        self.view.highlight = {\"data\": data}\n\n        return self\n\n    def color_peptide(\n        self,\n        start: int,\n        end: int,\n        color: str = \"red\",\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; StructureView:\n        \"\"\"\n        Color a peptide by start and end residue numbers.\n\n        Args:\n            start: Start residue number.\n            end: End residue number.\n            color: Color for the peptide.\n            non_selected_color: Color for non-selected regions.\n\n        Returns:\n            The updated StructureView object.\n        \"\"\"\n        kwargs = {\"color\": color}\n        param = self.get_query_param_range(start, end, **kwargs)\n        data = self._augment_chain([param])\n\n        color_data = {\n            \"data\": data,\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = None\n\n        return self\n\n    def peptide_coverage(\n        self,\n        peptides: nw.DataFrame,\n        color: str = \"darkgreen\",\n        non_selected_color: str = \"lightgray\",\n    ):\n        \"\"\"\n        Plots peptide coverage on the structure.\n\n        Args:\n            peptides: Peptides object or DataFrame containing peptide data.\n            color: Color for the covered regions.\n            non_selected_color: Color for non-covered regions.\n\n        Returns:\n            The updated StructureView object.\n        \"\"\"\n        intervals = contiguous_peptides(peptides)\n\n        data = [self.get_query_param_range(start, end, color=color) for start, end in intervals]\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = None\n        return self\n\n    def non_overlapping_peptides(\n        self,\n        peptides: nw.DataFrame,\n        colors: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ):\n        \"\"\"Selects a set of non-overlapping peptides to display on the structure. Starts with the first\n        peptide and successively adds peptides that do not overlap with already selected peptides.\n\n        Args:\n            peptides: Peptides object or DataFrame containing peptide data.\n            colors: List of colors to cycle through for different peptides.\n            non_selected_color: Color for non-covered regions.\n\n        Returns:\n            The updated StructureView object.\n\n        \"\"\"\n\n        intervals = non_overlapping_peptides(peptides)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n        )\n\n        cdata = []\n        tdata = []\n        for (start, end), color in zip(intervals, itertools.cycle(colors)):\n            cdata.append(self.get_query_param_range(start, end, color=color))\n            df_f = peptides.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end)).to_native()\n            sequence = df_f[\"sequence\"].unique().first()\n            tdata.append(self.get_query_param_range(start, end, tooltip=f\"Peptide: {sequence}\"))\n\n        color_data = {\n            \"data\": self._augment_chain(cdata),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = {\"data\": self._augment_chain(tdata)}\n        return self\n\n    def peptide_redundancy(\n        self,\n        peptides: nw.DataFrame,\n        colors: list[str] | None = None,\n        clip: Optional[int] = None,\n        non_selected_color: str = \"lightgray\",\n    ):\n        \"\"\"Colors residues by peptide redundancy.\n\n        Args:\n            peptides: Peptides DataFrame containing peptide data.\n            colors: List of colors to use for different redundancy levels.\n            clip: Optional maximum redundancy value for clipping.\n            non_selected_color: Color for non-covered regions.\n\n        Returns:\n            The updated StructureView object.\n\n        \"\"\"\n        r_number, redundancy = peptide_redundancy(peptides)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n        )\n\n        if clip:\n            vals = ((redundancy.clip(None, clip) / clip) * (len(colors) - 1)).astype(int)\n        else:\n            vals = ((redundancy / redundancy.max()) * (len(colors) - 1)).astype(int)\n\n        data = []\n        tooltips = []\n        for rn, rv, rv_clip in zip(r_number, redundancy, vals):\n            tooltips.append(self.get_query_param(int(rn), tooltip=f\"Redundancy: {rv} peptides\"))\n\n            if rv == 0:\n                continue\n\n            color_elem = self.get_query_param(int(rn), color=colors[rv_clip])\n            data.append(color_elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self.view.color_data = color_data\n        self.view.tooltips = {\"data\": self._augment_chain(tooltips)}\n        return self\n\n    def set_mapping(self, mapping: StructureMapping):\n        self.mapping = mapping\n        return self\n\n    def get_query_param(self, resi: int, **kwargs):\n        resi = self.mapping.map(resi)\n\n        # TODO entity support\n        c_dict = {\n            self.residue_name: int(resi),\n            **kwargs,\n        }\n\n        return c_dict\n\n    def get_query_param_range(self, start: int, end: int, **kwargs):\n        start = self.mapping.map(start)\n        end = self.mapping.map(end)\n\n        # TODO entity support\n        c_dict = {\n            \"start_\" + self.residue_name: int(start),\n            \"end_\" + self.residue_name: int(end),\n            **kwargs,\n        }\n\n        return c_dict\n\n    @property\n    def residue_name(self) -&gt; str:\n        \"\"\"\n        Returns the residue name based on whether auth residue numbers are used.\n        \"\"\"\n        return \"auth_residue_number\" if self.mapping.auth_residue_numbers else \"residue_number\"\n\n    @property\n    def chain_name(self) -&gt; str:\n        \"\"\"\n        Returns the chain name based on whether auth chain labels are used.\n\n        Note that 'struct_asym_id' used in PDBeMolstar is equivalent to\n        'label_asym_id' in mmCIF.\n\n        \"\"\"\n        return \"auth_asym_id\" if self.mapping.auth_chain_labels else \"struct_asym_id\"\n\n    def _augment_chain(\n        self,\n        data: list[dict[str, ValueType]],\n    ) -&gt; list[dict[str, ValueType]]:\n        \"\"\"Augment a list of data with chain information\"\"\"\n        if self.mapping.chain:\n            aug_data = []\n            for elem, chain in itertools.product(data, self.mapping.chain):\n                aug_data.append(elem | {self.chain_name: chain})\n        else:\n            aug_data = data\n\n        return aug_data\n\n    def _repr_mimebundle_(self, include=None, exclude=None):\n        return self.show()._repr_mimebundle_(include=include, exclude=exclude)\n</code></pre>"},{"location":"reference/view/#view.StructureView.chain_name","title":"<code>chain_name</code>  <code>property</code>","text":"<p>Returns the chain name based on whether auth chain labels are used.</p> <p>Note that 'struct_asym_id' used in PDBeMolstar is equivalent to 'label_asym_id' in mmCIF.</p>"},{"location":"reference/view/#view.StructureView.residue_name","title":"<code>residue_name</code>  <code>property</code>","text":"<p>Returns the residue name based on whether auth residue numbers are used.</p>"},{"location":"reference/view/#view.StructureView.__init__","title":"<code>__init__(structure, mapping=StructureMapping(), hide_water=True, **kwargs)</code>","text":"<p>Initialize the PDBeMolstar visualization namespace. Can uses a <code>StructureMapping</code> which relates peptides to the structure</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Structure</code> <p>The structure to visualize.</p> required <code>mapping</code> <code>StructureMapping</code> <p>Optional structure mapping information.</p> <code>StructureMapping()</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for customization.</p> <code>{}</code> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def __init__(\n    self,\n    structure: Structure,\n    mapping: StructureMapping = StructureMapping(),\n    hide_water=True,\n    **kwargs: dict,\n):\n    \"\"\"\n    Initialize the PDBeMolstar visualization namespace.\n    Can uses a `StructureMapping` which relates peptides to the structure\n\n    Args:\n        structure: The structure to visualize.\n        mapping: Optional structure mapping information.\n        **kwargs: Additional keyword arguments for customization.\n    \"\"\"\n    self.structure = structure\n    self.mapping = mapping\n\n    from ipymolstar import PDBeMolstar\n\n    self.view = PDBeMolstar(\n        custom_data=self.structure.pdbemolstar_custom_data(),\n        hide_water=hide_water,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/view/#view.StructureView.color_peptide","title":"<code>color_peptide(start, end, color='red', non_selected_color='lightgray')</code>","text":"<p>Color a peptide by start and end residue numbers.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Start residue number.</p> required <code>end</code> <code>int</code> <p>End residue number.</p> required <code>color</code> <code>str</code> <p>Color for the peptide.</p> <code>'red'</code> <code>non_selected_color</code> <code>str</code> <p>Color for non-selected regions.</p> <code>'lightgray'</code> <p>Returns:</p> Type Description <code>StructureView</code> <p>The updated StructureView object.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def color_peptide(\n    self,\n    start: int,\n    end: int,\n    color: str = \"red\",\n    non_selected_color: str = \"lightgray\",\n) -&gt; StructureView:\n    \"\"\"\n    Color a peptide by start and end residue numbers.\n\n    Args:\n        start: Start residue number.\n        end: End residue number.\n        color: Color for the peptide.\n        non_selected_color: Color for non-selected regions.\n\n    Returns:\n        The updated StructureView object.\n    \"\"\"\n    kwargs = {\"color\": color}\n    param = self.get_query_param_range(start, end, **kwargs)\n    data = self._augment_chain([param])\n\n    color_data = {\n        \"data\": data,\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self.view.color_data = color_data\n    self.view.tooltips = None\n\n    return self\n</code></pre>"},{"location":"reference/view/#view.StructureView.highlight","title":"<code>highlight(resi)</code>","text":"<p>Highlights a residue in the structure.</p> <p>Parameters:</p> Name Type Description Default <code>resi</code> <code>int</code> <p>Residue number to highlight.</p> required <p>Returns:</p> Type Description <code>StructureView</code> <p>The updated StructureView object.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def highlight(self, resi: int) -&gt; StructureView:\n    \"\"\"\n    Highlights a residue in the structure.\n\n    Args:\n        resi: Residue number to highlight.\n\n    Returns:\n        The updated StructureView object.\n    \"\"\"\n    param = self.get_query_param(resi)\n    data = self._augment_chain([param])\n    self.view.highlight = {\"data\": data}\n\n    return self\n</code></pre>"},{"location":"reference/view/#view.StructureView.non_overlapping_peptides","title":"<code>non_overlapping_peptides(peptides, colors=None, non_selected_color='lightgray')</code>","text":"<p>Selects a set of non-overlapping peptides to display on the structure. Starts with the first peptide and successively adds peptides that do not overlap with already selected peptides.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>Peptides object or DataFrame containing peptide data.</p> required <code>colors</code> <code>list[str] | None</code> <p>List of colors to cycle through for different peptides.</p> <code>None</code> <code>non_selected_color</code> <code>str</code> <p>Color for non-covered regions.</p> <code>'lightgray'</code> <p>Returns:</p> Type Description <p>The updated StructureView object.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def non_overlapping_peptides(\n    self,\n    peptides: nw.DataFrame,\n    colors: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n):\n    \"\"\"Selects a set of non-overlapping peptides to display on the structure. Starts with the first\n    peptide and successively adds peptides that do not overlap with already selected peptides.\n\n    Args:\n        peptides: Peptides object or DataFrame containing peptide data.\n        colors: List of colors to cycle through for different peptides.\n        non_selected_color: Color for non-covered regions.\n\n    Returns:\n        The updated StructureView object.\n\n    \"\"\"\n\n    intervals = non_overlapping_peptides(peptides)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n    )\n\n    cdata = []\n    tdata = []\n    for (start, end), color in zip(intervals, itertools.cycle(colors)):\n        cdata.append(self.get_query_param_range(start, end, color=color))\n        df_f = peptides.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end)).to_native()\n        sequence = df_f[\"sequence\"].unique().first()\n        tdata.append(self.get_query_param_range(start, end, tooltip=f\"Peptide: {sequence}\"))\n\n    color_data = {\n        \"data\": self._augment_chain(cdata),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self.view.color_data = color_data\n    self.view.tooltips = {\"data\": self._augment_chain(tdata)}\n    return self\n</code></pre>"},{"location":"reference/view/#view.StructureView.peptide_coverage","title":"<code>peptide_coverage(peptides, color='darkgreen', non_selected_color='lightgray')</code>","text":"<p>Plots peptide coverage on the structure.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>Peptides object or DataFrame containing peptide data.</p> required <code>color</code> <code>str</code> <p>Color for the covered regions.</p> <code>'darkgreen'</code> <code>non_selected_color</code> <code>str</code> <p>Color for non-covered regions.</p> <code>'lightgray'</code> <p>Returns:</p> Type Description <p>The updated StructureView object.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def peptide_coverage(\n    self,\n    peptides: nw.DataFrame,\n    color: str = \"darkgreen\",\n    non_selected_color: str = \"lightgray\",\n):\n    \"\"\"\n    Plots peptide coverage on the structure.\n\n    Args:\n        peptides: Peptides object or DataFrame containing peptide data.\n        color: Color for the covered regions.\n        non_selected_color: Color for non-covered regions.\n\n    Returns:\n        The updated StructureView object.\n    \"\"\"\n    intervals = contiguous_peptides(peptides)\n\n    data = [self.get_query_param_range(start, end, color=color) for start, end in intervals]\n    color_data = {\n        \"data\": self._augment_chain(data),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self.view.color_data = color_data\n    self.view.tooltips = None\n    return self\n</code></pre>"},{"location":"reference/view/#view.StructureView.peptide_redundancy","title":"<code>peptide_redundancy(peptides, colors=None, clip=None, non_selected_color='lightgray')</code>","text":"<p>Colors residues by peptide redundancy.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>Peptides DataFrame containing peptide data.</p> required <code>colors</code> <code>list[str] | None</code> <p>List of colors to use for different redundancy levels.</p> <code>None</code> <code>clip</code> <code>Optional[int]</code> <p>Optional maximum redundancy value for clipping.</p> <code>None</code> <code>non_selected_color</code> <code>str</code> <p>Color for non-covered regions.</p> <code>'lightgray'</code> <p>Returns:</p> Type Description <p>The updated StructureView object.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def peptide_redundancy(\n    self,\n    peptides: nw.DataFrame,\n    colors: list[str] | None = None,\n    clip: Optional[int] = None,\n    non_selected_color: str = \"lightgray\",\n):\n    \"\"\"Colors residues by peptide redundancy.\n\n    Args:\n        peptides: Peptides DataFrame containing peptide data.\n        colors: List of colors to use for different redundancy levels.\n        clip: Optional maximum redundancy value for clipping.\n        non_selected_color: Color for non-covered regions.\n\n    Returns:\n        The updated StructureView object.\n\n    \"\"\"\n    r_number, redundancy = peptide_redundancy(peptides)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n    )\n\n    if clip:\n        vals = ((redundancy.clip(None, clip) / clip) * (len(colors) - 1)).astype(int)\n    else:\n        vals = ((redundancy / redundancy.max()) * (len(colors) - 1)).astype(int)\n\n    data = []\n    tooltips = []\n    for rn, rv, rv_clip in zip(r_number, redundancy, vals):\n        tooltips.append(self.get_query_param(int(rn), tooltip=f\"Redundancy: {rv} peptides\"))\n\n        if rv == 0:\n            continue\n\n        color_elem = self.get_query_param(int(rn), color=colors[rv_clip])\n        data.append(color_elem)\n\n    color_data = {\n        \"data\": self._augment_chain(data),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self.view.color_data = color_data\n    self.view.tooltips = {\"data\": self._augment_chain(tooltips)}\n    return self\n</code></pre>"},{"location":"reference/view/#view.summarize_peptide_df","title":"<code>summarize_peptide_df(df)</code>","text":"<p>Summarize a peptide DataFrame.</p> Source code in <code>hdxms_datasets/view.py</code> <pre><code>def summarize_peptide_df(df: nw.DataFrame) -&gt; str:\n    \"\"\"\n    Summarize a peptide DataFrame.\n\n    \"\"\"\n    exposures = df[\"exposure\"].unique().to_list()\n    peptides = df.select([\"start\", \"end\"]).unique()\n\n    mean_length = peptides.with_columns((peptides[\"end\"] - peptides[\"start\"] + 1).alias(\"length\"))[\n        \"length\"\n    ].mean()\n\n    s = []\n    s.append(f\"Number of unique peptides: {len(peptides)}\")\n    s.append(f\"Mean peptide length: {mean_length:.2f}\")\n    s.append(f\"Exposures: {exposures}\")\n    s.append(f\"Total number of data points: {len(df)}\")\n\n    if \"n_clusters\" in df.columns:\n        n_clusters = df[\"n_clusters\"].mean()\n        s.append(f\"Mean number of clusters per peptide/exposure: {n_clusters}\")\n\n    if \"n_replicates\" in df.columns:\n        n_replicates = df[\"n_replicates\"].mean()\n        s.append(f\"Mean number of replicates per peptide/exposure: {n_replicates}\")\n\n    if \"n_charges\" in df.columns:\n        n_charges = df[\"n_charges\"].mean()\n        s.append(f\"Mean number of charge states per peptide/exposure: {n_charges}\")\n\n    return \"\\n\".join(s)\n</code></pre>"},{"location":"reference/migration/v020/","title":"v020","text":"<p>Functions to help with migration from v020 datasets.</p>"},{"location":"reference/migration/v020/#migration.v020.get_peptides","title":"<code>get_peptides(spec, data_files, root_dir=Path.cwd(), **kwargs)</code>","text":"<p>Get peptides from the spec</p> Source code in <code>hdxms_datasets/migration/v020.py</code> <pre><code>def get_peptides(\n    spec: dict, data_files: dict, root_dir: Path = Path.cwd(), **kwargs\n) -&gt; list[Peptides]:\n    \"\"\"Get peptides from the spec\"\"\"\n\n    peptides = []\n    for deut_type, p_spec in spec.items():\n        data_file = data_files[p_spec[\"data_file\"]]\n        p = Peptides(\n            data_file=root_dir / data_file[\"filename\"],\n            data_format=data_file[\"format\"],\n            deuteration_type=deut_type,\n            filters=p_spec.get(\"filters\", {}),\n            **kwargs,\n            **get_metadata(p_spec),\n        )\n\n        peptides.append(p)\n\n    return peptides\n</code></pre>"},{"location":"reference/stable/v020/backend/","title":"backend","text":""},{"location":"reference/stable/v020/backend/#stable.v020.backend.get_backend","title":"<code>get_backend()</code>","text":"<p>Returns the backend used for data handling.</p> Source code in <code>hdxms_datasets/stable/v020/backend.py</code> <pre><code>def get_backend():\n    \"\"\"\n    Returns the backend used for data handling.\n    \"\"\"\n    try:\n        import polars  # NOQA: F401 # type: ignore[import]\n\n        return \"polars\"\n    except ImportError:\n        pass\n\n    try:\n        import pandas  # NOQA: F401 # type: ignore[import]\n\n        return \"pandas\"\n    except ImportError:\n        pass\n\n    try:\n        import modin  # NOQA: F401 # type: ignore[import]\n\n        return \"modin\"\n    except ImportError:\n        pass\n\n    try:\n        import pyarrow  # NOQA: F401 # type: ignore[import]\n\n        return \"pyarrow\"\n    except ImportError:\n        pass\n\n    raise ImportError(\"No suitable backend found. Please install pandas, polars, pyarrow or modin.\")\n</code></pre>"},{"location":"reference/stable/v020/convert/","title":"convert","text":""},{"location":"reference/stable/v020/convert/#stable.v020.convert.convert_rt","title":"<code>convert_rt(rt_str)</code>","text":"<p>convert hd examiner rt string to float example: \"7.44-7.65\" -&gt; 7.545</p> Source code in <code>hdxms_datasets/stable/v020/convert.py</code> <pre><code>def convert_rt(rt_str: str) -&gt; float:\n    \"\"\"convert hd examiner rt string to float\n    example: \"7.44-7.65\" -&gt; 7.545\n    \"\"\"\n    lower, upper = rt_str.split(\"-\")\n    mean = (float(lower) + float(upper)) / 2.0\n    return mean\n</code></pre>"},{"location":"reference/stable/v020/datasets/","title":"datasets","text":""},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet","title":"<code>DataSet</code>  <code>dataclass</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass DataSet:\n    data_id: str\n    \"\"\"Unique identifier for the dataset\"\"\"\n\n    states: dict[str, DataState]\n\n    metadata: dict = field(default_factory=dict)  # author, publication, etc\n\n    def get_state(self, state: str | int) -&gt; DataState:\n        \"\"\"\n        Get a specific state by name or index\n        \"\"\"\n        if isinstance(state, int):\n            return self.states[list(self.states.keys())[state]]\n        elif isinstance(state, str):\n            return self.states[state]\n        else:\n            raise TypeError(f\"Invalid type {type(state)} for state {state!r}\")\n\n    @classmethod\n    def from_spec(\n        cls,\n        hdx_spec: dict,\n        data_dir: Path,\n        data_id: Optional[str] = None,\n        metadata: Optional[dict] = None,\n    ):\n        data_id = data_id or uuid.uuid4().hex\n        data_files = parse_data_files(hdx_spec[\"data_files\"], data_dir)\n\n        peptide_table_files = {\n            k: f for k, f in data_files.items() if isinstance(f, PeptideTableFile)\n        }\n        peptides = parse_peptides(hdx_spec[\"peptides\"], peptide_table_files)\n\n        structure_file = next(\n            (f for f in data_files.values() if isinstance(f, StructureFile)), None\n        )\n\n        protein_spec = hdx_spec[\"protein\"]\n\n        for state_name, state_peptide_dict in peptides.items():\n            # Get protein information for this state\n            try:\n                protein_info = protein_spec[state_name]\n            except KeyError:\n                if ALLOW_MISSING_FIELDS:\n                    # Generate minimal protein info from peptides\n                    # take partially deuterated peptides as default,\n                    # if not available, take the first one\n                    peptide_df = state_peptide_dict.get(\n                        \"partially_deuterated\", next(iter(state_peptide_dict.values()))\n                    ).load()\n                    protein_info = default_protein_info(peptide_df)\n                    warnings.warn(\n                        f\"Generated minimal protein info for state '{state_name}'. \"\n                        f\"This is not recommended for production use.\"\n                    )\n                else:\n                    raise KeyError(\n                        f\"No protein information found for state '{state_name}'. \"\n                        f\"Use 'allow_missing_fields()' context manager to generate minimal info.\"\n                    )\n\n            structure_spec = hdx_spec.get(\"structures\", {}).get(\n                protein_info.get(\"structure\", \"\"), None\n            )\n            if structure_spec is None or structure_file is None:\n                if ALLOW_MISSING_FIELDS:\n                    # If no structure is specified, use a null structure\n                    structure = Structure.null_structure()\n                else:\n                    raise ValueError(\n                        f\"No structure information found for state '{state_name}'. \"\n                        f\"Use 'allow_missing_fields()' context manager to generate a null structure.\"\n                    )\n            else:\n                structure = Structure(\n                    data_file=structure_file,\n                    chain=structure_spec.get(\"chain\", []),  # empty list for all chains\n                    auth_residue_numbers=structure_spec.get(\"auth_residue_numbers\", False),\n                )\n\n            # Create and store the DataState object\n            self.states[state_name] = DataState(\n                name=state_name,\n                peptides=state_peptide_dict,\n                protein=protein_info,\n                structure=structure,\n            )\n\n        return cls(\n            data_id=data_id,\n            data_files=data_files,\n            hdx_specification=hdx_spec,\n            metadata=metadata or {},\n        )\n\n    @property\n    def protein_spec(self) -&gt; dict[str, ProteinInfo]:\n        \"\"\"Access the protein section of the specification\"\"\"\n        return self.hdx_specification.get(\"protein\", {})\n\n    @property\n    def peptide_spec(self) -&gt; dict:\n        return self.hdx_specification[\"peptides\"]\n\n    @property\n    def peptides_per_state(self) -&gt; dict[str, list[str]]:\n        \"\"\"Dictionary of state names and list of peptide sets for each state\"\"\"\n        return {state: list(spec) for state, spec in self.peptide_spec.items()}\n\n    def describe(\n        self,\n        peptide_template: Optional[str] = \"Total peptides: $num_peptides, timepoints: $timepoints\",\n        return_type: Union[Type[str], type[dict]] = str,\n    ) -&gt; Union[dict, str]:\n        def fmt_t(val: str | float | int) -&gt; str:\n            if isinstance(val, str):\n                return val\n            elif isinstance(val, (int, float)):\n                return f\"{val:.1f}\"\n            else:\n                raise TypeError(f\"Invalid type {type(val)} for value {val!r}\")\n\n        output_dict = {}\n        for state in self.states.values():\n            state_desc = {}\n            if peptide_template:\n                for peptides_types, peptides in state.peptides.items():\n                    peptide_df = peptides.load()\n                    timepoints = peptide_df[\"exposure\"].unique()\n                    mapping = {\n                        \"num_peptides\": len(peptide_df),\n                        \"num_timepoints\": len(timepoints),\n                        \"timepoints\": \", \".join([fmt_t(t) for t in timepoints]),\n                    }\n                    mapping[\"timepoints\"]\n                    state_desc[peptides_types] = Template(peptide_template).substitute(**mapping)\n\n            output_dict[state.name] = state_desc\n\n        if return_type is str:\n            return yaml.dump(output_dict, sort_keys=False)\n        elif return_type is dict:\n            return output_dict\n        else:\n            raise TypeError(f\"Invalid return type {return_type!r}\")\n\n    def cite(self) -&gt; str:\n        \"\"\"\n        Returns citation information\n        \"\"\"\n\n        raise NotImplementedError(\"Citation information is not yet implemented\")\n        try:\n            return self.metadata[\"publications\"]\n        except KeyError:\n            return \"No publication information available\"\n\n    def __len__(self) -&gt; int:\n        return len(self.states)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.data_id","title":"<code>data_id</code>  <code>instance-attribute</code>","text":"<p>Unique identifier for the dataset</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.peptides_per_state","title":"<code>peptides_per_state</code>  <code>property</code>","text":"<p>Dictionary of state names and list of peptide sets for each state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.protein_spec","title":"<code>protein_spec</code>  <code>property</code>","text":"<p>Access the protein section of the specification</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.cite","title":"<code>cite()</code>","text":"<p>Returns citation information</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def cite(self) -&gt; str:\n    \"\"\"\n    Returns citation information\n    \"\"\"\n\n    raise NotImplementedError(\"Citation information is not yet implemented\")\n    try:\n        return self.metadata[\"publications\"]\n    except KeyError:\n        return \"No publication information available\"\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataSet.get_state","title":"<code>get_state(state)</code>","text":"<p>Get a specific state by name or index</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_state(self, state: str | int) -&gt; DataState:\n    \"\"\"\n    Get a specific state by name or index\n    \"\"\"\n    if isinstance(state, int):\n        return self.states[list(self.states.keys())[state]]\n    elif isinstance(state, str):\n        return self.states[state]\n    else:\n        raise TypeError(f\"Invalid type {type(state)} for state {state!r}\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState","title":"<code>DataState</code>  <code>dataclass</code>","text":"<p>Encapsulates all data for a specific protein state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass DataState:\n    \"\"\"Encapsulates all data for a specific protein state\"\"\"\n\n    name: str\n    \"\"\"Name of the state\"\"\"\n\n    peptides: dict[str, Peptides]\n    \"\"\"Dictionary of peptide sets for this state\"\"\"\n\n    protein: ProteinInfo\n    \"\"\"Protein information for this state\"\"\"\n\n    structure: Structure\n    \"\"\"Optional structure file information for this state\"\"\"\n\n    def get_peptides(self, peptide_set: str) -&gt; Peptides:\n        \"\"\"Get a specific peptide set\"\"\"\n        try:\n            return self.peptides[peptide_set]\n        except KeyError:\n            raise KeyError(f\"Peptide set '{peptide_set}' not found in state '{self.name}'\")\n\n    def get_sequence(self) -&gt; str:\n        \"\"\"Get the protein sequence for this state\"\"\"\n        return self.protein[\"sequence\"]\n\n    def get_protein_property(self, property_name: str) -&gt; Any:\n        \"\"\"Get a specific protein property\"\"\"\n        try:\n            return self.protein[property_name]\n        except KeyError:\n            raise KeyError(f\"Property '{property_name}' not found in state '{self.name}'\")\n\n    def compute_uptake_metrics(self) -&gt; nw.DataFrame:\n        \"\"\"Compute uptake metrics for this state\"\"\"\n        peptide_types = list(self.peptides.keys())\n\n        if \"fully_deuterated\" in peptide_types:\n            fd = self.peptides[\"fully_deuterated\"].load()\n        else:\n            fd = None\n\n        if \"non_deuterated\" in peptide_types:\n            nd = self.peptides[\"non_deuterated\"].load()\n        else:\n            nd = None\n\n        pd = self.peptides[\"partially_deuterated\"].load()\n\n        merged = process.merge_peptides(\n            partially_deuterated=pd, fully_deuterated=fd, non_deuterated=nd\n        )\n        return process.compute_uptake_metrics(merged)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>Name of the state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.peptides","title":"<code>peptides</code>  <code>instance-attribute</code>","text":"<p>Dictionary of peptide sets for this state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.protein","title":"<code>protein</code>  <code>instance-attribute</code>","text":"<p>Protein information for this state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.structure","title":"<code>structure</code>  <code>instance-attribute</code>","text":"<p>Optional structure file information for this state</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.compute_uptake_metrics","title":"<code>compute_uptake_metrics()</code>","text":"<p>Compute uptake metrics for this state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def compute_uptake_metrics(self) -&gt; nw.DataFrame:\n    \"\"\"Compute uptake metrics for this state\"\"\"\n    peptide_types = list(self.peptides.keys())\n\n    if \"fully_deuterated\" in peptide_types:\n        fd = self.peptides[\"fully_deuterated\"].load()\n    else:\n        fd = None\n\n    if \"non_deuterated\" in peptide_types:\n        nd = self.peptides[\"non_deuterated\"].load()\n    else:\n        nd = None\n\n    pd = self.peptides[\"partially_deuterated\"].load()\n\n    merged = process.merge_peptides(\n        partially_deuterated=pd, fully_deuterated=fd, non_deuterated=nd\n    )\n    return process.compute_uptake_metrics(merged)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.get_peptides","title":"<code>get_peptides(peptide_set)</code>","text":"<p>Get a specific peptide set</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_peptides(self, peptide_set: str) -&gt; Peptides:\n    \"\"\"Get a specific peptide set\"\"\"\n    try:\n        return self.peptides[peptide_set]\n    except KeyError:\n        raise KeyError(f\"Peptide set '{peptide_set}' not found in state '{self.name}'\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.get_protein_property","title":"<code>get_protein_property(property_name)</code>","text":"<p>Get a specific protein property</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_protein_property(self, property_name: str) -&gt; Any:\n    \"\"\"Get a specific protein property\"\"\"\n    try:\n        return self.protein[property_name]\n    except KeyError:\n        raise KeyError(f\"Property '{property_name}' not found in state '{self.name}'\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.DataState.get_sequence","title":"<code>get_sequence()</code>","text":"<p>Get the protein sequence for this state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_sequence(self) -&gt; str:\n    \"\"\"Get the protein sequence for this state\"\"\"\n    return self.protein[\"sequence\"]\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar","title":"<code>PDBeMolstar</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>class PDBeMolstar:\n    def __init__(self, structure: Structure, **kwargs: Any):\n        \"\"\"\n        Initialize the PDBeMolstar visualization namespace.\n\n        Args:\n            structure: The structure to visualize.\n            **kwargs: Additional keyword arguments for customization.\n        \"\"\"\n        self.structure = structure\n        self._molstar_kwargs = kwargs\n\n    @property\n    def residue_name(self) -&gt; str:\n        \"\"\"\n        Returns the residue name based on whether auth residue numbers are used.\n        \"\"\"\n        return \"auth_residue_number\" if self.structure.auth_residue_numbers else \"residue_number\"\n\n    @property\n    def chain_name(self) -&gt; str:\n        \"\"\"\n        Returns the chain name based on whether auth chain labels are used.\n        \"\"\"\n        return \"auth_asym_id\" if self.structure.auth_chain_labels else \"struct_asym_id\"\n\n    def show(self, hide_water=True, **kwargs) -&gt; PDBeMolstar:\n        from ipymolstar import PDBeMolstar\n\n        molstar_kwargs = {**self._molstar_kwargs, **kwargs}\n        return PDBeMolstar(\n            custom_data=self.structure.data_file.pdbemolstar_custom_data(),\n            hide_water=hide_water,\n            **molstar_kwargs,\n        )\n\n    def color_peptide(\n        self, start: int, end: int, color: str = \"red\", non_selected_color: str = \"lightgray\"\n    ) -&gt; PDBeMolstar:\n        c_dict = {\n            \"start_\" + self.residue_name: start,\n            \"end_\" + self.residue_name: end,\n            \"color\": color,\n        }\n\n        data = self._augment_chain([c_dict])\n\n        color_data = {\n            \"data\": data,\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = None\n        return self\n\n    def peptide_coverage(\n        self, df, start=\"start\", end=\"end\", color=\"red\", non_selected_color: str = \"lightgray\"\n    ) -&gt; PDBeMolstar:\n        intervals = contiguous_peptides(df, start=start, end=end)\n\n        data = []\n        for start, end in intervals:\n            elem = {\n                f\"start_{self.residue_name}\": start,\n                f\"end_{self.residue_name}\": end,\n                \"color\": color,\n            }\n            data.append(elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = None\n        return self\n\n    def non_overlapping_peptides(\n        self,\n        df,\n        start=\"start\",\n        end=\"end\",\n        colors: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; PDBeMolstar:\n        \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n        intervals = non_overlapping_peptides(df, start=start, end=end)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n        )\n\n        data = []\n        for (start, end), color in zip(intervals, itertools.cycle(colors)):\n            elem = {\n                f\"start_{self.residue_name}\": start,\n                f\"end_{self.residue_name}\": end,\n                \"color\": color,\n            }\n            data.append(elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = None\n        return self\n\n    def peptide_redundancy(\n        self,\n        df,\n        start=\"start\",\n        end=\"end\",\n        colors: list[str] | None = None,\n        non_selected_color: str = \"lightgray\",\n    ) -&gt; PDBeMolstar:\n        \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n        r_number, redundancy = peptide_redundancy(df, start=start, end=end)\n\n        colors = (\n            colors\n            if colors is not None\n            else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n        )\n        color_lut = {i + 1: colors[i] for i in range(len(colors))}\n\n        data = []\n        tooltips = []\n        for rn, rv in zip(r_number, redundancy.clip(0, len(colors) - 1)):\n            tooltips.append(\n                {\n                    f\"{self.residue_name}\": int(rn),\n                    \"tooltip\": f\"Redundancy: {rv} peptides\",\n                }\n            )\n\n            if rv == 0:\n                continue\n            color_elem = {\n                f\"{self.residue_name}\": int(rn),\n                \"color\": color_lut[rv],\n            }\n            data.append(color_elem)\n\n        color_data = {\n            \"data\": self._augment_chain(data),\n            \"nonSelectedColor\": non_selected_color,\n        }\n\n        self._molstar_kwargs[\"color_data\"] = color_data\n        self._molstar_kwargs[\"tooltips\"] = {\"data\": self._augment_chain(tooltips)}\n        return self\n\n    def _augment_chain(self, data: list[dict[str, ValueType]]) -&gt; list[dict[str, ValueType]]:\n        \"\"\"augment a list of data with chain information\"\"\"\n        if self.structure.chain:\n            aug_data = []\n            for elem, chain in itertools.product(data, self.structure.chain):\n                aug_data.append(elem | {self.chain_name: chain})\n        else:\n            aug_data = data\n\n        return aug_data\n\n    def _repr_mimebundle_(self, include=None, exclude=None):\n        return self.show()._repr_mimebundle_(\n            include=include, exclude=exclude\n        )  # or however ipymolstar handles display\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.chain_name","title":"<code>chain_name</code>  <code>property</code>","text":"<p>Returns the chain name based on whether auth chain labels are used.</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.residue_name","title":"<code>residue_name</code>  <code>property</code>","text":"<p>Returns the residue name based on whether auth residue numbers are used.</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.__init__","title":"<code>__init__(structure, **kwargs)</code>","text":"<p>Initialize the PDBeMolstar visualization namespace.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Structure</code> <p>The structure to visualize.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for customization.</p> <code>{}</code> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def __init__(self, structure: Structure, **kwargs: Any):\n    \"\"\"\n    Initialize the PDBeMolstar visualization namespace.\n\n    Args:\n        structure: The structure to visualize.\n        **kwargs: Additional keyword arguments for customization.\n    \"\"\"\n    self.structure = structure\n    self._molstar_kwargs = kwargs\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.non_overlapping_peptides","title":"<code>non_overlapping_peptides(df, start='start', end='end', colors=None, non_selected_color='lightgray')</code>","text":"<p>selects a set of non-overlapping peptides to display on the structure</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def non_overlapping_peptides(\n    self,\n    df,\n    start=\"start\",\n    end=\"end\",\n    colors: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n) -&gt; PDBeMolstar:\n    \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n    intervals = non_overlapping_peptides(df, start=start, end=end)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#1B9E77\", \"#D95F02\", \"#7570B3\", \"#E7298A\", \"#66A61E\", \"#E6AB02\"]\n    )\n\n    data = []\n    for (start, end), color in zip(intervals, itertools.cycle(colors)):\n        elem = {\n            f\"start_{self.residue_name}\": start,\n            f\"end_{self.residue_name}\": end,\n            \"color\": color,\n        }\n        data.append(elem)\n\n    color_data = {\n        \"data\": self._augment_chain(data),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self._molstar_kwargs[\"color_data\"] = color_data\n    self._molstar_kwargs[\"tooltips\"] = None\n    return self\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PDBeMolstar.peptide_redundancy","title":"<code>peptide_redundancy(df, start='start', end='end', colors=None, non_selected_color='lightgray')</code>","text":"<p>selects a set of non-overlapping peptides to display on the structure</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def peptide_redundancy(\n    self,\n    df,\n    start=\"start\",\n    end=\"end\",\n    colors: list[str] | None = None,\n    non_selected_color: str = \"lightgray\",\n) -&gt; PDBeMolstar:\n    \"\"\"selects a set of non-overlapping peptides to display on the structure\"\"\"\n    r_number, redundancy = peptide_redundancy(df, start=start, end=end)\n\n    colors = (\n        colors\n        if colors is not None\n        else [\"#C6DBEF\", \"#9ECAE1\", \"#6BAED6\", \"#4292C6\", \"#2171B5\", \"#08519C\", \"#08306B\"]\n    )\n    color_lut = {i + 1: colors[i] for i in range(len(colors))}\n\n    data = []\n    tooltips = []\n    for rn, rv in zip(r_number, redundancy.clip(0, len(colors) - 1)):\n        tooltips.append(\n            {\n                f\"{self.residue_name}\": int(rn),\n                \"tooltip\": f\"Redundancy: {rv} peptides\",\n            }\n        )\n\n        if rv == 0:\n            continue\n        color_elem = {\n            f\"{self.residue_name}\": int(rn),\n            \"color\": color_lut[rv],\n        }\n        data.append(color_elem)\n\n    color_data = {\n        \"data\": self._augment_chain(data),\n        \"nonSelectedColor\": non_selected_color,\n    }\n\n    self._molstar_kwargs[\"color_data\"] = color_data\n    self._molstar_kwargs[\"tooltips\"] = {\"data\": self._augment_chain(tooltips)}\n    return self\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PeptideMetadata","title":"<code>PeptideMetadata</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for peptide metadata</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>class PeptideMetadata(TypedDict):\n    \"\"\"TypedDict for peptide metadata\"\"\"\n\n    pH: float  # pH of the experiment (pH read, uncorrected)\n    temperature: float  # Temperature of the experiment (K)\n    d_percentage: float  # Deuteration percentage\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PeptideTableFile","title":"<code>PeptideTableFile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataFile</code></p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass(frozen=True)\nclass PeptideTableFile(DataFile):\n    name: str\n\n    filepath_or_buffer: Union[Path, StringIO, BytesIO]\n\n    format: Optional[HDXFormat] = None\n\n    extension: Optional[str] = None\n    \"\"\"File extension, e.g. .csv, in case of a file-like object\"\"\"\n\n    def __post_init__(self):\n        if self.format is None:\n            df = self.read()\n            fmt = identify_format(df, exact=False)\n            self.__dict__[\"format\"] = fmt\n\n    def read(self) -&gt; nw.DataFrame:\n        if isinstance(self.filepath_or_buffer, (StringIO, BytesIO)):\n            extension = self.extension\n            assert isinstance(extension, str), \"File-like object must have an extension\"\n        else:\n            extension = self.filepath_or_buffer.suffix[1:]\n\n        if extension == \"csv\":\n            return read_csv(self.filepath_or_buffer)\n        else:\n            raise ValueError(f\"Invalid file extension {self.extension!r}\")\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.PeptideTableFile.extension","title":"<code>extension = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>File extension, e.g. .csv, in case of a file-like object</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Peptides","title":"<code>Peptides</code>  <code>dataclass</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass Peptides:\n    data_file: PeptideTableFile\n    filters: dict[str, ValueType | list[ValueType]] = field(default_factory=dict)\n    metadata: PeptideMetadata | None = None\n\n    def load(\n        self,\n        convert: bool = True,\n        aggregate: bool | None = None,\n        sort_rows: bool = True,\n        sort_columns: bool = True,\n        drop_null: bool = True,\n    ) -&gt; nw.DataFrame:\n        df = process.apply_filters(self.data_file.read(), **self.filters)\n\n        is_aggregated = getattr(self.data_file.format, \"aggregated\", False)\n        if aggregate is None:\n            aggregate = not is_aggregated\n\n        if aggregate and is_aggregated:\n            warnings.warn(\"Data format is pre-aggregated. Aggregation will be skipped.\")\n            aggregate = False\n\n        if not convert and aggregate:\n            warnings.warn(\"Cannot aggregate data without conversion. Aggeregation will be skipped.\")\n            aggregate = False\n\n        if not convert and sort_rows:\n            warnings.warn(\"Cannot sort rows without conversion. Sorting will be skipped.\")\n            sort_rows = False\n\n        if not convert and sort_columns:\n            warnings.warn(\"Cannot sort columns without conversion. Sorting will be skipped.\")\n            sort_columns = False\n\n        if convert:\n            assert self.data_file.format is not None, (\n                \"Data file format must be set before conversion\"\n            )\n            df = self.data_file.format.convert(df)\n\n        if aggregate:\n            df = process.aggregate(df)\n\n        if drop_null:\n            df = process.drop_null_columns(df)\n\n        if sort_rows:\n            df = process.sort_rows(df)\n\n        if sort_columns:\n            df = process.sort_columns(df)\n\n        return df\n\n    def get_temperature(self) -&gt; Optional[float]:\n        \"\"\"Get the temperature of the experiment\"\"\"\n\n        if self.metadata is None:\n            return None\n        elif \"temperature\" not in self.metadata:\n            return None\n\n        temperature = self.metadata[\"temperature\"]\n        return temperature\n\n    def get_pH(self) -&gt; Optional[float]:\n        \"\"\"Get the pH of the experiment\"\"\"\n        if self.metadata is None:\n            return None\n        elif \"pH\" not in self.metadata:\n            return None\n        return self.metadata[\"pH\"]\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Peptides.get_pH","title":"<code>get_pH()</code>","text":"<p>Get the pH of the experiment</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_pH(self) -&gt; Optional[float]:\n    \"\"\"Get the pH of the experiment\"\"\"\n    if self.metadata is None:\n        return None\n    elif \"pH\" not in self.metadata:\n        return None\n    return self.metadata[\"pH\"]\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Peptides.get_temperature","title":"<code>get_temperature()</code>","text":"<p>Get the temperature of the experiment</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def get_temperature(self) -&gt; Optional[float]:\n    \"\"\"Get the temperature of the experiment\"\"\"\n\n    if self.metadata is None:\n        return None\n    elif \"temperature\" not in self.metadata:\n        return None\n\n    temperature = self.metadata[\"temperature\"]\n    return temperature\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.ProteinInfo","title":"<code>ProteinInfo</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for protein information in a state</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>class ProteinInfo(TypedDict):\n    \"\"\"TypedDict for protein information in a state\"\"\"\n\n    sequence: str  # Amino acid sequence\n    n_term: int  # N-terminal residue number\n    c_term: int  # C-terminal residue number\n    mutations: NotRequired[list[str]]  # Optional list of mutations\n    oligomeric_state: NotRequired[int]  # Optional oligomeric state\n    ligand: NotRequired[str]  # Optional bound ligand information\n    uniprot_id: NotRequired[str]  # Optional UniProt ID\n    molecular_weight: NotRequired[float]  # Optional molecular weight in Da\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Structure","title":"<code>Structure</code>  <code>dataclass</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass\nclass Structure:\n    data_file: StructureFile\n    chain: list[str] = field(default_factory=list)  # empty list for all chains\n    auth_residue_numbers: bool = field(default=False)\n    auth_chain_labels: bool = field(default=False)\n\n    _molstar_kwargs: dict[str, Any] = field(default_factory=dict, init=False)\n\n    def pdbemolstar(self, **kwargs):\n        \"\"\"Return a PDBeMolstar visualization namespace.\"\"\"\n        return PDBeMolstar(self, **kwargs)\n\n    @classmethod\n    def null_structure(cls) -&gt; Structure:\n        \"\"\"\n        Returns a null structure with no data.\n        This is useful for cases where no structure is available.\n        \"\"\"\n        return cls(\n            data_file=StructureFile(\n                name=\"null\",\n                filepath_or_buffer=BytesIO(),\n                format=\"null\",\n                extension=\".null\",\n            ),\n            auth_residue_numbers=False,\n        )\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Structure.null_structure","title":"<code>null_structure()</code>  <code>classmethod</code>","text":"<p>Returns a null structure with no data. This is useful for cases where no structure is available.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@classmethod\ndef null_structure(cls) -&gt; Structure:\n    \"\"\"\n    Returns a null structure with no data.\n    This is useful for cases where no structure is available.\n    \"\"\"\n    return cls(\n        data_file=StructureFile(\n            name=\"null\",\n            filepath_or_buffer=BytesIO(),\n            format=\"null\",\n            extension=\".null\",\n        ),\n        auth_residue_numbers=False,\n    )\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.Structure.pdbemolstar","title":"<code>pdbemolstar(**kwargs)</code>","text":"<p>Return a PDBeMolstar visualization namespace.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def pdbemolstar(self, **kwargs):\n    \"\"\"Return a PDBeMolstar visualization namespace.\"\"\"\n    return PDBeMolstar(self, **kwargs)\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.StructureFile","title":"<code>StructureFile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataFile</code></p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@dataclass(frozen=True)\nclass StructureFile(DataFile):\n    name: str\n\n    filepath_or_buffer: Union[Path, BytesIO]\n\n    format: str\n\n    extension: Optional[str] = None\n    \"\"\"File extension, e.g. .pdf, in case of a file-like object\"\"\"\n\n    def pdbemolstar_custom_data(self):\n        \"\"\"\n        Returns a dictionary with custom data for PDBeMolstar visualization.\n        \"\"\"\n\n        if self.format in [\"bcif\"]:\n            binary = True\n        else:\n            binary = False\n\n        if isinstance(self.filepath_or_buffer, BytesIO):\n            data = self.filepath_or_buffer.getvalue()\n        elif isinstance(self.filepath_or_buffer, Path):\n            if self.filepath_or_buffer.is_file():\n                data = self.filepath_or_buffer.read_bytes()\n            else:\n                raise ValueError(f\"Path {self.filepath_or_buffer} is not a file.\")\n\n        return {\n            \"data\": data,\n            \"format\": self.format,\n            \"binary\": binary,\n        }\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.StructureFile.extension","title":"<code>extension = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>File extension, e.g. .pdf, in case of a file-like object</p>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.StructureFile.pdbemolstar_custom_data","title":"<code>pdbemolstar_custom_data()</code>","text":"<p>Returns a dictionary with custom data for PDBeMolstar visualization.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def pdbemolstar_custom_data(self):\n    \"\"\"\n    Returns a dictionary with custom data for PDBeMolstar visualization.\n    \"\"\"\n\n    if self.format in [\"bcif\"]:\n        binary = True\n    else:\n        binary = False\n\n    if isinstance(self.filepath_or_buffer, BytesIO):\n        data = self.filepath_or_buffer.getvalue()\n    elif isinstance(self.filepath_or_buffer, Path):\n        if self.filepath_or_buffer.is_file():\n            data = self.filepath_or_buffer.read_bytes()\n        else:\n            raise ValueError(f\"Path {self.filepath_or_buffer} is not a file.\")\n\n    return {\n        \"data\": data,\n        \"format\": self.format,\n        \"binary\": binary,\n    }\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.allow_missing_fields","title":"<code>allow_missing_fields(allow=True)</code>","text":"<p>Context manager to temporarily allow missing protein information</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>@contextmanager\ndef allow_missing_fields(allow=True):\n    \"\"\"Context manager to temporarily allow missing protein information\"\"\"\n    global ALLOW_MISSING_FIELDS\n    old_value = ALLOW_MISSING_FIELDS\n    ALLOW_MISSING_FIELDS = allow\n    try:\n        yield\n    finally:\n        ALLOW_MISSING_FIELDS = old_value\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.create_dataset","title":"<code>create_dataset(target_dir, author_name, tag=None, template_dir=TEMPLATE_DIR)</code>","text":"<p>Create a dataset in the specified target directory.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <code>Path</code> <p>The directory where the dataset will be created.</p> required <code>author_name</code> <code>str</code> <p>The name of the author of the dataset.</p> required <code>tag</code> <code>Optional[str]</code> <p>An optional tag to append to the directory name. Defaults to None.</p> <code>None</code> <code>template_dir</code> <code>Path</code> <p>The directory containing the template files for the dataset. Defaults to TEMPLATE_DIR.</p> <code>TEMPLATE_DIR</code> <p>Returns:</p> Type Description <code>str</code> <p>The id of the created dataset.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def create_dataset(\n    target_dir: Path,\n    author_name: str,\n    tag: Optional[str] = None,\n    template_dir: Path = TEMPLATE_DIR,\n) -&gt; str:\n    \"\"\"\n    Create a dataset in the specified target directory.\n\n    Args:\n        target_dir: The directory where the dataset will be created.\n        author_name: The name of the author of the dataset.\n        tag: An optional tag to append to the directory name. Defaults to None.\n        template_dir: The directory containing the template files for the dataset. Defaults to TEMPLATE_DIR.\n\n    Returns:\n        The id of the created dataset.\n\n    \"\"\"\n    dirname = str(int(time.time()))\n\n    if tag:\n        dirname += f\"_{tag}\"\n\n    dirname += f\"_{author_name}\"\n\n    target_dir.mkdir(parents=True, exist_ok=True)\n    target_dir = target_dir / dirname\n\n    shutil.copytree(template_dir, target_dir)\n\n    (target_dir / \"readme.md\").write_text(f\"# {dirname}\")\n\n    return dirname\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.parse_data_files","title":"<code>parse_data_files(data_file_spec, data_dir)</code>","text":"<p>Parse data file specifications from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_spec</code> <code>dict</code> <p>Dictionary with data file specifications.</p> required <code>data_dir</code> <code>Path</code> <p>Path to data directory.</p> required <p>Returns:</p> Type Description <code>dict[str, DataFile]</code> <p>Dictionary with parsed data file specifications.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def parse_data_files(data_file_spec: dict, data_dir: Path) -&gt; dict[str, DataFile]:\n    \"\"\"\n    Parse data file specifications from a YAML file.\n\n    Args:\n        data_file_spec: Dictionary with data file specifications.\n        data_dir: Path to data directory.\n\n    Returns:\n        Dictionary with parsed data file specifications.\n    \"\"\"\n\n    data_files = {}\n    for name, spec in data_file_spec.items():\n        fpath = Path(data_dir / spec[\"filename\"])\n\n        if spec[\"type\"] == \"structure\":\n            format = spec[\"format\"]\n            data_file = StructureFile(\n                name=name,\n                filepath_or_buffer=fpath,\n                format=format,\n                extension=fpath.suffix,\n            )\n        elif spec[\"type\"] == \"peptide_table\":\n            format = FMT_LUT.get(spec[\"format\"], None)\n            data_file = PeptideTableFile(\n                name=name,\n                filepath_or_buffer=fpath,\n                format=format,\n                extension=fpath.suffix,\n            )\n        else:\n            raise ValueError(f\"Unknown data file type {spec['type']} for {name}.\")\n\n        data_files[name] = data_file\n\n    return data_files\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.parse_peptides","title":"<code>parse_peptides(peptides_spec, data_files)</code>","text":"<p>Parse the peptides specification and return a dictionary of PeptideTableFile objects.</p> <p>Parameters:</p> Name Type Description Default <code>peptides_spec</code> <code>dict[str, Any]</code> <p>Dictionary containing peptide specifications.</p> required <code>data_files</code> <code>dict[str, PeptideTableFile]</code> <p>Dictionary of available data files.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, Peptides]]</code> <p>Dictionary of Peptides dictionaries.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def parse_peptides(\n    peptides_spec: dict[str, Any], data_files: dict[str, PeptideTableFile]\n) -&gt; dict[str, dict[str, Peptides]]:\n    \"\"\"\n    Parse the peptides specification and return a dictionary of PeptideTableFile objects.\n\n    Args:\n        peptides_spec: Dictionary containing peptide specifications.\n        data_files: Dictionary of available data files.\n\n    Returns:\n        Dictionary of Peptides dictionaries.\n    \"\"\"\n    peptides = {}\n    for state_name, state_peptides in peptides_spec.items():\n        # Build peptide dictionary for this state\n        state_peptide_dict = {}\n\n        # Process each peptide set for this state\n        for peptide_type, peptide_spec in state_peptides.items():\n            peptide_obj = Peptides(\n                data_file=data_files[peptide_spec[\"data_file\"]],\n                filters=peptide_spec[\"filters\"],\n                metadata=peptide_spec.get(\"metadata\", None),\n            )\n\n            # Add to state-specific dictionary\n            state_peptide_dict[peptide_type] = peptide_obj\n\n        peptides[state_name] = state_peptide_dict\n\n    return peptides\n</code></pre>"},{"location":"reference/stable/v020/datasets/#stable.v020.datasets.parse_structures","title":"<code>parse_structures(structures_spec, data_file)</code>","text":"<p>Parse the structures specification and return a dictionary of Structure objects.</p> <p>Parameters:</p> Name Type Description Default <code>structures_spec</code> <code>dict[str, Any]</code> <p>Dictionary containing structure specifications.</p> required <code>data_file</code> <code>StructureFile</code> <p>StructureFile object for the structure data.</p> required <p>Returns:</p> Type Description <code>dict[str, Structure]</code> <p>Dictionary of Structure objects keyed by structure name.</p> Source code in <code>hdxms_datasets/stable/v020/datasets.py</code> <pre><code>def parse_structures(\n    structures_spec: dict[str, Any], data_file: StructureFile\n) -&gt; dict[str, Structure]:\n    \"\"\"\n    Parse the structures specification and return a dictionary of Structure objects.\n\n    Args:\n        structures_spec: Dictionary containing structure specifications.\n        data_file: StructureFile object for the structure data.\n\n    Returns:\n        Dictionary of Structure objects keyed by structure name.\n    \"\"\"\n    structures = {}\n    for name, spec in structures_spec.items():\n        structure = Structure(\n            data_file=data_file,\n            chain=spec.get(\"chain\", []),  # empty list for all chains\n            auth_residue_numbers=spec.get(\"auth_residue_numbers\", False),\n        )\n        structures[name] = structure\n    return structures\n</code></pre>"},{"location":"reference/stable/v020/datavault/","title":"datavault","text":""},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.DataVault","title":"<code>DataVault</code>","text":"Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>class DataVault:\n    def __init__(self, cache_dir: Union[Path, str]):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True, parents=True)\n\n    @property\n    def datasets(self) -&gt; list[str]:\n        \"\"\"List of available datasets in the cache dir\"\"\"\n        return [d.stem for d in self.cache_dir.iterdir() if self.is_dataset(d)]\n\n    @staticmethod\n    def is_dataset(path: Path) -&gt; bool:\n        \"\"\"\n        Checks if the supplied path is a HDX-MS dataset.\n        \"\"\"\n\n        return (path / \"hdx_spec.yaml\").exists()\n\n    def clear_cache(self) -&gt; None:\n        for dir in self.cache_dir.iterdir():\n            shutil.rmtree(dir)\n\n    def get_metadata(self, data_id: str) -&gt; dict:\n        return yaml.safe_load((self.cache_dir / data_id / \"metadata.yaml\").read_text())\n\n    def load_dataset(self, data_id: str) -&gt; DataSet:\n        hdx_spec = yaml.safe_load((self.cache_dir / data_id / \"hdx_spec.yaml\").read_text())\n        dataset_metadata = self.get_metadata(data_id)\n\n        return DataSet.from_spec(\n            hdx_spec=hdx_spec,\n            data_dir=self.cache_dir / data_id,\n            data_id=data_id,\n            metadata=dataset_metadata,\n        )\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.DataVault.datasets","title":"<code>datasets</code>  <code>property</code>","text":"<p>List of available datasets in the cache dir</p>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.DataVault.is_dataset","title":"<code>is_dataset(path)</code>  <code>staticmethod</code>","text":"<p>Checks if the supplied path is a HDX-MS dataset.</p> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>@staticmethod\ndef is_dataset(path: Path) -&gt; bool:\n    \"\"\"\n    Checks if the supplied path is a HDX-MS dataset.\n    \"\"\"\n\n    return (path / \"hdx_spec.yaml\").exists()\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.RemoteDataVault","title":"<code>RemoteDataVault</code>","text":"<p>               Bases: <code>DataVault</code></p> <p>A vault for HDX-MS datasets, with the ability to fetch datasets from a remote repository.</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Union[Path, str]</code> <p>Directory to store downloaded datasets.</p> required <code>remote_url</code> <code>str</code> <p>URL of the remote repository (default: DATABASE_URL).</p> <code>DATABASE_URL</code> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>class RemoteDataVault(DataVault):\n    \"\"\"\n    A vault for HDX-MS datasets, with the ability to fetch datasets from a remote repository.\n\n    Args:\n        cache_dir: Directory to store downloaded datasets.\n        remote_url: URL of the remote repository (default: DATABASE_URL).\n    \"\"\"\n\n    def __init__(self, cache_dir: Union[Path, str], remote_url: str = DATABASE_URL):\n        super().__init__(cache_dir)\n        self.remote_url = remote_url\n\n    def get_index(self) -&gt; nw.DataFrame:\n        \"\"\"Retrieves the index of available datasets\n\n        on success, returns the index dataframe and\n        stores as `remote_index` attribute.\n\n        \"\"\"\n\n        url = urllib.parse.urljoin(self.remote_url, \"index.csv\")\n        response = requests.get(url)\n\n        if response.ok:\n            (self.cache_dir / \"index.csv\").write_bytes(response.content)\n            return nw.read_csv(str(self.cache_dir / \"index.csv\"), backend=BACKEND)\n        else:\n            raise urllib.error.HTTPError(\n                url,\n                response.status_code,\n                \"Error downloading database index\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n    def fetch_dataset(self, data_id: str) -&gt; bool:\n        \"\"\"\n        Download a dataset from the online repository to the cache dir\n\n        Args:\n            data_id: The ID of the dataset to download.\n\n        Returns:\n            `True` if the dataset was downloaded successfully, `False`  otherwise.\n        \"\"\"\n\n        output_pth = self.cache_dir / data_id\n        if output_pth.exists():\n            return False\n        else:\n            output_pth.mkdir()\n\n        dataset_url = urllib.parse.urljoin(self.remote_url, data_id + \"/\")\n\n        files = [\"hdx_spec.yaml\", \"metadata.yaml\"]\n        optional_files = [\"CITATION.cff\"]\n        hdx_spec = None\n        for f in files + optional_files:\n            url = urllib.parse.urljoin(dataset_url, f)\n            response = requests.get(url)\n\n            if response.ok:\n                (output_pth / f).write_bytes(response.content)\n\n            elif f in files:\n                raise urllib.error.HTTPError(\n                    url,\n                    response.status_code,\n                    f\"Error for file {f!r}\",\n                    response.headers,  # type: ignore\n                    None,\n                )\n\n            if f == \"hdx_spec.yaml\":\n                hdx_spec = yaml.safe_load(response.text)\n\n        if hdx_spec is None:\n            raise ValueError(f\"Could not find HDX spec for data_id {data_id!r}\")\n\n        data_pth = output_pth / \"data\"\n        data_pth.mkdir()\n\n        for file_spec in hdx_spec[\"data_files\"].values():\n            filename = file_spec[\"filename\"]\n            f_url = urllib.parse.urljoin(dataset_url, filename)\n            response = requests.get(f_url)\n\n            if response.ok:\n                (output_pth / filename).write_bytes(response.content)\n            else:\n                raise urllib.error.HTTPError(\n                    f_url,\n                    response.status_code,\n                    f\"Error for data file {filename!r}\",\n                    response.headers,  # type: ignore\n                    None,\n                )\n\n        return True\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.RemoteDataVault.fetch_dataset","title":"<code>fetch_dataset(data_id)</code>","text":"<p>Download a dataset from the online repository to the cache dir</p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>The ID of the dataset to download.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the dataset was downloaded successfully, <code>False</code>  otherwise.</p> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>def fetch_dataset(self, data_id: str) -&gt; bool:\n    \"\"\"\n    Download a dataset from the online repository to the cache dir\n\n    Args:\n        data_id: The ID of the dataset to download.\n\n    Returns:\n        `True` if the dataset was downloaded successfully, `False`  otherwise.\n    \"\"\"\n\n    output_pth = self.cache_dir / data_id\n    if output_pth.exists():\n        return False\n    else:\n        output_pth.mkdir()\n\n    dataset_url = urllib.parse.urljoin(self.remote_url, data_id + \"/\")\n\n    files = [\"hdx_spec.yaml\", \"metadata.yaml\"]\n    optional_files = [\"CITATION.cff\"]\n    hdx_spec = None\n    for f in files + optional_files:\n        url = urllib.parse.urljoin(dataset_url, f)\n        response = requests.get(url)\n\n        if response.ok:\n            (output_pth / f).write_bytes(response.content)\n\n        elif f in files:\n            raise urllib.error.HTTPError(\n                url,\n                response.status_code,\n                f\"Error for file {f!r}\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n        if f == \"hdx_spec.yaml\":\n            hdx_spec = yaml.safe_load(response.text)\n\n    if hdx_spec is None:\n        raise ValueError(f\"Could not find HDX spec for data_id {data_id!r}\")\n\n    data_pth = output_pth / \"data\"\n    data_pth.mkdir()\n\n    for file_spec in hdx_spec[\"data_files\"].values():\n        filename = file_spec[\"filename\"]\n        f_url = urllib.parse.urljoin(dataset_url, filename)\n        response = requests.get(f_url)\n\n        if response.ok:\n            (output_pth / filename).write_bytes(response.content)\n        else:\n            raise urllib.error.HTTPError(\n                f_url,\n                response.status_code,\n                f\"Error for data file {filename!r}\",\n                response.headers,  # type: ignore\n                None,\n            )\n\n    return True\n</code></pre>"},{"location":"reference/stable/v020/datavault/#stable.v020.datavault.RemoteDataVault.get_index","title":"<code>get_index()</code>","text":"<p>Retrieves the index of available datasets</p> <p>on success, returns the index dataframe and stores as <code>remote_index</code> attribute.</p> Source code in <code>hdxms_datasets/stable/v020/datavault.py</code> <pre><code>def get_index(self) -&gt; nw.DataFrame:\n    \"\"\"Retrieves the index of available datasets\n\n    on success, returns the index dataframe and\n    stores as `remote_index` attribute.\n\n    \"\"\"\n\n    url = urllib.parse.urljoin(self.remote_url, \"index.csv\")\n    response = requests.get(url)\n\n    if response.ok:\n        (self.cache_dir / \"index.csv\").write_bytes(response.content)\n        return nw.read_csv(str(self.cache_dir / \"index.csv\"), backend=BACKEND)\n    else:\n        raise urllib.error.HTTPError(\n            url,\n            response.status_code,\n            \"Error downloading database index\",\n            response.headers,  # type: ignore\n            None,\n        )\n</code></pre>"},{"location":"reference/stable/v020/expr/","title":"expr","text":""},{"location":"reference/stable/v020/formats/","title":"formats","text":""},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_v3_cluster","title":"<code>DynamX_v3_cluster</code>","text":"Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class DynamX_v3_cluster:\n    columns = [\n        \"Protein\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"Modification\",\n        \"Fragment\",\n        \"MaxUptake\",\n        \"MHP\",\n        \"State\",\n        \"Exposure\",\n        \"File\",\n        \"z\",\n        \"RT\",\n        \"Inten\",\n        \"Center\",\n    ]\n    state_name = \"State\"\n    exposure_name = \"Exposure\"\n    aggregated = False\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        return from_dynamx_cluster(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_v3_cluster.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    return from_dynamx_cluster(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_vx_state","title":"<code>DynamX_vx_state</code>","text":"<p>There are also DynamX state data files which do not have 'Modification' and 'Fragment' columns. not sure which version this is.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class DynamX_vx_state:\n    \"\"\"There are also DynamX state data files which do not have 'Modification' and 'Fragment' columns.\n    not sure which version this is.\n    \"\"\"\n\n    columns = [\n        \"Protein\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"MaxUptake\",\n        \"MHP\",\n        \"State\",\n        \"Exposure\",\n        \"Center\",\n        \"Center SD\",\n        \"Uptake\",\n        \"Uptake SD\",\n        \"RT\",\n        \"RT SD\",\n    ]\n\n    state_name = \"State\"\n    exposure_name = \"Exposure\"\n    aggregated = True\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        return from_dynamx_state(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.DynamX_vx_state.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    return from_dynamx_state(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDExaminer_v3","title":"<code>HDExaminer_v3</code>","text":"Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class HDExaminer_v3:\n    columns = [\n        \"Protein State\",\n        \"Deut Time\",\n        \"Experiment\",\n        \"Start\",\n        \"End\",\n        \"Sequence\",\n        \"Charge\",\n        \"Search RT\",\n        \"Actual RT\",\n        \"# Spectra\",\n        \"Peak Width Da\",\n        \"m/z Shift Da\",\n        \"Max Inty\",\n        \"Exp Cent\",\n        \"Theor Cent\",\n        \"Score\",\n        \"Cent Diff\",\n        \"# Deut\",\n        \"Deut %\",\n        \"Confidence\",\n    ]\n    state_name = \"Protein State\"\n    exposure_name = \"Deut Time\"\n    aggregated = False\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        return from_hdexaminer(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDExaminer_v3.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    return from_hdexaminer(df)\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDXFormat","title":"<code>HDXFormat</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>class HDXFormat(Protocol):\n    columns: list[str]\n    state_name: str\n    exposure_name: str\n    # aggregated: bool = False  # whether the data is aggregated or expanded as multiple replicates\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n        ...\n\n    @property\n    def aggregated(self) -&gt; bool: ...\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.HDXFormat.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.OpenHDXFormat","title":"<code>OpenHDXFormat</code>  <code>dataclass</code>","text":"<p>A format where columns names are standardized to a common set.</p> <p>Hence OpenHDXFormat.convert() is a no-op.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>@dataclass\nclass OpenHDXFormat:\n    \"\"\"A format where columns names are standardized to a common set.\n\n    Hence OpenHDXFormat.convert() is a no-op.\n\n    \"\"\"\n\n    columns = STANDARD_COLUMNS + OPTIONAL_COLUMNS\n    state_name = \"state\"\n    exposure_name = \"exposure\"\n    aggregated: bool  #  = True  # whether the data is aggregated or expanded as multiple replicates\n\n    def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n        \"\"\"\n        Convert the DataFrame to a standard format.\n        \"\"\"\n\n        return df\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.OpenHDXFormat.convert","title":"<code>convert(df)</code>","text":"<p>Convert the DataFrame to a standard format.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def convert(self, df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"\n    Convert the DataFrame to a standard format.\n    \"\"\"\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/formats/#stable.v020.formats.identify_format","title":"<code>identify_format(df, *, exact=True)</code>","text":"<p>Identify which HDXFormat subclass the given column list matches. If there is no match, return an OpenHDXFormat instance with aggregated set to True if 'replicate' is in the columns.</p> <p>Parameters:</p> Name Type Description Default <code>exact</code> <code>bool</code> <p>If True, order must match; otherwise, uses set equality.</p> <code>True</code> <p>Returns:</p> Type Description <code>Optional[HDXFormat]</code> <p>The matching HDXFormat subclass, or None if no match.</p> Source code in <code>hdxms_datasets/stable/v020/formats.py</code> <pre><code>def identify_format(df: nw.DataFrame, *, exact: bool = True) -&gt; Optional[HDXFormat]:\n    \"\"\"\n    Identify which HDXFormat subclass the given column list matches. If there is no match,\n    return an OpenHDXFormat instance with aggregated set to True if 'replicate' is in the columns.\n\n    Args:\n        exact: If True, order must match; otherwise, uses set equality.\n\n    Returns:\n        The matching HDXFormat subclass, or None if no match.\n    \"\"\"\n    cols = df.columns\n    for fmt in HDX_FORMATS:\n        template = fmt.columns\n        if exact and cols == template:\n            return fmt\n        elif not exact and set(cols) == set(template):\n            return fmt\n\n    # it there is no match, we try to return the OpenHDXFormat\n    aggregated = \"replicate\" not in cols\n    fmt = OpenHDXFormat(aggregated=aggregated)\n\n    return fmt\n</code></pre>"},{"location":"reference/stable/v020/plot/","title":"plot","text":""},{"location":"reference/stable/v020/plot/#stable.v020.plot.find_wrap","title":"<code>find_wrap(peptides, margin=4, step=5, wrap_limit=200)</code>","text":"<p>Find the minimum wrap value for a given list of intervals.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>Dataframe with columns 'start' and 'end' representing intervals.</p> required <code>margin</code> <code>int</code> <p>The margin applied to the wrap value. Defaults to 4.</p> <code>4</code> <code>step</code> <code>int</code> <p>The increment step for the wrap value. Defaults to 5.</p> <code>5</code> <code>wrap_limit</code> <code>int</code> <p>The maximum allowed wrap value. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The minimum wrap value that does not overlap with any intervals.</p> Source code in <code>hdxms_datasets/stable/v020/plot.py</code> <pre><code>def find_wrap(\n    peptides: pl.DataFrame,\n    margin: int = 4,\n    step: int = 5,\n    wrap_limit: int = 200,\n) -&gt; int:\n    \"\"\"\n    Find the minimum wrap value for a given list of intervals.\n\n    Args:\n        peptides: Dataframe with columns 'start' and 'end' representing intervals.\n        margin: The margin applied to the wrap value. Defaults to 4.\n        step: The increment step for the wrap value. Defaults to 5.\n        wrap_limit: The maximum allowed wrap value. Defaults to 200.\n\n    Returns:\n        int: The minimum wrap value that does not overlap with any intervals.\n    \"\"\"\n    wrap = step\n\n    while True:\n        peptides_y = peptides.with_columns(\n            (pl.int_range(pl.len(), dtype=pl.UInt32).alias(\"y\") % wrap)\n        )\n\n        no_overlaps = True\n        for name, df in peptides_y.group_by(\"y\", maintain_order=True):\n            overlaps = (np.array(df[\"end\"]) + 1 + margin)[:-1] &gt;= np.array(df[\"start\"])[1:]\n            if np.any(overlaps):\n                no_overlaps = False\n                break\n                # return wrap\n\n        wrap += step\n        if wrap &gt; wrap_limit:\n            return wrap_limit  # Return the maximum wrap limit if no valid wrap found\n        elif no_overlaps:\n            return wrap\n</code></pre>"},{"location":"reference/stable/v020/plot/#stable.v020.plot.unique_peptides","title":"<code>unique_peptides(df)</code>","text":"<p>Checks if all peptides in the DataFrame are unique.</p> Source code in <code>hdxms_datasets/stable/v020/plot.py</code> <pre><code>def unique_peptides(df: pl.DataFrame) -&gt; bool:\n    \"\"\"\n    Checks if all peptides in the DataFrame are unique.\n    \"\"\"\n\n    return len(df) == len(df.unique(subset=[\"start\", \"end\"]))\n</code></pre>"},{"location":"reference/stable/v020/process/","title":"process","text":""},{"location":"reference/stable/v020/process/#stable.v020.process.TemperatureDict","title":"<code>TemperatureDict</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict for temperature dictionary.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>class TemperatureDict(TypedDict):\n    \"\"\"TypedDict for temperature dictionary.\"\"\"\n\n    value: float\n    unit: Literal[\"C\", \"K\"]\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.aggregate_columns","title":"<code>aggregate_columns(df, columns, by=['start', 'end', 'exposure'])</code>","text":"<p>Aggregate the DataFrame the specified columns by intensity-weighted average.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def aggregate_columns(\n    df: nw.DataFrame, columns: list[str], by: list[str] = [\"start\", \"end\", \"exposure\"]\n):\n    \"\"\"\n    Aggregate the DataFrame the specified columns by intensity-weighted average.\n    \"\"\"\n    groups = df.group_by(by)\n    output = {k: [] for k in by}\n    for col in columns:\n        output[col] = []\n        output[f\"{col}_sd\"] = []\n\n    for (start, end, exposure), df_group in groups:\n        output[\"start\"].append(start)\n        output[\"end\"].append(end)\n        output[\"exposure\"].append(exposure)\n\n        for col in columns:\n            val = ufloat_stats(df_group[col], df_group[\"intensity\"])\n            output[col].append(val.nominal_value)\n            output[f\"{col}_sd\"].append(val.std_dev)\n\n    agg_df = nw.from_dict(output, backend=BACKEND)\n    return agg_df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.compute_uptake_metrics","title":"<code>compute_uptake_metrics(df, exception='raise')</code>","text":"<p>Tries to add derived columns to the DataFrame. Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def compute_uptake_metrics(df: nw.DataFrame, exception=\"raise\") -&gt; nw.DataFrame:\n    \"\"\"\n    Tries to add derived columns to the DataFrame.\n    Possible columns to add are: uptake, uptake_sd, fd_uptake, fd_uptake_sd, rfu, max_uptake.\n    \"\"\"\n    all_columns = {\n        \"max_uptake\": hdx_expr.max_uptake,\n        \"uptake\": hdx_expr.uptake,\n        \"uptake_sd\": hdx_expr.uptake_sd,\n        \"fd_uptake\": hdx_expr.fd_uptake,\n        \"fd_uptake_sd\": hdx_expr.fd_uptake_sd,\n        \"frac_fd_control\": hdx_expr.frac_fd_control,\n        \"frac_fd_control_sd\": hdx_expr.frac_fd_control_sd,\n        \"frac_max_uptake\": hdx_expr.frac_max_uptake,\n        \"frac_max_uptake_sd\": hdx_expr.frac_max_uptake_sd,\n    }\n\n    for col, expr in all_columns.items():\n        if col not in df.columns:\n            try:\n                df = df.with_columns(expr)\n            except Exception as e:\n                if exception == \"raise\":\n                    raise e\n                elif exception == \"warn\":\n                    warnings.warn(f\"Failed to add column {col}: {e}\")\n                elif exception == \"ignore\":\n                    pass\n                else:\n                    raise ValueError(\"Invalid exception handling option\")\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.convert_temperature","title":"<code>convert_temperature(temperature_dict, target_unit='C')</code>","text":"<p>Convenience function to convert temperature values.</p> <p>Parameters:</p> Name Type Description Default <code>temperature_dict</code> <code>TemperatureDict</code> <p>Dictionary with temperature value(s) and unit.</p> required <code>target_unit</code> <code>str</code> <p>Target unit for temperature. Must be \"C, or \"K\"</p> <code>'C'</code> <p>Returns:</p> Type Description <code>float</code> <p>Converted temperature value(s).</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def convert_temperature(temperature_dict: TemperatureDict, target_unit: str = \"C\") -&gt; float:\n    \"\"\"\n    Convenience function to convert temperature values.\n\n    Args:\n        temperature_dict: Dictionary with temperature value(s) and unit.\n        target_unit: Target unit for temperature. Must be \"C, or \"K\"\n\n    Returns:\n        Converted temperature value(s).\n    \"\"\"\n\n    src_unit = temperature_dict[\"unit\"]\n    temp_offset = TEMPERATURE_OFFSETS[src_unit] - TEMPERATURE_OFFSETS[target_unit]\n    return temperature_dict[\"value\"] + temp_offset\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.convert_time","title":"<code>convert_time(time_dict, target_unit='s')</code>","text":"<p>Convenience function to convert time values.</p> <p>Parameters:</p> Name Type Description Default <code>time_dict</code> <code>dict</code> <p>Dictionary with time value(s) and unit.</p> required <code>target_unit</code> <code>Literal['s', 'min', 'h']</code> <p>Target unit for time.</p> <code>'s'</code> <p>Returns:</p> Type Description <code>Union[float, list[float]]</code> <p>Converted time value(s).</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def convert_time(\n    time_dict: dict, target_unit: Literal[\"s\", \"min\", \"h\"] = \"s\"\n) -&gt; Union[float, list[float]]:\n    \"\"\"\n    Convenience function to convert time values.\n\n    Args:\n        time_dict: Dictionary with time value(s) and unit.\n        target_unit: Target unit for time.\n\n    Returns:\n        Converted time value(s).\n    \"\"\"\n    raise DeprecationWarning()\n    src_unit = time_dict[\"unit\"]\n\n    time_factor = TIME_FACTORS[src_unit] / TIME_FACTORS[target_unit]\n    if values := time_dict.get(\"values\"):\n        return [v * time_factor for v in values]\n    elif value := time_dict.get(\"value\"):\n        return value * time_factor\n    else:\n        raise ValueError(\"Invalid time dictionary\")\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.drop_null_columns","title":"<code>drop_null_columns(df)</code>","text":"<p>Drop columns that are all null from the DataFrame.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def drop_null_columns(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Drop columns that are all null from the DataFrame.\"\"\"\n    all_null_columns = [col for col in df.columns if df[col].is_null().all()]\n    return df.drop(all_null_columns)\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.dynamx_cluster_to_state","title":"<code>dynamx_cluster_to_state(cluster_data, nd_exposure=0.0)</code>","text":"<p>convert dynamx cluster data to state data must contain only a single state</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def dynamx_cluster_to_state(cluster_data: nw.DataFrame, nd_exposure: float = 0.0) -&gt; nw.DataFrame:\n    \"\"\"\n    convert dynamx cluster data to state data\n    must contain only a single state\n    \"\"\"\n\n    assert len(cluster_data[\"state\"].unique()) == 1, \"Multiple states found in data\"\n\n    # determine undeuterated masses per peptide\n    nd_data = cluster_data.filter(nw.col(\"exposure\") == nd_exposure)\n    nd_peptides: list[tuple[int, int]] = sorted(\n        {(start, end) for start, end in zip(nd_data[\"start\"], nd_data[\"end\"])}\n    )\n\n    peptides_nd_mass = {}\n    for p in nd_peptides:\n        start, end = p\n        df_nd_peptide = nd_data.filter((nw.col(\"start\") == start) &amp; (nw.col(\"end\") == end))\n\n        masses = df_nd_peptide[\"z\"] * (df_nd_peptide[\"center\"] - PROTON_MASS)\n        nd_mass = ufloat_stats(masses, df_nd_peptide[\"inten\"])\n\n        peptides_nd_mass[p] = nd_mass\n\n    groups = cluster_data.group_by([\"start\", \"end\", \"exposure\"])\n    unique_columns = [\n        \"end\",\n        \"exposure\",\n        \"fragment\",\n        \"maxuptake\",\n        \"mhp\",\n        \"modification\",\n        \"protein\",\n        \"sequence\",\n        \"start\",\n        \"state\",\n        \"stop\",\n    ]\n    records = []\n    for (start, end, exposure), df_group in groups:\n        record = {col: df_group[col][0] for col in unique_columns}\n\n        rt = ufloat_stats(df_group[\"rt\"], df_group[\"inten\"])\n        record[\"rt\"] = rt.nominal_value\n        record[\"rt_sd\"] = rt.std_dev\n\n        # state data 'center' is mass as if |charge| would be 1\n        center = ufloat_stats(\n            df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS) + PROTON_MASS, df_group[\"inten\"]\n        )\n        record[\"center\"] = center.nominal_value\n        record[\"center_sd\"] = center.std_dev\n\n        masses = df_group[\"z\"] * (df_group[\"center\"] - PROTON_MASS)\n        exp_mass = ufloat_stats(masses, df_group[\"inten\"])\n\n        if (start, end) in peptides_nd_mass:\n            uptake = exp_mass - peptides_nd_mass[(start, end)]\n            record[\"uptake\"] = uptake.nominal_value\n            record[\"uptake_sd\"] = uptake.std_dev\n        else:\n            record[\"uptake\"] = None\n            record[\"uptake_sd\"] = None\n\n        records.append(record)\n\n    d = records_to_dict(records)\n    df = nw.from_dict(d, backend=BACKEND)\n\n    if set(df.columns) == set(STATE_DATA_COLUMN_ORDER):\n        df = df[STATE_DATA_COLUMN_ORDER]\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.filter_peptides","title":"<code>filter_peptides(df, state=None, exposure=None)</code>","text":"<p>Convenience function to filter a peptides DataFrame. .</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe.</p> required <code>state</code> <code>Optional[str]</code> <p>Name of protein state to select.</p> <code>None</code> <code>exposure</code> <code>Optional[dict]</code> <p>Exposure value(s) to select. Exposure is given as a :obj:<code>dict</code>, with keys \"value\" or \"values\" for exposure value, and \"unit\" for the time unit.</p> <code>None</code> <p>Examples:</p> <p>Filter peptides for a specific protein state and exposure time:</p> <pre><code>&gt;&gt;&gt; d = {\"state\", \"SecB WT apo\", \"exposure\": {\"value\": 0.167, \"unit\": \"min\"}\n&gt;&gt;&gt; filtered_df = filter_peptides(df, **d)\n</code></pre> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered dataframe.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def filter_peptides(\n    df: nw.DataFrame,\n    state: Optional[str] = None,\n    exposure: Optional[dict] = None,\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Convenience function to filter a peptides DataFrame. .\n\n    Args:\n        df: Input dataframe.\n        state: Name of protein state to select.\n        exposure: Exposure value(s) to select. Exposure is given as a :obj:`dict`, with keys \"value\" or \"values\" for\n            exposure value, and \"unit\" for the time unit.\n\n    Examples:\n        Filter peptides for a specific protein state and exposure time:\n\n        &gt;&gt;&gt; d = {\"state\", \"SecB WT apo\", \"exposure\": {\"value\": 0.167, \"unit\": \"min\"}\n        &gt;&gt;&gt; filtered_df = filter_peptides(df, **d)\n\n    Returns:\n        Filtered dataframe.\n    \"\"\"\n    raise DeprecationWarning()\n    if state is not None:\n        df = df.filter(nw.col(\"state\") == state)\n\n    if exposure is not None:\n        # NA unit is used when exposure is given as string, in case of HD examiner this can be 'FD'\n        if exposure[\"unit\"] == \"NA\":\n            t_val = exposure[\"value\"]\n        else:\n            t_val = convert_time(exposure, \"s\")\n        if isinstance(t_val, list):\n            if all(isinstance(v, float) for v in t_val):\n                col = nw.col(\"exposure\")\n            elif all(isinstance(v, str) for v in t_val):\n                col = nw.col(\"exposure\").cast(nw.Float64)\n            else:\n                raise ValueError(\"Invalid exposure values\")\n            df = df.filter(col.is_in(t_val))\n        else:\n            df = df.filter(nw.col(\"exposure\") == t_val)\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.records_to_dict","title":"<code>records_to_dict(records)</code>","text":"<p>Convert a list of records to a dictionary of lists.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list[dict]</code> <p>List of dictionaries.</p> required <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>Dictionary with keys as column names and values as lists.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def records_to_dict(records: list[dict]) -&gt; dict[str, list]:\n    \"\"\"\n    Convert a list of records to a dictionary of lists.\n\n    Args:\n        records: List of dictionaries.\n\n    Returns:\n        Dictionary with keys as column names and values as lists.\n    \"\"\"\n\n    df_dict = defaultdict(list)\n    for record in records:\n        for key, value in record.items():\n            df_dict[key].append(value)\n\n    return dict(df_dict)\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.sort_rows","title":"<code>sort_rows(df)</code>","text":"<p>Sorts the DataFrame by state, exposure, start, end, file.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def sort_rows(df: nw.DataFrame) -&gt; nw.DataFrame:\n    \"\"\"Sorts the DataFrame by state, exposure, start, end, file.\"\"\"\n    all_by = [\"state\", \"exposure\", \"start\", \"end\", \"replicate\"]\n    by = [col for col in all_by if col in df.columns]\n    return df.sort(by=by)\n</code></pre>"},{"location":"reference/stable/v020/process/#stable.v020.process.ufloat_stats","title":"<code>ufloat_stats(array, weights)</code>","text":"<p>Calculate the weighted mean and standard deviation.</p> Source code in <code>hdxms_datasets/stable/v020/process.py</code> <pre><code>def ufloat_stats(array, weights) -&gt; Variable:\n    \"\"\"Calculate the weighted mean and standard deviation.\"\"\"\n    weighted_stats = DescrStatsW(array, weights=weights, ddof=0)\n    return ufloat(weighted_stats.mean, weighted_stats.std)\n</code></pre>"},{"location":"reference/stable/v020/reader/","title":"reader","text":"<p>Reader functions for various file formats</p>"},{"location":"reference/stable/v020/reader/#stable.v020.reader.read_dynamx","title":"<code>read_dynamx(filepath_or_buffer, time_conversion=('min', 's'))</code>","text":"<p>Reads DynamX .csv files and returns the resulting peptide table as a narwhals DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>filepath_or_buffer</code> <code>Path | str | IO | bytes</code> <p>File path of the .csv file or :class:<code>~io.StringIO</code> object.</p> required <code>time_conversion</code> <code>Optional[tuple[Literal['h', 'min', 's'], Literal['h', 'min', 's']]]</code> <p>How to convert the time unit of the field 'exposure'. Format is ('', &lt;'to'&gt;). Unit options are 'h', 'min' or 's'. <code>('min', 's')</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Peptide table as a narwhals DataFrame.</p> Source code in <code>hdxms_datasets/stable/v020/reader.py</code> <pre><code>def read_dynamx(\n    filepath_or_buffer: Path | str | IO | bytes,\n    time_conversion: Optional[tuple[Literal[\"h\", \"min\", \"s\"], Literal[\"h\", \"min\", \"s\"]]] = (\n        \"min\",\n        \"s\",\n    ),\n) -&gt; nw.DataFrame:\n    \"\"\"\n    Reads DynamX .csv files and returns the resulting peptide table as a narwhals DataFrame.\n\n    Args:\n        filepath_or_buffer: File path of the .csv file or :class:`~io.StringIO` object.\n        time_conversion: How to convert the time unit of the field 'exposure'. Format is ('&lt;from&gt;', &lt;'to'&gt;).\n            Unit options are 'h', 'min' or 's'.\n\n    Returns:\n        Peptide table as a narwhals DataFrame.\n    \"\"\"\n\n    df = read_csv(filepath_or_buffer)\n    df = df.rename({col: col.replace(\" \", \"_\").lower() for col in df.columns})\n\n    # insert 'stop' column (which is end + 1)\n    columns = df.columns\n    columns.insert(columns.index(\"end\") + 1, \"stop\")\n    df = df.with_columns((nw.col(\"end\") + 1).alias(\"stop\")).select(columns)\n\n    if time_conversion is not None:\n        time_lut = {\"h\": 3600, \"min\": 60, \"s\": 1}\n        time_factor = time_lut[time_conversion[0]] / time_lut[time_conversion[1]]\n        df = df.with_columns((nw.col(\"exposure\") * time_factor))\n\n    return df\n</code></pre>"},{"location":"reference/stable/v020/utils/","title":"utils","text":""},{"location":"reference/stable/v020/utils/#stable.v020.utils.contiguous_peptides","title":"<code>contiguous_peptides(df, start='start', end='end')</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing contiguous regions.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef contiguous_peptides(df: IntoFrame, start=\"start\", end=\"end\") -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing contiguous regions.\n    \"\"\"\n    # cast to ensure df is a narwhals DataFrame\n    df = cast(nw.DataFrame, df).select([start, end]).unique().sort(by=[start, end])\n\n    regions = []\n    current_start, current_end = None, 0\n\n    for start_val, end_val in df.select([nw.col(start), nw.col(end)]).iter_rows(named=False):\n        if current_start is None:\n            # Initialize the first region\n            current_start, current_end = start_val, end_val\n        elif start_val &lt;= current_end + 1:  # Check for contiguity\n            # Extend the current region\n            current_end = max(current_end, end_val)\n        else:\n            # Save the previous region and start a new one\n            regions.append((current_start, current_end))\n            current_start, current_end = start_val, end_val\n\n    # Don't forget to add the last region\n    if current_start is not None:\n        regions.append((current_start, current_end))\n\n    return regions\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.default_protein_info","title":"<code>default_protein_info(peptides)</code>","text":"<p>Generate minimal protein info from a set of peptides</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef default_protein_info(peptides: IntoFrame) -&gt; ProteinInfo:\n    \"\"\"Generate minimal protein info from a set of peptides\"\"\"\n\n    # Find minimum start and maximum end positions\n    min_start = peptides[\"start\"].min()  # type: ignore\n    max_end = peptides[\"end\"].max()  # type: ignore\n\n    # Estimate sequence length\n    sequence_length = max_end - min_start + 1\n\n    placeholder_sequence = \"X\" * sequence_length\n    sequence = reconstruct_sequence(peptides, placeholder_sequence, n_term=min_start)\n\n    # Create a minimal ProteinInfo\n    return {\n        \"sequence\": sequence,  # sequence with \"X\" gaps\n        \"n_term\": int(min_start),\n        \"c_term\": int(max_end),\n    }\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.non_overlapping_peptides","title":"<code>non_overlapping_peptides(df, start='start', end='end')</code>","text":"<p>Given a dataframe with 'start' and 'end' columns, each describing a range, (inclusive intervals), this function returns a list of tuples representing non-overlapping peptides.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef non_overlapping_peptides(\n    df: IntoFrame,\n    start: str = \"start\",\n    end: str = \"end\",\n) -&gt; list[tuple[int, int]]:\n    \"\"\"\n    Given a dataframe with 'start' and 'end' columns, each describing a range,\n    (inclusive intervals), this function returns a list of tuples\n    representing non-overlapping peptides.\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([start, end]).unique().sort(by=[start, end])\n\n    regions = df.rows()\n    out = [regions[0]]\n    for start_val, end_val in regions[1:]:\n        if start_val &gt; out[-1][1]:\n            out.append((start_val, end_val))\n        else:\n            continue\n\n    return out\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.peptide_redundancy","title":"<code>peptide_redundancy(df, start='start', end='end')</code>","text":"<p>Compute the redundancy of peptides in a DataFrame based on their start and end positions. Redundancy is defined as the number of peptides overlapping at each position.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>IntoFrame</code> <p>DataFrame containing peptide information with 'start' and 'end' columns.</p> required <code>start</code> <code>str</code> <p>Column name for the start position.</p> <code>'start'</code> <code>end</code> <code>str</code> <p>Column name for the end position.</p> <code>'end'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tuple containing:</p> <code>ndarray</code> <ul> <li>r_number: An array of positions from the minimum start to the maximum end.</li> </ul> <code>tuple[ndarray, ndarray]</code> <ul> <li>redundancy: An array of redundancy counts for each position in r_number.</li> </ul> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef peptide_redundancy(\n    df: IntoFrame, start: str = \"start\", end: str = \"end\"\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the redundancy of peptides in a DataFrame based on their start and end positions.\n    Redundancy is defined as the number of peptides overlapping at each position.\n\n    Args:\n        df: DataFrame containing peptide information with 'start' and 'end' columns.\n        start: Column name for the start position.\n        end: Column name for the end position.\n\n    Returns:\n        A tuple containing:\n        - r_number: An array of positions from the minimum start to the maximum end.\n        - redundancy: An array of redundancy counts for each position in r_number.\n\n    \"\"\"\n    df = cast(nw.DataFrame, df).select([start, end]).unique().sort(by=[start, end])\n    vmin, vmax = df[start][0], df[end][-1]\n\n    r_number = np.arange(vmin, vmax + 1, dtype=int)\n    redundancy = np.zeros_like(r_number, dtype=int)\n    for s, e in df.rows():\n        i0, i1 = np.searchsorted(r_number, (s, e))\n        redundancy[i0:i1] += 1\n\n    return r_number, redundancy\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.reconstruct_sequence","title":"<code>reconstruct_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Reconstruct the sequence form a dataframe of peptides with sequence information. The sequence is reconstructed by replacing the known sequence with the peptide sequences at the specified start and end positions.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>DataFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>Starting sequence. Can be a string 'X' as placeholder.</p> required <code>n_term</code> <code>int</code> <p>The residue number of the N-terminal residue. This is typically 1, can be negative in case of purification tags.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>The reconstructed sequence.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef reconstruct_sequence(peptides: nw.DataFrame, known_sequence: str, n_term: int = 1) -&gt; str:\n    \"\"\"\n    Reconstruct the sequence form a dataframe of peptides with sequence information.\n    The sequence is reconstructed by replacing the known sequence with the peptide\n    sequences at the specified start and end positions.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: Starting sequence. Can be a string 'X' as placeholder.\n        n_term: The residue number of the N-terminal residue. This is typically 1, can be\n            negative in case of purification tags.\n\n    Returns:\n        The reconstructed sequence.\n    \"\"\"\n\n    reconstructed = list(known_sequence)\n    for start, end, sequence in peptides.select([\"start\", \"end\", \"sequence\"]).iter_rows():  # type: ignore\n        start_idx = start - n_term\n        assert end - start + 1 == len(sequence), (\n            f\"Length mismatch at {start}:{end} with sequence {sequence}\"\n        )\n\n        for i, aa in enumerate(sequence, start=start_idx):\n            reconstructed[i] = aa\n\n    return \"\".join(reconstructed)\n</code></pre>"},{"location":"reference/stable/v020/utils/#stable.v020.utils.verify_sequence","title":"<code>verify_sequence(peptides, known_sequence, n_term=1)</code>","text":"<p>Verify the sequence of peptides against the given sequence.</p> <p>Parameters:</p> Name Type Description Default <code>peptides</code> <code>IntoFrame</code> <p>DataFrame containing peptide information.</p> required <code>known_sequence</code> <code>str</code> <p>The original sequence to check against.</p> required <code>n_term</code> <code>int</code> <p>The number of N-terminal residues to consider.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[tuple[int, str, str]]</code> <p>A tuple containing the fixed sequence and a list of mismatches.</p> Source code in <code>hdxms_datasets/stable/v020/utils.py</code> <pre><code>@nw.narwhalify\ndef verify_sequence(\n    peptides: IntoFrame, known_sequence: str, n_term: int = 1\n) -&gt; list[tuple[int, str, str]]:\n    \"\"\"\n    Verify the sequence of peptides against the given sequence.\n\n    Args:\n        peptides: DataFrame containing peptide information.\n        known_sequence: The original sequence to check against.\n        n_term: The number of N-terminal residues to consider.\n\n    Returns:\n        A tuple containing the fixed sequence and a list of mismatches.\n    \"\"\"\n\n    reconstructed_sequence = reconstruct_sequence(peptides, known_sequence, n_term)\n\n    mismatches = []\n    for r_number, (expected, found) in enumerate(\n        zip(known_sequence, reconstructed_sequence), start=n_term\n    ):\n        if expected != found:\n            mismatches.append((r_number, expected, found))\n\n    return mismatches\n</code></pre>"},{"location":"reference/web/components/","title":"components","text":""},{"location":"reference/web/components/#web.components.InputNumeric","title":"<code>InputNumeric(label, value=0, on_value=None, disabled=False, optional=False, continuous_update=False, clearable=False, classes=[], style=None, autofocus=False)</code>","text":"<p>Numeric input (float | integers).</p> <p>Basic example:</p> <pre><code>import solara\n\nint_value = solara.reactive(42)\ncontinuous_update = solara.reactive(True)\n\n@solara.component\ndef Page():\n    solara.Checkbox(label=\"Continuous update\", value=continuous_update)\n    solara.InputInt(\"Enter an integer number\", value=int_value, continuous_update=continuous_update.value)\n    with solara.Row():\n        solara.Button(\"Clear\", on_click=lambda: int_value.set(42))\n    solara.Markdown(f\"**You entered**: {int_value.value}\")\n</code></pre>"},{"location":"reference/web/components/#web.components.InputNumeric--arguments","title":"Arguments","text":"<ul> <li><code>label</code>: Label to display next to the slider.</li> <li><code>value</code>: The currently entered value.</li> <li><code>on_value</code>: Callback to call when the value changes.</li> <li><code>disabled</code>: Whether the input is disabled.</li> <li><code>optional</code>: Whether the value can be None.</li> <li><code>continuous_update</code>: Whether to call the <code>on_value</code> callback on every change or only when the input loses focus or the enter key is pressed.</li> <li><code>clearable</code>: Whether the input can be cleared.</li> <li><code>classes</code>: List of CSS classes to apply to the input.</li> <li><code>style</code>: CSS style to apply to the input.</li> <li><code>autofocus</code>: Determines if a component is to be autofocused or not (Default is False). Autofocus will occur either during page load, or when the component becomes visible (for example, dialog being opened). Only one component per page should have autofocus on each such event.</li> </ul> Source code in <code>hdxms_datasets/web/components.py</code> <pre><code>@solara.component\ndef InputNumeric(\n    label: str,\n    value: Union[None, num, solara.Reactive[num], solara.Reactive[Optional[num]]] = 0,\n    on_value: Union[None, Callable[[Optional[num]], None], Callable[[num], None]] = None,\n    disabled: bool = False,\n    optional: bool = False,\n    continuous_update: bool = False,\n    clearable: bool = False,\n    classes: list[str] = [],\n    style: Optional[Union[str, dict[str, str]]] = None,\n    autofocus: bool = False,\n):\n    \"\"\"Numeric input (float | integers).\n\n    Basic example:\n\n    ```solara\n    import solara\n\n    int_value = solara.reactive(42)\n    continuous_update = solara.reactive(True)\n\n    @solara.component\n    def Page():\n        solara.Checkbox(label=\"Continuous update\", value=continuous_update)\n        solara.InputInt(\"Enter an integer number\", value=int_value, continuous_update=continuous_update.value)\n        with solara.Row():\n            solara.Button(\"Clear\", on_click=lambda: int_value.set(42))\n        solara.Markdown(f\"**You entered**: {int_value.value}\")\n    ```\n\n    ## Arguments\n\n    * `label`: Label to display next to the slider.\n    * `value`: The currently entered value.\n    * `on_value`: Callback to call when the value changes.\n    * `disabled`: Whether the input is disabled.\n    * `optional`: Whether the value can be None.\n    * `continuous_update`: Whether to call the `on_value` callback on every change or only when the input loses focus or the enter key is pressed.\n    * `clearable`: Whether the input can be cleared.\n    * `classes`: List of CSS classes to apply to the input.\n    * `style`: CSS style to apply to the input.\n    * `autofocus`: Determines if a component is to be autofocused or not (Default is False). Autofocus will occur either during page load, or when the component becomes visible (for example, dialog being opened). Only one component per page should have autofocus on each such event.\n    \"\"\"\n\n    def str_to_num(value: Optional[str]):\n        if value:\n            try:\n                return int(value)\n            except ValueError:\n                try:\n                    return float(value)\n                except ValueError:\n                    raise ValueError(f\"Cannot convert {value} to a number\")\n        else:\n            if optional:\n                return None\n            else:\n                raise ValueError(\"Value cannot be empty\")\n\n    return _InputNumeric(\n        str_to_num,\n        label=label,\n        value=value,\n        on_value=on_value,\n        disabled=disabled,\n        continuous_update=continuous_update,\n        clearable=clearable,\n        classes=classes,\n        style=style,\n        autofocus=autofocus,\n    )\n</code></pre>"},{"location":"reference/web/models/","title":"models","text":""},{"location":"reference/web/state/","title":"state","text":""},{"location":"reference/web/state/#web.state.ListStore","title":"<code>ListStore</code>","text":"<p>               Bases: <code>Store[list[T]]</code></p> <p>baseclass for reactive list</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>class ListStore(Store[list[T]]):\n    \"\"\"baseclass for reactive list\"\"\"\n\n    def __init__(self, items: Optional[list[T]] = None):\n        super().__init__(items if items is not None else [])\n\n    def __len__(self):\n        return len(self.value)\n\n    def __getitem__(self, idx: int) -&gt; T:\n        return self.value[idx]\n\n    def __iter__(self):\n        return iter(self.value)\n\n    def get_item(self, idx: int, default: R = NO_DEFAULT) -&gt; T | R:\n        try:\n            return self._reactive.value[idx]\n        except IndexError:\n            if default is NO_DEFAULT:\n                raise IndexError(f\"Index {idx} is out of range\")\n            return default\n\n    def set(self, items: list[T]) -&gt; None:\n        self._reactive.value = items\n\n    def set_item(self, idx: int, item: T) -&gt; None:\n        new_items = self._reactive.value.copy()\n        if idx == len(new_items):\n            new_items.append(item)\n        elif idx &lt; len(new_items):\n            new_items[idx] = item\n        else:\n            raise IndexError(f\"Index {idx} is out of range\")\n        self._reactive.value = new_items\n\n    def append(self, item: T) -&gt; None:\n        self._reactive.value = [*self._reactive.value, item]\n\n    def extend(self, items: list[T]) -&gt; None:\n        new_value = self.value.copy()\n        new_value.extend(items)\n        self._reactive.value = new_value\n\n    def insert(self, idx: int, item: T) -&gt; None:\n        new_value = self.value.copy()\n        new_value.insert(idx, item)\n        self._reactive.value = new_value\n\n    def remove(self, item: T) -&gt; None:\n        self._reactive.value = [it for it in self.value if it != item]\n\n    def pop(self, idx: int) -&gt; T:\n        item = self.value[idx]\n        self._reactive.value = self.value[:idx] + self.value[idx + 1 :]\n        return item\n\n    def clear(self) -&gt; None:\n        self._reactive.value = []\n\n    def index(self, item: T) -&gt; int:\n        return self.value.index(item)\n\n    def update(self, idx: int, **kwargs):\n        new_value = self.value.copy()\n        updated_item = merge_state(new_value[idx], **kwargs)\n        new_value[idx] = updated_item\n        self._reactive.value = new_value\n\n    def count(self, item: T) -&gt; int:\n        return self.value.count(item)\n\n    def find_item(self, **kwargs) -&gt; Optional[T]:\n        \"\"\"find item in list by attributes\"\"\"\n        for item in self.value:\n            if all(getattr(item, k) == v for k, v in kwargs.items()):\n                return item\n        return None\n\n    def find_index(self, **kwargs) -&gt; Optional[int]:\n        \"\"\"find item in list by attributes\"\"\"\n        for idx, item in enumerate(self.value):\n            if all(getattr(item, k) == v for k, v in kwargs.items()):\n                return idx\n        return None\n</code></pre>"},{"location":"reference/web/state/#web.state.ListStore.find_index","title":"<code>find_index(**kwargs)</code>","text":"<p>find item in list by attributes</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>def find_index(self, **kwargs) -&gt; Optional[int]:\n    \"\"\"find item in list by attributes\"\"\"\n    for idx, item in enumerate(self.value):\n        if all(getattr(item, k) == v for k, v in kwargs.items()):\n            return idx\n    return None\n</code></pre>"},{"location":"reference/web/state/#web.state.ListStore.find_item","title":"<code>find_item(**kwargs)</code>","text":"<p>find item in list by attributes</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>def find_item(self, **kwargs) -&gt; Optional[T]:\n    \"\"\"find item in list by attributes\"\"\"\n    for item in self.value:\n        if all(getattr(item, k) == v for k, v in kwargs.items()):\n            return item\n    return None\n</code></pre>"},{"location":"reference/web/state/#web.state.PeptideStore","title":"<code>PeptideStore</code>","text":"<p>               Bases: <code>DictStore[str, ListStore[PeptideInfo]]</code></p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>class PeptideStore(DictStore[str, ListStore[PeptideInfo]]):\n    def add_peptide(self, state: str | None, peptide_type: PeptideType):  # TODO type ppetide_type\n        new_peptide = PeptideInfo(type=peptide_type)\n        self[state].append(new_peptide)\n\n    def update_peptide(self, state: str, peptide_idx: int, peptide: PeptideInfo):\n        new_peptides = self[state]\n        new_peptides.set_item(peptide_idx, peptide)\n        self.set_item(state, new_peptides)\n\n    def remove_peptide(self, state: str, peptide_idx: int):\n        new_peptides = self.value[state]\n        new_peptides.pop(peptide_idx)\n        self.set_item(state, new_peptides)\n\n    def add_state(self, state: str):\n        # append the new state to the dict\n        new_value = self.value | {state: ListStore[PeptideInfo]([])}\n        self.set(new_value)\n\n    def remove_state(self, state: str):\n        self.pop(state)\n\n    def validate(self) -&gt; dict[str, tuple[bool, str, str]]:\n        \"\"\"validate all peptides in the store\"\"\"\n        errors = {}\n        for state, peptides in self.items():\n            for idx, peptide in enumerate(peptides):\n                is_valid, error = peptide.validate()\n                if not is_valid:\n                    errors[state] = (idx, peptide.type, error)\n        return errors\n</code></pre>"},{"location":"reference/web/state/#web.state.PeptideStore.validate","title":"<code>validate()</code>","text":"<p>validate all peptides in the store</p> Source code in <code>hdxms_datasets/web/state.py</code> <pre><code>def validate(self) -&gt; dict[str, tuple[bool, str, str]]:\n    \"\"\"validate all peptides in the store\"\"\"\n    errors = {}\n    for state, peptides in self.items():\n        for idx, peptide in enumerate(peptides):\n            is_valid, error = peptide.validate()\n            if not is_valid:\n                errors[state] = (idx, peptide.type, error)\n    return errors\n</code></pre>"},{"location":"reference/web/upload_form/","title":"upload_form","text":""},{"location":"usage/loading/","title":"Loading Datasets","text":"<p>The <code>hdxms_datasets</code> package features a central <code>DataVault</code> object, which is used to fetch datasets from an online  database to a local cache dir, as well as parse those locally saved peptide sets into a narwhals <code>DataFrame</code>.</p>"},{"location":"usage/loading/#basic-usage","title":"Basic usage","text":"<pre><code># Creating a RemoveDataVault, specifying a chache dir, using the default remote database\nvault = RemoteDataVault(\n    cache_dir=\".cache\",\n)\nvault.get_index().to_native()\n\n#%%\n# Fetch a dataset by ID\nvault.fetch_dataset(\"1704204434_SecB_Krishnamurthy\")\n\n# Load the dataset\nds = vault.load_dataset(\"1704204434_SecB_Krishnamurthy\")\n\n# Print a string describing the states in the dataset\nprint(ds.describe())\n\n# Load ND control peptides as a narwhals DataFrame\nnd_control = ds.get_peptides(0, \"non_deuterated\").load()\n\n# # Load FD control peptides as a narwhals DataFrame\nfd_control = ds.get_peptides(0, \"fully_deuterated\").load()\n\n# Load experimental peptides as narwhals dataframe\npd_peptides = ds.get_peptides(0, \"partially_deuterated\").load()\npd_peptides\n# %%\n# Merge peptides, matching each partially dueterated peptide timepoint with nd/fd control uptake or mass\nmerged = merge_peptides(pd_peptides, nd_peptides=nd_control, fd_peptides=fd_control)\n\n# %%\n\n# compute d-uptake, max uptake, full deuteration uptake, RFU\nprocessed = compute_uptake_metrics(merged)\nprocessed.to_native()\n</code></pre> <p>The code above creates a <code>RemoteDataVault</code>, thereby creating a cache directory. Then the dataset <code>\"1704204434_SecB_Krishnamurthy\"</code> is fetched  from the database and stored in the cache dir.</p> <p>From here, HDX-MS data can be loaded and processed. </p>"}]}